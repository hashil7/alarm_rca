{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_topology_data(num_rings=10, nodes_per_ring=10, logical_rings_per_physical=3):\n",
    "    \"\"\"\n",
    "    Create dummy topology data for testing\n",
    "    \n",
    "    Args:\n",
    "        num_rings: Number of physical rings to create\n",
    "        nodes_per_ring: Number of nodes per ring\n",
    "        logical_rings_per_physical: Number of logical rings per physical ring\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with topology data\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for pr_idx in range(num_rings):\n",
    "        physical_ring = f\"RING_{pr_idx}\"\n",
    "        block_name = f\"BLOCK_{pr_idx}\"  # One block per physical ring\n",
    "        \n",
    "        # Each physical ring has multiple logical rings from the same block\n",
    "        for lr_idx in range(logical_rings_per_physical):\n",
    "            logical_ring = f\"LR_{pr_idx}_{lr_idx}\"\n",
    "            \n",
    "            # Create a linear path for each logical ring (not a complete ring)\n",
    "            for i in range(nodes_per_ring - 1):  # Connect nodes in a path, not a ring\n",
    "                node_a = f\"NODE_{pr_idx}_{lr_idx}_{i}\"\n",
    "                node_b = f\"NODE_{pr_idx}_{lr_idx}_{i+1}\"\n",
    "                \n",
    "                data.append({\n",
    "                    'aendname': node_a,\n",
    "                    'bendname': node_b,\n",
    "                    'aendip': f\"10.{pr_idx}.{lr_idx}.{i}\",\n",
    "                    'bendip': f\"10.{pr_idx}.{lr_idx}.{i+1}\",\n",
    "                    'aendifIndex': i,\n",
    "                    'bendifIndex': i+1,\n",
    "                    'block_name': block_name,\n",
    "                    'physicalringname': physical_ring,\n",
    "                    'lrname': logical_ring\n",
    "                })\n",
    "            \n",
    "            # Connect the first and last nodes to the block\n",
    "            # First node connects to block\n",
    "            data.append({\n",
    "                'aendname': f\"NODE_{pr_idx}_{lr_idx}_0\",\n",
    "                'bendname': block_name,\n",
    "                'aendip': f\"10.{pr_idx}.{lr_idx}.0\",\n",
    "                'bendip': f\"10.{pr_idx}.99.99\",  # Special IP for block\n",
    "                'aendifIndex': 100 + lr_idx,\n",
    "                'bendifIndex': 100 + lr_idx,\n",
    "                'block_name': block_name,\n",
    "                'physicalringname': physical_ring,\n",
    "                'lrname': logical_ring\n",
    "            })\n",
    "            \n",
    "            # Last node connects to block\n",
    "            data.append({\n",
    "                'aendname': f\"NODE_{pr_idx}_{lr_idx}_{nodes_per_ring-1}\",\n",
    "                'bendname': block_name,\n",
    "                'aendip': f\"10.{pr_idx}.{lr_idx}.{nodes_per_ring-1}\",\n",
    "                'bendip': f\"10.{pr_idx}.99.99\",  # Special IP for block\n",
    "                'aendifIndex': 200 + lr_idx,\n",
    "                'bendifIndex': 200 + lr_idx,\n",
    "                'block_name': block_name,\n",
    "                'physicalringname': physical_ring,\n",
    "                'lrname': logical_ring\n",
    "            })\n",
    "    \n",
    "    # Add connections between blocks from different physical rings\n",
    "    # for pr_idx in range(num_rings):\n",
    "    #     if pr_idx < num_rings - 1:  # Connect to next physical ring\n",
    "    #         # Connect this block to the next ring's block\n",
    "    #         block_a = f\"BLOCK_{pr_idx}\"\n",
    "    #         block_b = f\"BLOCK_{pr_idx+1}\"\n",
    "            \n",
    "    #         data.append({\n",
    "    #             'aendname': block_a,\n",
    "    #             'bendname': block_b,\n",
    "    #             'aendip': f\"10.{pr_idx}.99.99\",\n",
    "    #             'bendip': f\"10.{pr_idx+1}.99.99\",\n",
    "    #             'aendifIndex': 300 + pr_idx,\n",
    "    #             'bendifIndex': 300 + pr_idx + 1,\n",
    "    #             'block_name': block_a,\n",
    "    #             'physicalringname': f\"RING_{pr_idx}\",\n",
    "    #             'lrname': \"INTER_BLOCK\"  # Inter-block connection\n",
    "    #         })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_data = create_dummy_topology_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_with_position_features(topology_df):\n",
    "    \"\"\"Build NetworkX graph with enhanced position features\"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Track ring membership and positions\n",
    "    ring_positions = {}  # (pr_id, lr_id) -> list of positions\n",
    "    \n",
    "    # First pass: identify all rings and node positions\n",
    "    for _, row in topology_df.iterrows():\n",
    "        for node_col in ['aendname', 'bendname']:\n",
    "            node = row[node_col]\n",
    "            if not isinstance(node, str) or 'NODE_' not in node:\n",
    "                continue\n",
    "                \n",
    "            parts = node.split('_')\n",
    "            if len(parts) >= 4:\n",
    "                try:\n",
    "                    pr_id = int(parts[1])\n",
    "                    lr_id = int(parts[2])\n",
    "                    pos = int(parts[3])\n",
    "                    \n",
    "                    key = (pr_id, lr_id)\n",
    "                    if key not in ring_positions:\n",
    "                        ring_positions[key] = []\n",
    "                    \n",
    "                    if pos not in ring_positions[key]:\n",
    "                        ring_positions[key].append(pos)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    \n",
    "    # Sort positions within each ring\n",
    "    for key in ring_positions:\n",
    "        ring_positions[key].sort()\n",
    "    \n",
    "    # Add nodes and edges with position-aware features\n",
    "    for _, row in topology_df.iterrows():\n",
    "        aend = row['aendname']\n",
    "        bend = row['bendname']\n",
    "        \n",
    "        # Add nodes with enhanced features\n",
    "        for node in [aend, bend]:\n",
    "            if node in G:\n",
    "                continue  # Skip if already added\n",
    "                \n",
    "            # Default features\n",
    "            features = {\n",
    "                'is_block': 'BLOCK' in str(node),\n",
    "                'pr_id': -1,\n",
    "                'lr_id': -1,\n",
    "                'position': -1,\n",
    "  \n",
    "            }\n",
    "            \n",
    "            # Extract position information\n",
    "            if isinstance(node, str) and 'NODE_' in node:\n",
    "                parts = node.split('_')\n",
    "                if len(parts) >= 4:\n",
    "                    try:\n",
    "                        pr_id = int(parts[1])\n",
    "                        lr_id = int(parts[2])\n",
    "                        pos = int(parts[3])\n",
    "                        \n",
    "                        # Get normalized position (crucial for learning the pattern)\n",
    "\n",
    "\n",
    "                        \n",
    "                        features.update({\n",
    "                            'pr_id': pr_id,\n",
    "                            'lr_id': lr_id,\n",
    "                            'position': pos,\n",
    "\n",
    "                        })\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            \n",
    "            G.add_node(node, **features)\n",
    "        \n",
    "        # Add edge\n",
    "        G.add_edge(aend, bend)\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = build_graph_with_position_features(topo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_failure_status_to_graph(G, failed_node_names=None):\n",
    "    \"\"\"Add 'failed' attribute to graph nodes\"\"\"\n",
    "    if failed_node_names is None:\n",
    "        failed_node_names = []\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    G_copy = G.copy()\n",
    "    \n",
    "    # Set failed attribute for all nodes\n",
    "    for node in G_copy.nodes():\n",
    "        G_copy.nodes[node]['failed'] = node in failed_node_names\n",
    "    \n",
    "    return G_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphStateGNN(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(GraphStateGNN, self).__init__()\n",
    "        \n",
    "        # Feature processing\n",
    "        self.node_encoder = nn.Linear(in_channels, 64)\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.conv1 = GCNConv(64, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "        \n",
    "        # Isolation prediction layer\n",
    "        self.predictor = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"The first feature of x is the 'failed' status\"\"\"\n",
    "        # Initial node encoding\n",
    "        h = F.relu(self.node_encoder(x))\n",
    "        \n",
    "        # Message passing to understand graph structure\n",
    "        h = F.relu(self.conv1(h, edge_index))\n",
    "        h = F.dropout(h, p=0.2, training=self.training)\n",
    "        \n",
    "        h = F.relu(self.conv2(h, edge_index))\n",
    "        \n",
    "        # Predict isolation probability\n",
    "        out = torch.sigmoid(self.predictor(h))\n",
    "        \n",
    "        # Make sure failed nodes are never predicted as isolated\n",
    "        # failed status is the first feature (x[:, 0])\n",
    "        failed_mask = (x[:, 0] < 0.5).float().unsqueeze(1)\n",
    "        out = out * failed_mask\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_isolated_nodes(G, node_list, node_to_idx):\n",
    "    \"\"\"Calculate which nodes are isolated based on graph failures\"\"\"\n",
    "    y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "    \n",
    "    for i, node in enumerate(node_list):\n",
    "        # Skip failed nodes\n",
    "        if G.nodes[node].get('failed', False):\n",
    "            continue\n",
    "            \n",
    "        # Get node attributes\n",
    "        attrs = G.nodes[node]\n",
    "        pr_id = attrs.get('pr_id', -1)\n",
    "        lr_id = attrs.get('lr_id', -1)\n",
    "        pos = attrs.get('position', -1)\n",
    "        \n",
    "        if pr_id < 0 or lr_id < 0 or pos < 0:\n",
    "            continue\n",
    "        \n",
    "        # Find failed nodes in same ring\n",
    "        failed_in_ring = []\n",
    "        for other_node in node_list:\n",
    "            if not G.nodes[other_node].get('failed', False):\n",
    "                continue\n",
    "                \n",
    "            other_attrs = G.nodes[other_node]\n",
    "            other_pr = other_attrs.get('pr_id', -1)\n",
    "            other_lr = other_attrs.get('lr_id', -1)\n",
    "            other_pos = other_attrs.get('position', -1)\n",
    "            \n",
    "            # Check if in same ring\n",
    "            if other_pr == pr_id and other_lr == lr_id and other_pos >= 0:\n",
    "                failed_in_ring.append((other_node, other_pos))\n",
    "        \n",
    "        # Check if between any two failed nodes\n",
    "        if len(failed_in_ring) >= 2:\n",
    "            for i in range(len(failed_in_ring)):\n",
    "                for j in range(i+1, len(failed_in_ring)):\n",
    "                    pos1 = failed_in_ring[i][1]\n",
    "                    pos2 = failed_in_ring[j][1]\n",
    "                    \n",
    "                    if min(pos1, pos2) < pos < max(pos1, pos2):\n",
    "                        y[node_to_idx[node]] = 1.0\n",
    "                        break\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_from_graph(G):\n",
    "    \"\"\"Convert graph with failure attributes to PyG data\"\"\"\n",
    "    from torch_geometric.data import Data\n",
    "    \n",
    "    # Create node mapping\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # Create edge index\n",
    "    edge_index = []\n",
    "    for u, v in G.edges():\n",
    "        edge_index.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edge_index.append([node_to_idx[v], node_to_idx[u]])\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "    \n",
    "    # Create node features\n",
    "    x = []\n",
    "    failed_nodes = []\n",
    "    for i, node in enumerate(node_list):\n",
    "        attrs = G.nodes[node]\n",
    "        \n",
    "        # Get failed status as first feature\n",
    "        is_failed = float(attrs.get('failed', False))\n",
    "        if is_failed > 0.5:\n",
    "            failed_nodes.append(i)\n",
    "        \n",
    "        features = [\n",
    "            is_failed,                       # Failed status as first feature\n",
    "            float(attrs.get('is_block', False)),\n",
    "            attrs.get('pr_id', -1) / 10.0,\n",
    "            attrs.get('lr_id', -1) / 10.0,\n",
    "            attrs.get('position', -1) / 10.0,\n",
    "        ]\n",
    "        x.append(features)\n",
    "    \n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    # Calculate isolated nodes (ground truth)\n",
    "    y = calculate_isolated_nodes(G, node_list, node_to_idx)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph_state_model(graph_examples, num_epochs=30):\n",
    "    \"\"\"Train model on graphs with failed attributes\"\"\"\n",
    "    from torch_geometric.loader import DataLoader\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Prepare training data\n",
    "    train_data = [prepare_data_from_graph(G) for G in graph_examples]\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Create model\n",
    "    in_channels = train_data[0].x.size(1)\n",
    "    model = GraphStateGNN(in_channels).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Class weights\n",
    "    pos_weight = torch.tensor([10.0]).to(device)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "            \n",
    "            # Loss with weighting\n",
    "            loss = F.binary_cross_entropy(\n",
    "                out.squeeze(), \n",
    "                batch.y,\n",
    "                weight=pos_weight * batch.y + (1.0 - batch.y)\n",
    "            )\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Track predictions\n",
    "            pred = (out.squeeze() > 0.5).float()\n",
    "            all_preds.append(pred.detach().cpu())\n",
    "            all_labels.append(batch.y.detach().cpu())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{num_epochs}:\")\n",
    "        print(f\"  Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "        print(f\"  Predictions: {all_preds.sum().item()}/{len(all_preds)}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_examples(G, num_examples=100):\n",
    "    \"\"\"Generate graphs with different failure patterns\"\"\"\n",
    "    import random\n",
    "    \n",
    "    examples = []\n",
    "    nodes = list(G.nodes())\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        # Randomly select 1-4 failed nodes\n",
    "        num_failures = 2\n",
    "        failed_nodes = random.sample(nodes, num_failures)\n",
    "        \n",
    "        # Create graph with these failures\n",
    "        G_example = add_failure_status_to_graph(G, failed_nodes)\n",
    "        \n",
    "        # Add to examples\n",
    "        examples.append(G_example)\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_isolations(model, G):\n",
    "    \"\"\"Predict which nodes are isolated in a graph\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Prepare data\n",
    "    data = prepare_data_from_graph(G)\n",
    "    data = data.to(device)\n",
    "    \n",
    "    # Get node mapping\n",
    "    node_list = list(G.nodes())\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        isolation_pred = (out.squeeze() > 0.5).cpu().numpy()\n",
    "    \n",
    "    # Get isolated nodes\n",
    "    isolated_nodes = [node_list[i] for i, is_isolated in enumerate(isolation_pred) \n",
    "                     if is_isolated]\n",
    "    \n",
    "    return isolated_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training data...\n",
      "Training model...\n",
      "Epoch 1/30:\n",
      "  Loss: 0.5981\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 2/30:\n",
      "  Loss: 0.4847\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 3/30:\n",
      "  Loss: 0.3464\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 4/30:\n",
      "  Loss: 0.1888\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 5/30:\n",
      "  Loss: 0.0772\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 6/30:\n",
      "  Loss: 0.0317\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 7/30:\n",
      "  Loss: 0.0212\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 8/30:\n",
      "  Loss: 0.0190\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 9/30:\n",
      "  Loss: 0.0193\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 10/30:\n",
      "  Loss: 0.0197\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 11/30:\n",
      "  Loss: 0.0188\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 12/30:\n",
      "  Loss: 0.0191\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 13/30:\n",
      "  Loss: 0.0191\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 14/30:\n",
      "  Loss: 0.0187\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 15/30:\n",
      "  Loss: 0.0188\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 16/30:\n",
      "  Loss: 0.0186\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 17/30:\n",
      "  Loss: 0.0189\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 18/30:\n",
      "  Loss: 0.0187\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 19/30:\n",
      "  Loss: 0.0181\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 20/30:\n",
      "  Loss: 0.0260\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 21/30:\n",
      "  Loss: 0.0188\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 22/30:\n",
      "  Loss: 0.0185\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 23/30:\n",
      "  Loss: 0.0181\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 24/30:\n",
      "  Loss: 0.0187\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 25/30:\n",
      "  Loss: 0.0184\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 26/30:\n",
      "  Loss: 0.0180\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 27/30:\n",
      "  Loss: 0.0177\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 28/30:\n",
      "  Loss: 0.0179\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 29/30:\n",
      "  Loss: 0.0180\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "Epoch 30/30:\n",
      "  Loss: 0.0176\n",
      "  Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "  Predictions: 0.0/62000\n",
      "\n",
      "Predicting isolations...\n",
      "Failures: ['NODE_0_0_2', 'NODE_0_0_7']\n",
      "Isolated nodes: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating training data...\")\n",
    "graph_examples = generate_graph_examples(G, num_examples=200)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "model = train_graph_state_model(graph_examples)\n",
    "\n",
    "# Create a test graph with specific failures\n",
    "test_failures = [\"NODE_0_0_2\", \"NODE_0_0_7\"]\n",
    "test_graph = add_failure_status_to_graph(G, test_failures)\n",
    "\n",
    "# Predict isolations\n",
    "print(\"\\nPredicting isolations...\")\n",
    "isolated = predict_isolations(model, test_graph)\n",
    "\n",
    "print(f\"Failures: {test_failures}\")\n",
    "print(f\"Isolated nodes: {len(isolated)}\")\n",
    "for node in isolated[:5]:\n",
    "    print(f\"- {node}\")\n",
    "if len(isolated) > 5:\n",
    "    print(f\"...and {len(isolated) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nms/.local/lib/python3.13/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.0822\n",
      "Epoch 2: Loss = 0.0772\n",
      "Epoch 3: Loss = 0.0788\n",
      "Epoch 4: Loss = 0.0785\n",
      "Epoch 5: Loss = 0.0751\n",
      "Epoch 6: Loss = 0.0760\n",
      "Epoch 7: Loss = 0.0786\n",
      "Epoch 8: Loss = 0.0770\n",
      "Epoch 9: Loss = 0.0777\n",
      "Epoch 10: Loss = 0.0800\n",
      "Epoch 11: Loss = 0.0755\n",
      "Epoch 12: Loss = 0.0780\n",
      "Epoch 13: Loss = 0.0758\n",
      "Epoch 14: Loss = 0.0782\n",
      "Epoch 15: Loss = 0.0767\n",
      "Epoch 16: Loss = 0.0769\n",
      "Epoch 17: Loss = 0.0765\n",
      "Epoch 18: Loss = 0.0790\n",
      "Epoch 19: Loss = 0.0782\n",
      "Epoch 20: Loss = 0.0753\n",
      "Epoch 21: Loss = 0.0773\n",
      "Epoch 22: Loss = 0.0783\n",
      "Epoch 23: Loss = 0.0778\n",
      "Epoch 24: Loss = 0.0782\n",
      "Epoch 25: Loss = 0.0784\n",
      "Epoch 26: Loss = 0.0778\n",
      "Epoch 27: Loss = 0.0797\n",
      "Epoch 28: Loss = 0.0777\n",
      "Epoch 29: Loss = 0.0782\n",
      "Epoch 30: Loss = 0.0773\n",
      "Epoch 31: Loss = 0.0763\n",
      "Epoch 32: Loss = 0.0786\n",
      "Epoch 33: Loss = 0.0789\n",
      "Epoch 34: Loss = 0.0771\n",
      "Epoch 35: Loss = 0.0763\n",
      "Epoch 36: Loss = 0.0772\n",
      "Epoch 37: Loss = 0.0794\n",
      "Epoch 38: Loss = 0.0790\n",
      "Epoch 39: Loss = 0.0785\n",
      "Epoch 40: Loss = 0.0769\n",
      "Epoch 41: Loss = 0.0771\n",
      "Epoch 42: Loss = 0.0749\n",
      "Epoch 43: Loss = 0.0786\n",
      "Epoch 44: Loss = 0.0736\n",
      "Epoch 45: Loss = 0.0752\n",
      "Epoch 46: Loss = 0.0763\n",
      "Epoch 47: Loss = 0.0757\n",
      "Epoch 48: Loss = 0.0762\n",
      "Epoch 49: Loss = 0.0775\n",
      "Epoch 50: Loss = 0.0789\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your functions 'create_dummy_topology_data' and 'build_graph_with_position_features' are defined\n",
    "\n",
    "# Create dummy data and graph\n",
    "topology_df = create_dummy_topology_data(num_rings=10, nodes_per_ring=10, logical_rings_per_physical=3)\n",
    "G_nx = build_graph_with_position_features(topology_df)\n",
    "\n",
    "# Convert networkx graph to PyG Data object\n",
    "def nx_to_pyg_data(G):\n",
    "    # Map node features to vectors; here, we simply use a small feature vector based on our features\n",
    "    mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "    edge_index = []\n",
    "    features = []\n",
    "    for node, data in G.nodes(data=True):\n",
    "        # Feature vector: [is_block, pr_id, lr_id, position]\n",
    "        is_block = 1.0 if data.get('is_block', False) else 0.0\n",
    "        pr_id = float(data.get('pr_id', -1))\n",
    "        lr_id = float(data.get('lr_id', -1))\n",
    "        position = float(data.get('position', -1))\n",
    "        features.append([is_block, pr_id, lr_id, position])\n",
    "    for u, v in G.edges():\n",
    "        edge_index.append([mapping[u], mapping[v]])\n",
    "        edge_index.append([mapping[v], mapping[u]])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    x = torch.tensor(features, dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    # Save mapping to refer back to node names if needed\n",
    "    data.node_names = list(G.nodes())\n",
    "    return data\n",
    "\n",
    "data = nx_to_pyg_data(G_nx)\n",
    "\n",
    "# Helper function: get ring membership based on node names and their features\n",
    "def get_ring_id(node_name):\n",
    "    # Assuming node_name format: \"NODE_{pr}_{lr}_{pos}\"\n",
    "    parts = node_name.split('_')\n",
    "    if len(parts) >= 4:\n",
    "        return (int(parts[1]), int(parts[2]))\n",
    "    return None\n",
    "\n",
    "# Build a training dataset:\n",
    "def create_training_sample(data, endpoints_per_sample=1):\n",
    "    \"\"\"\n",
    "    For each training sample, we randomly select a ring, pick two endpoint nodes from that ring,\n",
    "    and label nodes between them as class 1, others as class 0.\n",
    "    \"\"\"\n",
    "    # Group nodes by ring using data.node_names and node feature information.\n",
    "    ring_dict = {}\n",
    "    for idx, node_name in enumerate(data.node_names):\n",
    "        if \"NODE_\" in node_name:\n",
    "            ring = get_ring_id(node_name)\n",
    "            if ring is not None:\n",
    "                ring_dict.setdefault(ring, []).append((idx, node_name))\n",
    "    \n",
    "    # Filter out rings with less than 2 nodes.\n",
    "    rings = [nodes for nodes in ring_dict.values() if len(nodes) >= 2]\n",
    "    if not rings:\n",
    "        raise ValueError(\"Not enough nodes to form training samples.\")\n",
    "    \n",
    "    # Select a random ring\n",
    "    selected_ring = random.choice(rings)\n",
    "    # Sort nodes in the ring by their position (extracted from the node name)\n",
    "    selected_ring.sort(key=lambda x: int(x[1].split('_')[3]))\n",
    "    # Randomly pick two endpoints ensuring the first is before the second\n",
    "    i, j = sorted(random.sample(range(len(selected_ring)), 2))\n",
    "    idx_a, name_a = selected_ring[i]\n",
    "    idx_b, name_b = selected_ring[j]\n",
    "    \n",
    "    # Create labels for all nodes: label 1 if the node is in-between on the ring, else 0.\n",
    "    labels = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    # Label the nodes in the selected ring between endpoints (including endpoints if desired)\n",
    "    for idx, name in selected_ring[i:j+1]:\n",
    "        labels[idx] = 1\n",
    "    # Additionally, you can include the endpoints as context features if needed.\n",
    "    return labels, (idx_a, idx_b)\n",
    "\n",
    "# Create a simple dataset of training samples\n",
    "class TopologyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, num_samples=1000):\n",
    "        self.data = data\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # For each sample, generate new labels and return the full graph with sample-specific labels\n",
    "        labels, endpoints = create_training_sample(self.data)\n",
    "        # endpoints can be used in a more complex model as additional input; here we just return them\n",
    "        sample = {\n",
    "            'data': self.data, \n",
    "            'labels': labels,\n",
    "            'endpoints': endpoints\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "dataset = TopologyDataset(data, num_samples=500)\n",
    "loader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "# Define a simple GNN Model\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        # Node-level predictions\n",
    "        out = self.lin(x)\n",
    "        return out\n",
    "\n",
    "# Instantiate model, loss and optimizer\n",
    "model = GNNModel(in_channels=4, hidden_channels=16, out_channels=2)  # binary classification: in-between or not\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "def train(model, loader, epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for sample in loader:\n",
    "            data_sample = sample['data'][0]  # since batch_size=1\n",
    "            labels = sample['labels'][0]\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data_sample)\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss/len(loader):.4f}\")\n",
    "\n",
    "train(model, loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_sample_with_marked_endpoints(data, endpoints_per_sample=1):\n",
    "    \"\"\"\n",
    "    Create a training sample with explicitly marked endpoint nodes\n",
    "    \"\"\"\n",
    "    # Group nodes by ring using data.node_names and node feature information\n",
    "    ring_dict = {}\n",
    "    for idx, node_name in enumerate(data.node_names):\n",
    "        if \"NODE_\" in node_name:\n",
    "            ring = get_ring_id(node_name)\n",
    "            if ring is not None:\n",
    "                ring_dict.setdefault(ring, []).append((idx, node_name))\n",
    "    \n",
    "    # Filter out rings with less than 2 nodes\n",
    "    rings = [nodes for nodes in ring_dict.values() if len(nodes) >= 2]\n",
    "    if not rings:\n",
    "        raise ValueError(\"Not enough nodes to form training samples.\")\n",
    "    \n",
    "    # Select a random ring\n",
    "    selected_ring = random.choice(rings)\n",
    "    \n",
    "    # Sort nodes in the ring by their position\n",
    "    selected_ring.sort(key=lambda x: int(x[1].split('_')[3]))\n",
    "    \n",
    "    # Randomly pick two endpoints ensuring the first is before the second\n",
    "    i, j = sorted(random.sample(range(len(selected_ring)), 2))\n",
    "    idx_a, name_a = selected_ring[i]\n",
    "    idx_b, name_b = selected_ring[j]\n",
    "    \n",
    "    # Create endpoint markers for all nodes (initialized to zeros)\n",
    "    endpoint_features = torch.zeros((data.num_nodes, 2), dtype=torch.float)\n",
    "    \n",
    "    # Mark endpoint nodes - first endpoint gets [1,0], second gets [0,1]\n",
    "    endpoint_features[idx_a, 0] = 1.0\n",
    "    endpoint_features[idx_b, 1] = 1.0\n",
    "    \n",
    "    # Create labels for nodes (1 if between endpoints, 0 otherwise)\n",
    "    labels = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    \n",
    "    # Label nodes between endpoints (not including endpoints themselves)\n",
    "    for idx, name in selected_ring[i+1:j]:\n",
    "        labels[idx] = 1\n",
    "    \n",
    "    # Create a copy of the data with added endpoint features\n",
    "    new_x = torch.cat([data.x, endpoint_features], dim=1)\n",
    "    new_data = Data(x=new_x, edge_index=data.edge_index)\n",
    "    new_data.node_names = data.node_names\n",
    "    \n",
    "    return new_data, labels, (idx_a, idx_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedTopologyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_data, num_samples=1000):\n",
    "        self.base_data = base_data\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # For each sample, generate new data with marked endpoints and labels\n",
    "        data_with_endpoints, labels, endpoints = create_training_sample_with_marked_endpoints(self.base_data)\n",
    "        \n",
    "        sample = {\n",
    "            'data': data_with_endpoints, \n",
    "            'labels': labels,\n",
    "            'endpoints': endpoints\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndpointAwareGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(EndpointAwareGNN, self).__init__()\n",
    "        # in_channels now includes the 2 additional endpoint marker features\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # Node-level predictions\n",
    "        out = self.lin(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def train_endpoint_aware(model, loader, epochs=50):\n",
    "    model.train()\n",
    "    \n",
    "    # Use Adam optimizer with weight decay\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for sample in loader:\n",
    "            # Get data with endpoints already marked in features\n",
    "            data_sample = sample['data'][0]  # since batch_size=1\n",
    "            labels = sample['labels'][0]\n",
    "            endpoints = sample['endpoints'][0]\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data_sample)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, pred = out.max(dim=1)\n",
    "            \n",
    "            # Store predictions and labels for metrics calculation\n",
    "            all_preds.append(pred.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(out, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Concatenate all predictions and labels\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss/len(loader):.4f}\")\n",
    "        print(f\"  Accuracy = {accuracy:.4f}, Precision = {precision:.4f}\")\n",
    "        print(f\"  Recall = {recall:.4f}, F1 Score = {f1:.4f}\")\n",
    "        # Save the model weights\n",
    "        model_save_path = 'trained_gnn_model.pt'\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"  Model saved to {model_save_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nms/.local/lib/python3.13/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.0543\n",
      "  Accuracy = 0.9895, Precision = 0.0199\n",
      "  Recall = 0.0036, F1 Score = 0.0061\n",
      "  Model saved to trained_gnn_model.pt\n",
      "Epoch 2: Loss = 0.0348\n",
      "  Accuracy = 0.9912, Precision = 0.0000\n",
      "  Recall = 0.0000, F1 Score = 0.0000\n",
      "  Model saved to trained_gnn_model.pt\n",
      "Epoch 3: Loss = 0.0326\n",
      "  Accuracy = 0.9911, Precision = 0.0000\n",
      "  Recall = 0.0000, F1 Score = 0.0000\n",
      "  Model saved to trained_gnn_model.pt\n",
      "Epoch 4: Loss = 0.0326\n",
      "  Accuracy = 0.9912, Precision = 0.0000\n",
      "  Recall = 0.0000, F1 Score = 0.0000\n",
      "  Model saved to trained_gnn_model.pt\n",
      "Epoch 5: Loss = 0.0291\n",
      "  Accuracy = 0.9919, Precision = 0.0000\n",
      "  Recall = 0.0000, F1 Score = 0.0000\n",
      "  Model saved to trained_gnn_model.pt\n",
      "Epoch 6: Loss = 0.0322\n",
      "  Accuracy = 0.9909, Precision = 0.0000\n",
      "  Recall = 0.0000, F1 Score = 0.0000\n",
      "  Model saved to trained_gnn_model.pt\n",
      "Epoch 7: Loss = 0.0302\n",
      "  Accuracy = 0.9912, Precision = 0.0000\n",
      "  Recall = 0.0000, F1 Score = 0.0000\n",
      "  Model saved to trained_gnn_model.pt\n",
      "Epoch 8: Loss = 0.0286\n",
      "  Accuracy = 0.9921, Precision = 0.0000\n",
      "  Recall = 0.0000, F1 Score = 0.0000\n",
      "  Model saved to trained_gnn_model.pt\n",
      "Epoch 9: Loss = 0.0308\n",
      "  Accuracy = 0.9915, Precision = 0.0000\n",
      "  Recall = 0.0000, F1 Score = 0.0000\n",
      "  Model saved to trained_gnn_model.pt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtrain_endpoint_aware\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtrain_endpoint_aware\u001b[39m\u001b[34m(model, loader, epochs)\u001b[39m\n\u001b[32m     32\u001b[39m loss = criterion(out, labels)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m optimizer.step()\n\u001b[32m     38\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create the base data object\n",
    "data = nx_to_pyg_data(G_nx)\n",
    "\n",
    "# Create the enhanced dataset\n",
    "dataset = EnhancedTopologyDataset(data, num_samples=500)\n",
    "loader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "# Create model with additional endpoint features\n",
    "model = EndpointAwareGNN(\n",
    "    in_channels=data.x.size(1) + 2,  # Original features + 2 endpoint markers\n",
    "    hidden_channels=32, \n",
    "    out_channels=2\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train_endpoint_aware(model, loader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_for_specific_nodes(model, data, node_indices):\n",
    "    \"\"\"\n",
    "    Get predictions for specific nodes from an already prepared graph\n",
    "    \n",
    "    Args:\n",
    "        model: Trained GNNModel\n",
    "        data: PyG Data object containing the graph\n",
    "        node_indices: Indices of the nodes to get predictions for\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of predictions for specified nodes\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass on the entire graph\n",
    "        all_node_predictions = model(data)\n",
    "        \n",
    "        # Extract only the predictions for specified nodes\n",
    "        node_predictions = all_node_predictions[node_indices]\n",
    "    \n",
    "    return node_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GNNModel:\n\tUnexpected key(s) in state_dict: \"attention.0.weight\", \"attention.0.bias\", \"attention.2.weight\", \"attention.2.bias\". \n\tsize mismatch for conv1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv1.lin.weight: copying a param with shape torch.Size([32, 6]) from checkpoint, the shape in current model is torch.Size([64, 5]).\n\tsize mismatch for conv2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv2.lin.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for lin.weight: copying a param with shape torch.Size([2, 32]) from checkpoint, the shape in current model is torch.Size([1, 64]).\n\tsize mismatch for lin.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load your trained model\u001b[39;00m\n\u001b[32m      2\u001b[39m model = GNNModel(in_channels=\u001b[32m5\u001b[39m, hidden_channels=\u001b[32m64\u001b[39m, out_channels=\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrained_gnn_model.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Define input nodes\u001b[39;00m\n\u001b[32m      6\u001b[39m node1 = \u001b[33m\"\u001b[39m\u001b[33mNODE_0_0_2\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:2581\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2573\u001b[39m         error_msgs.insert(\n\u001b[32m   2574\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2575\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2576\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2577\u001b[39m             ),\n\u001b[32m   2578\u001b[39m         )\n\u001b[32m   2580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2581\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2583\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2584\u001b[39m         )\n\u001b[32m   2585\u001b[39m     )\n\u001b[32m   2586\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for GNNModel:\n\tUnexpected key(s) in state_dict: \"attention.0.weight\", \"attention.0.bias\", \"attention.2.weight\", \"attention.2.bias\". \n\tsize mismatch for conv1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv1.lin.weight: copying a param with shape torch.Size([32, 6]) from checkpoint, the shape in current model is torch.Size([64, 5]).\n\tsize mismatch for conv2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv2.lin.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for lin.weight: copying a param with shape torch.Size([2, 32]) from checkpoint, the shape in current model is torch.Size([1, 64]).\n\tsize mismatch for lin.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([1])."
     ]
    }
   ],
   "source": [
    "# Load your trained model\n",
    "model = GNNModel(in_channels=5, hidden_channels=64, out_channels=1)\n",
    "model.load_state_dict(torch.load('trained_gnn_model.pt'))\n",
    "\n",
    "# Define input nodes\n",
    "node1 = \"NODE_0_0_2\"\n",
    "node2 = \"NODE_0_0_7\"\n",
    "\n",
    "# Make predictions\n",
    "results = predict_for_two_nodes(model, G, node1, node2)\n",
    "\n",
    "# Print results\n",
    "print(f\"Input nodes: {results['input_nodes']}\")\n",
    "print(f\"Affected nodes: {len(results['affected_nodes'])}\")\n",
    "for node, prob in results['affected_nodes'][:5]:\n",
    "    print(f\"- {node}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader  # Use PyG's DataLoader\n",
    "\n",
    "class SingleGraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_samples=1000, num_rings=10, nodes_per_ring=10, logical_rings_per_physical=3):\n",
    "        # Create graph\n",
    "        topology_df = create_dummy_topology_data(num_rings, nodes_per_ring, logical_rings_per_physical)\n",
    "        self.G = build_graph_with_position_features(topology_df)\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        # Precompute static graph data\n",
    "        self.node_list = list(self.G.nodes())\n",
    "        \n",
    "        # Create node name to index mapping\n",
    "        self.node_to_idx = {node: i for i, node in enumerate(self.node_list)}\n",
    "        \n",
    "        # Convert edges to index pairs\n",
    "        edge_list = []\n",
    "        for u, v in self.G.edges():\n",
    "            edge_list.append([self.node_to_idx[u], self.node_to_idx[v]])\n",
    "            edge_list.append([self.node_to_idx[v], self.node_to_idx[u]])\n",
    "        \n",
    "        self.edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "        \n",
    "        # Create node features\n",
    "        node_features = []\n",
    "        for node in self.node_list:\n",
    "            attrs = self.G.nodes[node]\n",
    "            features = [\n",
    "                float(attrs.get('is_block', False)),\n",
    "                attrs.get('pr_id', -1) / 10.0,\n",
    "                attrs.get('lr_id', -1) / 10.0,\n",
    "                attrs.get('position', -1) / 10.0,\n",
    "            ]\n",
    "            node_features.append(features)\n",
    "        \n",
    "        self.node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Generate random sample using the static graph\n",
    "        from torch_geometric.data import Data\n",
    "        \n",
    "        # Select random ring and two nodes\n",
    "        rings = {}\n",
    "        for node, data in self.G.nodes(data=True):\n",
    "            pr_id = data.get('pr_id', -1)\n",
    "            lr_id = data.get('lr_id', -1)\n",
    "            if pr_id >= 0 and lr_id >= 0:\n",
    "                key = (pr_id, lr_id)\n",
    "                if key not in rings:\n",
    "                    rings[key] = []\n",
    "                rings[key].append((node, data.get('position', -1)))\n",
    "        \n",
    "        # Filter rings with enough nodes\n",
    "        valid_rings = [nodes for nodes in rings.values() if len(nodes) >= 2]\n",
    "        if not valid_rings:\n",
    "            # Fallback if no valid rings\n",
    "            return Data(x=self.node_features, edge_index=self.edge_index, \n",
    "                       y=torch.zeros(len(self.node_list), dtype=torch.long))\n",
    "        \n",
    "        # Select a random ring\n",
    "        selected_ring = random.choice(valid_rings)\n",
    "        \n",
    "        # Sort by position\n",
    "        selected_ring.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # Pick two random nodes as endpoints\n",
    "        if len(selected_ring) < 2:\n",
    "            endpoint_indices = [0, 0]  # Fallback\n",
    "        else:\n",
    "            i, j = sorted(random.sample(range(len(selected_ring)), 2))\n",
    "            endpoint_indices = [self.node_to_idx[selected_ring[i][0]], \n",
    "                              self.node_to_idx[selected_ring[j][0]]]\n",
    "        \n",
    "        # Create endpoint markers\n",
    "        endpoint_markers = torch.zeros((len(self.node_list), 2), dtype=torch.float)\n",
    "        endpoint_markers[endpoint_indices[0], 0] = 1.0\n",
    "        endpoint_markers[endpoint_indices[1], 1] = 1.0\n",
    "        \n",
    "        # Combine features with endpoint markers\n",
    "        x = torch.cat([self.node_features, endpoint_markers], dim=1)\n",
    "        \n",
    "        # Create labels - nodes between endpoints are isolated\n",
    "        y = torch.zeros(len(self.node_list), dtype=torch.long)\n",
    "        \n",
    "        # Only label if we have two distinct endpoints\n",
    "        if endpoint_indices[0] != endpoint_indices[1]:\n",
    "            # Get all nodes in the ring\n",
    "            ring_nodes = [self.node_to_idx[node] for node, _ in selected_ring]\n",
    "            \n",
    "            # Get min/max index in the sorted ring\n",
    "            min_idx = min(endpoint_indices)\n",
    "            max_idx = max(endpoint_indices)\n",
    "            \n",
    "            # Label nodes between endpoints in the ring\n",
    "            ring_pos_dict = {self.node_to_idx[node]: pos for node, pos in selected_ring}\n",
    "            min_pos = ring_pos_dict[min_idx]\n",
    "            max_pos = ring_pos_dict[max_idx]\n",
    "            \n",
    "            for node_idx in ring_nodes:\n",
    "                if node_idx in endpoint_indices:\n",
    "                    continue\n",
    "                pos = ring_pos_dict.get(node_idx, -1)\n",
    "                if min_pos < pos < max_pos:\n",
    "                    y[node_idx] = 1\n",
    "        \n",
    "        return Data(x=x, edge_index=self.edge_index, y=y, endpoints=torch.tensor(endpoint_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GNNModel.__init__() got an unexpected keyword argument 'in_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     76\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Positive predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_preds.sum().item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_preds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[43mtrain_single_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mtrain_single_graph\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m loader = DataLoader(dataset, batch_size=\u001b[32m32\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Define model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m model = \u001b[43mGNNModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     32\u001b[39m optimizer = optim.Adam(model.parameters(), lr=\u001b[32m0.01\u001b[39m, weight_decay=\u001b[32m5e-4\u001b[39m)\n\u001b[32m     33\u001b[39m criterion = nn.CrossEntropyLoss()\n",
      "\u001b[31mTypeError\u001b[39m: GNNModel.__init__() got an unexpected keyword argument 'in_channels'"
     ]
    }
   ],
   "source": [
    "\n",
    "# The GNN model remains the same as previous\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, feature_dim=6, hidden_dim=64, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(feature_dim, hidden_dim)\n",
    "        self.conv2 = GATConv(hidden_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = x.float()\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.classifier(x)\n",
    "        return torch.sigmoid(x.squeeze())\n",
    "\n",
    "def train_single_graph():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = SingleGraphDataset(num_samples=1000)\n",
    "    \n",
    "    # Use PyG's DataLoader\n",
    "    from torch_geometric.loader import DataLoader\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Define model\n",
    "    model = GNNModel(in_channels=dataset.node_features.size(1) + 2).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(batch)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, pred = out.max(dim=1)\n",
    "            \n",
    "            # Store for metrics\n",
    "            all_preds.append(pred.cpu())\n",
    "            all_labels.append(batch.y.cpu())\n",
    "            \n",
    "            # Loss and backward\n",
    "            loss = criterion(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        \n",
    "        from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"  Loss = {total_loss/len(loader):.4f}\")\n",
    "        print(f\"  Accuracy = {accuracy:.4f}, Precision = {precision:.4f}\")\n",
    "        print(f\"  Recall = {recall:.4f}, F1 Score = {f1:.4f}\")\n",
    "        print(f\"  Positive predictions: {all_preds.sum().item()}/{len(all_preds)}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_single_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import random\n",
    "\n",
    "class NodeIsolationDataset(Dataset):\n",
    "    def __init__(self,G_nx, num_samples=1000, transform=None, pre_transform=None):\n",
    "        super(NodeIsolationDataset, self).__init__(None, transform, pre_transform)\n",
    "        \n",
    "        # Create base graph\n",
    "        \n",
    "        self.G = G_nx\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        # Create mapping from node names to indices\n",
    "        self.node_list = list(self.G.nodes())\n",
    "        self.node_to_idx = {node: i for i, node in enumerate(self.node_list)}\n",
    "        \n",
    "        # Extract ring information for easier access\n",
    "        self.rings = {}\n",
    "        for node, data in self.G.nodes(data=True):\n",
    "            pr_id = data.get('pr_id')\n",
    "            lr_id = data.get('lr_id')\n",
    "            position = data.get('position')\n",
    "            \n",
    "            if (pr_id, lr_id) not in self.rings:\n",
    "                self.rings[(pr_id, lr_id)] = []\n",
    "            \n",
    "            self.rings[(pr_id, lr_id)].append((node, position))\n",
    "        \n",
    "        # Pre-build edge index\n",
    "        edge_index = []\n",
    "        for src, dst in self.G.edges():\n",
    "            src_idx = self.node_to_idx[src]\n",
    "            dst_idx = self.node_to_idx[dst]\n",
    "            edge_index.append([src_idx, dst_idx])\n",
    "            edge_index.append([dst_idx, src_idx])  # Undirected graph\n",
    "        \n",
    "        self.edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "        \n",
    "        # Pre-build node features (without failure markers)\n",
    "        base_features = []\n",
    "        for node in self.node_list:\n",
    "            data = self.G.nodes[node]\n",
    "            features = [\n",
    "                float(data.get('is_block', False)),\n",
    "                data.get('pr_id', -1),  # Normalize\n",
    "                data.get('lr_id', -1) ,  # Normalize\n",
    "                data.get('position', -1),  # Normalize\n",
    "            ]\n",
    "            base_features.append(features)\n",
    "        \n",
    "        self.base_features = torch.tensor(base_features, dtype=torch.float)\n",
    "    \n",
    "    def len(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def get(self, idx):\n",
    "        # Generate a new sample for each index\n",
    "        # 1. Select a random ring\n",
    "        valid_rings = [(pr_lr, nodes) for pr_lr, nodes in self.rings.items() if len(nodes) >= 3]\n",
    "        \n",
    "        if not valid_rings:\n",
    "            # Fallback if no valid rings\n",
    "            return Data(\n",
    "                x=self.base_features, \n",
    "                edge_index=self.edge_index,\n",
    "                y=torch.zeros(len(self.node_list), dtype=torch.long)\n",
    "            )\n",
    "        \n",
    "        ring_key, ring_nodes = random.choice(valid_rings)\n",
    "        \n",
    "        # 2. Pick two random nodes from the ring to mark as failed\n",
    "        sorted_ring_nodes = sorted(ring_nodes, key=lambda x: x[1])  # Sort by position\n",
    "        \n",
    "        if len(sorted_ring_nodes) < 2:\n",
    "            # Safety check\n",
    "            i, j = 0, 0\n",
    "        else:\n",
    "            i, j = sorted(random.sample(range(len(sorted_ring_nodes)), 2))\n",
    "        \n",
    "        # Get the failed nodes\n",
    "        failed_node1, pos1 = sorted_ring_nodes[i]\n",
    "        failed_node2, pos2 = sorted_ring_nodes[j]\n",
    "        \n",
    "        # Get indices in our node list\n",
    "        failed_idx1 = self.node_to_idx[failed_node1]\n",
    "        failed_idx2 = self.node_to_idx[failed_node2]\n",
    "        \n",
    "        # 3. Create failure marker features (2 additional features)\n",
    "        failure_markers = torch.zeros((len(self.node_list), 2), dtype=torch.float)\n",
    "        failure_markers[failed_idx1, 0] = 1.0  # First failed node\n",
    "        failure_markers[failed_idx2, 1] = 1.0  # Second failed node\n",
    "        \n",
    "        # 4. Combine features\n",
    "        x = torch.cat([self.base_features, failure_markers], dim=1)\n",
    "        \n",
    "        # 5. Create label tensor - nodes between the two failed nodes should be isolated\n",
    "        y = torch.zeros(len(self.node_list), dtype=torch.long)\n",
    "        \n",
    "        # Only if nodes are in the same ring\n",
    "        min_pos = min(pos1, pos2)\n",
    "        max_pos = max(pos1, pos2)\n",
    "        \n",
    "        # Find nodes that should be isolated (in same ring with position between failed nodes)\n",
    "        for node, pos in sorted_ring_nodes:\n",
    "            if node in [failed_node1, failed_node2]:\n",
    "                continue  # Skip failed nodes\n",
    "            \n",
    "            # Check if position is between the failed nodes\n",
    "            if min_pos < pos < max_pos:\n",
    "                # Mark as isolated (target = 1)\n",
    "                node_idx = self.node_to_idx[node]\n",
    "                y[node_idx] = 1\n",
    "        \n",
    "        # 6. Create and return Data object\n",
    "        return Data(\n",
    "            x=x,\n",
    "            edge_index=self.edge_index,\n",
    "            y=y,\n",
    "            failed_nodes=torch.tensor([failed_idx1, failed_idx2], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=64):\n",
    "        super(IsolationGNN, self).__init__()\n",
    "        \n",
    "        # GNN layers\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        \n",
    "        # FIXED: Specify the correct output dimensions and heads\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels, heads=2)\n",
    "        \n",
    "        # FIXED: Match the output dimensions from GAT (hidden_channels * heads)\n",
    "        self.classifier = nn.Linear(hidden_channels * 2, 2)  # Binary classification\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # First layer with ReLU activation\n",
    "        h = torch.relu(self.conv1(x, edge_index))\n",
    "        \n",
    "        # Second layer\n",
    "        h = self.conv2(h, edge_index)  # Output shape: [nodes, heads, hidden_channels]\n",
    "        \n",
    "        # FIXED: Handle output shape from GAT correctly\n",
    "        # If h shape is [nodes, heads, features], reshape to [nodes, heads*features]\n",
    "        if len(h.shape) == 3:\n",
    "            batch_size, heads, features = h.shape\n",
    "            h = h.reshape(batch_size, heads * features)\n",
    "        \n",
    "        # Final classification\n",
    "        out = self.classifier(h)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_isolation_model(G_nx,num_epochs=30, batch_size=32):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = NodeIsolationDataset(G_nx,num_samples=1000)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Create model\n",
    "    model = IsolationGNN(in_channels=dataset.base_features.size(1) + 2).to(device)\n",
    "    \n",
    "    # Optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 5.0]).to(device))  # Weight positive class higher\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(out, batch.y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Get predictions\n",
    "            _, pred = out.max(dim=1)\n",
    "            \n",
    "            # Track predictions and labels for metrics\n",
    "            all_preds.append(pred.cpu())\n",
    "            all_labels.append(batch.y.cpu())\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Combine predictions and labels\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Loss: {total_loss/len(loader):.4f}\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}, Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "        print(f\"  Positive predictions: {all_preds.sum().item()}/{len(all_preds)}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_isolated_nodes(model, graph, failed_node1, failed_node2):\n",
    "    \"\"\"\n",
    "    Predict which nodes should be isolated when two nodes fail\n",
    "    \n",
    "    Args:\n",
    "        model: Trained IsolationGNN model\n",
    "        graph: NetworkX graph with node attributes\n",
    "        failed_node1, failed_node2: Names of the two failed nodes\n",
    "        \n",
    "    Returns:\n",
    "        List of node names predicted to be isolated\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert graph to appropriate format\n",
    "    node_list = list(graph.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # Check if failed nodes exist in the graph\n",
    "    if failed_node1 not in node_to_idx or failed_node2 not in node_to_idx:\n",
    "        print(f\"Error: One or both failed nodes not in graph\")\n",
    "        return []\n",
    "    \n",
    "    # Get indices of failed nodes\n",
    "    failed_idx1 = node_to_idx[failed_node1]\n",
    "    failed_idx2 = node_to_idx[failed_node2]\n",
    "    \n",
    "    # Create edge index\n",
    "    edge_index = []\n",
    "    for u, v in graph.edges():\n",
    "        edge_index.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edge_index.append([node_to_idx[v], node_to_idx[u]])\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "    \n",
    "    # Create node features\n",
    "    base_features = []\n",
    "    for node in node_list:\n",
    "        data = graph.nodes[node]\n",
    "        features = [\n",
    "            float(data.get('is_block', False)),\n",
    "            data.get('pr_id', -1) ,\n",
    "            data.get('lr_id', -1),\n",
    "            data.get('position', -1)\n",
    "        ]\n",
    "        base_features.append(features)\n",
    "    \n",
    "    base_features = torch.tensor(base_features, dtype=torch.float)\n",
    "    \n",
    "    # Create failure markers\n",
    "    failure_markers = torch.zeros((len(node_list), 2), dtype=torch.float)\n",
    "    failure_markers[failed_idx1, 0] = 1.0\n",
    "    failure_markers[failed_idx2, 1] = 1.0\n",
    "    \n",
    "    # Combine features\n",
    "    x = torch.cat([base_features, failure_markers], dim=1)\n",
    "    \n",
    "    # Create data object\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        for i, node_name in enumerate(node_list):  # first 10 nodes\n",
    "            print(f\"{node_name}: {out[i].tolist()}\")\n",
    "        _, pred = out.max(dim=1)\n",
    "    \n",
    "    # Get isolated nodes\n",
    "    isolated_indices = torch.nonzero(pred == 1).squeeze().tolist()\n",
    "    \n",
    "    # Convert to list if single item\n",
    "    if not isinstance(isolated_indices, list):\n",
    "        isolated_indices = [isolated_indices]\n",
    "    \n",
    "    # Convert to node names and exclude failed nodes\n",
    "    isolated_nodes = [node_list[idx] for idx in isolated_indices \n",
    "                    if idx != failed_idx1 and idx != failed_idx2]\n",
    "    \n",
    "    return isolated_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_data = create_dummy_topology_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_nx = build_graph_with_position_features(topo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nms/.local/lib/python3.13/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:\n",
      "  Loss: 0.2472\n",
      "  Accuracy: 0.9827, Precision: 0.0112\n",
      "  Recall: 0.0109, F1 Score: 0.0110\n",
      "  Positive predictions: 2687/310000\n",
      "Epoch 2/30:\n",
      "  Loss: 0.1916\n",
      "  Accuracy: 0.9914, Precision: 0.0000\n",
      "  Recall: 0.0000, F1 Score: 0.0000\n",
      "  Positive predictions: 0/310000\n",
      "Epoch 3/30:\n",
      "  Loss: 0.1767\n",
      "  Accuracy: 0.9918, Precision: 0.0000\n",
      "  Recall: 0.0000, F1 Score: 0.0000\n",
      "  Positive predictions: 0/310000\n",
      "Epoch 4/30:\n",
      "  Loss: 0.1706\n",
      "  Accuracy: 0.9919, Precision: 0.0000\n",
      "  Recall: 0.0000, F1 Score: 0.0000\n",
      "  Positive predictions: 0/310000\n",
      "Epoch 5/30:\n",
      "  Loss: 0.1675\n",
      "  Accuracy: 0.9919, Precision: 0.0000\n",
      "  Recall: 0.0000, F1 Score: 0.0000\n",
      "  Positive predictions: 0/310000\n",
      "Epoch 6/30:\n",
      "  Loss: 0.1652\n",
      "  Accuracy: 0.9915, Precision: 0.0000\n",
      "  Recall: 0.0000, F1 Score: 0.0000\n",
      "  Positive predictions: 0/310000\n",
      "Epoch 7/30:\n",
      "  Loss: 0.1580\n",
      "  Accuracy: 0.9917, Precision: 0.0000\n",
      "  Recall: 0.0000, F1 Score: 0.0000\n",
      "  Positive predictions: 0/310000\n",
      "Epoch 8/30:\n",
      "  Loss: 0.1424\n",
      "  Accuracy: 0.9918, Precision: 0.0000\n",
      "  Recall: 0.0000, F1 Score: 0.0000\n",
      "  Positive predictions: 0/310000\n",
      "Epoch 9/30:\n",
      "  Loss: 0.1278\n",
      "  Accuracy: 0.9908, Precision: 0.0712\n",
      "  Recall: 0.0076, F1 Score: 0.0138\n",
      "  Positive predictions: 281/310000\n",
      "Epoch 10/30:\n",
      "  Loss: 0.1033\n",
      "  Accuracy: 0.9888, Precision: 0.1755\n",
      "  Recall: 0.0881, F1 Score: 0.1173\n",
      "  Positive predictions: 1316/310000\n",
      "Epoch 11/30:\n",
      "  Loss: 0.0825\n",
      "  Accuracy: 0.9880, Precision: 0.2843\n",
      "  Recall: 0.3264, F1 Score: 0.3039\n",
      "  Positive predictions: 2863/310000\n",
      "Epoch 12/30:\n",
      "  Loss: 0.0809\n",
      "  Accuracy: 0.9855, Precision: 0.3031\n",
      "  Recall: 0.5964, F1 Score: 0.4019\n",
      "  Positive predictions: 4989/310000\n",
      "Epoch 13/30:\n",
      "  Loss: 0.0758\n",
      "  Accuracy: 0.9863, Precision: 0.3024\n",
      "  Recall: 0.5635, F1 Score: 0.3936\n",
      "  Positive predictions: 4550/310000\n",
      "Epoch 14/30:\n",
      "  Loss: 0.0764\n",
      "  Accuracy: 0.9858, Precision: 0.3246\n",
      "  Recall: 0.6881, F1 Score: 0.4411\n",
      "  Positive predictions: 5342/310000\n",
      "Epoch 15/30:\n",
      "  Loss: 0.0751\n",
      "  Accuracy: 0.9857, Precision: 0.3220\n",
      "  Recall: 0.6635, F1 Score: 0.4336\n",
      "  Positive predictions: 5254/310000\n",
      "Epoch 16/30:\n",
      "  Loss: 0.0740\n",
      "  Accuracy: 0.9862, Precision: 0.3409\n",
      "  Recall: 0.7379, F1 Score: 0.4663\n",
      "  Positive predictions: 5483/310000\n",
      "Epoch 17/30:\n",
      "  Loss: 0.0752\n",
      "  Accuracy: 0.9863, Precision: 0.3364\n",
      "  Recall: 0.6830, F1 Score: 0.4508\n",
      "  Positive predictions: 5181/310000\n",
      "Epoch 18/30:\n",
      "  Loss: 0.0768\n",
      "  Accuracy: 0.9859, Precision: 0.3458\n",
      "  Recall: 0.7203, F1 Score: 0.4672\n",
      "  Positive predictions: 5556/310000\n",
      "Epoch 19/30:\n",
      "  Loss: 0.0737\n",
      "  Accuracy: 0.9863, Precision: 0.3570\n",
      "  Recall: 0.7683, F1 Score: 0.4875\n",
      "  Positive predictions: 5647/310000\n",
      "Epoch 20/30:\n",
      "  Loss: 0.0710\n",
      "  Accuracy: 0.9869, Precision: 0.3459\n",
      "  Recall: 0.7085, F1 Score: 0.4648\n",
      "  Positive predictions: 5109/310000\n",
      "Epoch 21/30:\n",
      "  Loss: 0.0732\n",
      "  Accuracy: 0.9871, Precision: 0.3556\n",
      "  Recall: 0.6897, F1 Score: 0.4693\n",
      "  Positive predictions: 4963/310000\n",
      "Epoch 22/30:\n",
      "  Loss: 0.0672\n",
      "  Accuracy: 0.9875, Precision: 0.3550\n",
      "  Recall: 0.7288, F1 Score: 0.4775\n",
      "  Positive predictions: 4980/310000\n",
      "Epoch 23/30:\n",
      "  Loss: 0.0714\n",
      "  Accuracy: 0.9871, Precision: 0.3700\n",
      "  Recall: 0.7519, F1 Score: 0.4960\n",
      "  Positive predictions: 5300/310000\n",
      "Epoch 24/30:\n",
      "  Loss: 0.0687\n",
      "  Accuracy: 0.9877, Precision: 0.3971\n",
      "  Recall: 0.7932, F1 Score: 0.5293\n",
      "  Positive predictions: 5399/310000\n",
      "Epoch 25/30:\n",
      "  Loss: 0.0671\n",
      "  Accuracy: 0.9879, Precision: 0.3898\n",
      "  Recall: 0.7833, F1 Score: 0.5206\n",
      "  Positive predictions: 5220/310000\n",
      "Epoch 26/30:\n",
      "  Loss: 0.0673\n",
      "  Accuracy: 0.9881, Precision: 0.4047\n",
      "  Recall: 0.7736, F1 Score: 0.5314\n",
      "  Positive predictions: 5159/310000\n",
      "Epoch 27/30:\n",
      "  Loss: 0.0635\n",
      "  Accuracy: 0.9889, Precision: 0.3965\n",
      "  Recall: 0.7673, F1 Score: 0.5228\n",
      "  Positive predictions: 4742/310000\n",
      "Epoch 28/30:\n",
      "  Loss: 0.0578\n",
      "  Accuracy: 0.9893, Precision: 0.4004\n",
      "  Recall: 0.7677, F1 Score: 0.5263\n",
      "  Positive predictions: 4588/310000\n",
      "Epoch 29/30:\n",
      "  Loss: 0.0651\n",
      "  Accuracy: 0.9892, Precision: 0.4339\n",
      "  Recall: 0.7721, F1 Score: 0.5556\n",
      "  Positive predictions: 4810/310000\n",
      "Epoch 30/30:\n",
      "  Loss: 0.0657\n",
      "  Accuracy: 0.9899, Precision: 0.4469\n",
      "  Recall: 0.7616, F1 Score: 0.5633\n",
      "  Positive predictions: 4540/310000\n"
     ]
    }
   ],
   "source": [
    "model = train_isolation_model(G_nx,num_epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NODE_0_0_0: [2.3151636123657227, -2.1297192573547363]\n",
      "NODE_0_0_1: [2.1137428283691406, -1.9471200704574585]\n",
      "NODE_0_0_2: [1.825444221496582, -1.6798732280731201]\n",
      "NODE_0_0_3: [1.8049228191375732, -1.6564351320266724]\n",
      "NODE_0_0_4: [1.7815285921096802, -1.6319700479507446]\n",
      "NODE_0_0_5: [1.755421757698059, -1.606775164604187]\n",
      "NODE_0_0_6: [1.7244935035705566, -1.5778577327728271]\n",
      "NODE_0_0_7: [1.6938259601593018, -1.5495681762695312]\n",
      "NODE_0_0_8: [1.9507694244384766, -1.7858049869537354]\n",
      "NODE_0_0_9: [2.1780738830566406, -1.9944514036178589]\n",
      "BLOCK_0: [2.6768946647644043, -2.457437753677368]\n",
      "NODE_0_1_0: [2.3206024169921875, -2.137464761734009]\n",
      "NODE_0_1_1: [2.1223325729370117, -1.9592417478561401]\n",
      "NODE_0_1_2: [1.8352240324020386, -1.6932729482650757]\n",
      "NODE_0_1_3: [1.8099979162216187, -1.6647611856460571]\n",
      "NODE_0_1_4: [1.78563392162323, -1.638372778892517]\n",
      "NODE_0_1_5: [1.7629214525222778, -1.6152315139770508]\n",
      "NODE_0_1_6: [1.7371735572814941, -1.590527892112732]\n",
      "NODE_0_1_7: [1.7101103067398071, -1.5653419494628906]\n",
      "NODE_0_1_8: [1.9676573276519775, -1.8020445108413696]\n",
      "NODE_0_1_9: [2.18994140625, -2.005819320678711]\n",
      "NODE_0_2_0: [2.3244829177856445, -2.1439948081970215]\n",
      "NODE_0_2_1: [2.125962734222412, -1.9670885801315308]\n",
      "NODE_0_2_2: [1.8379830121994019, -1.7007147073745728]\n",
      "NODE_0_2_3: [1.809157371520996, -1.6685200929641724]\n",
      "NODE_0_2_4: [1.7843877077102661, -1.6410635709762573]\n",
      "NODE_0_2_5: [1.7620269060134888, -1.617313265800476]\n",
      "NODE_0_2_6: [1.7391356229782104, -1.5945361852645874]\n",
      "NODE_0_2_7: [1.7177776098251343, -1.5741420984268188]\n",
      "NODE_0_2_8: [1.980040431022644, -1.8147928714752197]\n",
      "NODE_0_2_9: [2.200146436691284, -2.015979528427124]\n",
      "NODE_1_0_0: [2.330300807952881, -2.151649236679077]\n",
      "NODE_1_0_1: [2.1268599033355713, -1.9673833847045898]\n",
      "NODE_1_0_2: [0.2719937562942505, -0.3487494885921478]\n",
      "NODE_1_0_3: [-0.7501634359359741, 0.6155253052711487]\n",
      "NODE_1_0_4: [-1.106449007987976, 0.9499883055686951]\n",
      "NODE_1_0_5: [-0.8606311082839966, 0.7178793549537659]\n",
      "NODE_1_0_6: [-0.6369314193725586, 0.5777304768562317]\n",
      "NODE_1_0_7: [0.10337021946907043, -0.08612646162509918]\n",
      "NODE_1_0_8: [0.5569684505462646, -0.5036477446556091]\n",
      "NODE_1_0_9: [2.191932201385498, -2.013307809829712]\n",
      "BLOCK_1: [2.691288471221924, -2.4764115810394287]\n",
      "NODE_1_1_0: [2.3356847763061523, -2.1592180728912354]\n",
      "NODE_1_1_1: [2.1328744888305664, -1.9769526720046997]\n",
      "NODE_1_1_2: [1.8445746898651123, -1.7106126546859741]\n",
      "NODE_1_1_3: [1.8182953596115112, -1.680672526359558]\n",
      "NODE_1_1_4: [1.794145941734314, -1.654021143913269]\n",
      "NODE_1_1_5: [1.7707439661026, -1.6294752359390259]\n",
      "NODE_1_1_6: [1.744994878768921, -1.604232907295227]\n",
      "NODE_1_1_7: [1.7186650037765503, -1.5791608095169067]\n",
      "NODE_1_1_8: [1.9771488904953003, -1.8162685632705688]\n",
      "NODE_1_1_9: [2.20314621925354, -2.024101972579956]\n",
      "NODE_1_2_0: [2.3362109661102295, -2.1625587940216064]\n",
      "NODE_1_2_1: [2.131889820098877, -1.9804025888442993]\n",
      "NODE_1_2_2: [1.843204140663147, -1.714152455329895]\n",
      "NODE_1_2_3: [1.816341519355774, -1.6837188005447388]\n",
      "NODE_1_2_4: [1.792384386062622, -1.6564610004425049]\n",
      "NODE_1_2_5: [1.768381118774414, -1.6304374933242798]\n",
      "NODE_1_2_6: [1.7445169687271118, -1.606060266494751]\n",
      "NODE_1_2_7: [1.7219727039337158, -1.584240436553955]\n",
      "NODE_1_2_8: [1.9855316877365112, -1.825710654258728]\n",
      "NODE_1_2_9: [2.210834503173828, -2.0322721004486084]\n",
      "NODE_2_0_0: [2.346378803253174, -2.174312114715576]\n",
      "NODE_2_0_1: [2.1409997940063477, -1.9882651567459106]\n",
      "NODE_2_0_2: [1.855176568031311, -1.7242165803909302]\n",
      "NODE_2_0_3: [1.8317047357559204, -1.6969057321548462]\n",
      "NODE_2_0_4: [1.8072971105575562, -1.6702097654342651]\n",
      "NODE_2_0_5: [1.781148076057434, -1.6433169841766357]\n",
      "NODE_2_0_6: [1.751621127128601, -1.6144298315048218]\n",
      "NODE_2_0_7: [1.7210851907730103, -1.5854825973510742]\n",
      "NODE_2_0_8: [1.9772263765335083, -1.8203736543655396]\n",
      "NODE_2_0_9: [2.2098913192749023, -2.03576397895813]\n",
      "BLOCK_2: [2.7062761783599854, -2.495871067047119]\n",
      "NODE_2_1_0: [2.3510565757751465, -2.181265354156494]\n",
      "NODE_2_1_1: [2.14453125, -1.9954534769058228]\n",
      "NODE_2_1_2: [1.8561903238296509, -1.7294936180114746]\n",
      "NODE_2_1_3: [1.8303148746490479, -1.6996856927871704]\n",
      "NODE_2_1_4: [1.8059173822402954, -1.6722784042358398]\n",
      "NODE_2_1_5: [1.7808055877685547, -1.6454222202301025]\n",
      "NODE_2_1_6: [1.7543989419937134, -1.6189414262771606]\n",
      "NODE_2_1_7: [1.7277299165725708, -1.5932742357254028]\n",
      "NODE_2_1_8: [1.9876117706298828, -1.8313320875167847]\n",
      "NODE_2_1_9: [2.218485116958618, -2.044490337371826]\n",
      "NODE_2_2_0: [2.3505358695983887, -2.1833934783935547]\n",
      "NODE_2_2_1: [2.1422691345214844, -1.9973951578140259]\n",
      "NODE_2_2_2: [1.8533287048339844, -1.731402039527893]\n",
      "NODE_2_2_3: [1.8280607461929321, -1.7023922204971313]\n",
      "NODE_2_2_4: [1.804052472114563, -1.6747475862503052]\n",
      "NODE_2_2_5: [1.778403401374817, -1.6465263366699219]\n",
      "NODE_2_2_6: [1.753452181816101, -1.6204913854599]\n",
      "NODE_2_2_7: [1.7292566299438477, -1.5967456102371216]\n",
      "NODE_2_2_8: [1.993270754814148, -1.8384103775024414]\n",
      "NODE_2_2_9: [2.2239298820495605, -2.0507419109344482]\n",
      "NODE_3_0_0: [2.362316846847534, -2.196747303009033]\n",
      "NODE_3_0_1: [2.1513924598693848, -2.0055315494537354]\n",
      "NODE_3_0_2: [1.8654621839523315, -1.7421541213989258]\n",
      "NODE_3_0_3: [1.841363787651062, -1.7138136625289917]\n",
      "NODE_3_0_4: [1.818869948387146, -1.6882641315460205]\n",
      "NODE_3_0_5: [1.7931321859359741, -1.661086916923523]\n",
      "NODE_3_0_6: [1.7635167837142944, -1.6317344903945923]\n",
      "NODE_3_0_7: [1.7337069511413574, -1.6029704809188843]\n",
      "NODE_3_0_8: [1.9905263185501099, -1.837965965270996]\n",
      "NODE_3_0_9: [2.2283236980438232, -2.0589303970336914]\n",
      "BLOCK_3: [2.7214155197143555, -2.5153582096099854]\n",
      "NODE_3_1_0: [2.3648617267608643, -2.201846122741699]\n",
      "NODE_3_1_1: [2.153064489364624, -2.0111265182495117]\n",
      "NODE_3_1_2: [1.8656002283096313, -1.7466076612472534]\n",
      "NODE_3_1_3: [1.84120512008667, -1.7180209159851074]\n",
      "NODE_3_1_4: [1.8181042671203613, -1.6911660432815552]\n",
      "NODE_3_1_5: [1.7919604778289795, -1.662782073020935]\n",
      "NODE_3_1_6: [1.765023946762085, -1.6350679397583008]\n",
      "NODE_3_1_7: [1.7378042936325073, -1.6085718870162964]\n",
      "NODE_3_1_8: [1.9982560873031616, -1.8466218709945679]\n",
      "NODE_3_1_9: [2.2343668937683105, -2.0654783248901367]\n",
      "NODE_3_2_0: [2.3658480644226074, -2.2054805755615234]\n",
      "NODE_3_2_1: [2.1533734798431396, -2.0154027938842773]\n",
      "NODE_3_2_2: [1.8637840747833252, -1.749346375465393]\n",
      "NODE_3_2_3: [1.8398504257202148, -1.721309781074524]\n",
      "NODE_3_2_4: [1.8157830238342285, -1.6933950185775757]\n",
      "NODE_3_2_5: [1.7893182039260864, -1.6638721227645874]\n",
      "NODE_3_2_6: [1.763501763343811, -1.636370062828064]\n",
      "NODE_3_2_7: [1.7385573387145996, -1.6113672256469727]\n",
      "NODE_3_2_8: [2.0018692016601562, -1.8519607782363892]\n",
      "NODE_3_2_9: [2.238259792327881, -2.0704619884490967]\n",
      "NODE_4_0_0: [2.3759164810180664, -2.217095136642456]\n",
      "NODE_4_0_1: [2.1604132652282715, -2.021545171737671]\n",
      "NODE_4_0_2: [1.8732486963272095, -1.7578117847442627]\n",
      "NODE_4_0_3: [1.8486968278884888, -1.7289245128631592]\n",
      "NODE_4_0_4: [1.8270150423049927, -1.7035173177719116]\n",
      "NODE_4_0_5: [1.803197979927063, -1.6774522066116333]\n",
      "NODE_4_0_6: [1.7750684022903442, -1.6488500833511353]\n",
      "NODE_4_0_7: [1.7451142072677612, -1.61971914768219]\n",
      "NODE_4_0_8: [2.001962184906006, -1.8542132377624512]\n",
      "NODE_4_0_9: [2.2446393966674805, -2.0805811882019043]\n",
      "BLOCK_4: [2.7353408336639404, -2.5338237285614014]\n",
      "NODE_4_1_0: [2.3777551651000977, -2.221550464630127]\n",
      "NODE_4_1_1: [2.161071300506592, -2.026204824447632]\n",
      "NODE_4_1_2: [1.8727272748947144, -1.7616641521453857]\n",
      "NODE_4_1_3: [1.8498789072036743, -1.734390139579773]\n",
      "NODE_4_1_4: [1.8282660245895386, -1.7085168361663818]\n",
      "NODE_4_1_5: [1.803343653678894, -1.6806285381317139]\n",
      "NODE_4_1_6: [1.7764229774475098, -1.6522709131240845]\n",
      "NODE_4_1_7: [1.7487379312515259, -1.6249080896377563]\n",
      "NODE_4_1_8: [2.0088255405426025, -1.8621745109558105]\n",
      "NODE_4_1_9: [2.2496824264526367, -2.086301803588867]\n",
      "NODE_4_2_0: [2.379460096359253, -2.225900888442993]\n",
      "NODE_4_2_1: [2.1615452766418457, -2.0307369232177734]\n",
      "NODE_4_2_2: [1.872170329093933, -1.7655779123306274]\n",
      "NODE_4_2_3: [1.8497434854507446, -1.738792061805725]\n",
      "NODE_4_2_4: [1.8271764516830444, -1.7120522260665894]\n",
      "NODE_4_2_5: [1.8008474111557007, -1.682069182395935]\n",
      "NODE_4_2_6: [1.7741302251815796, -1.6530438661575317]\n",
      "NODE_4_2_7: [1.748101830482483, -1.626320719718933]\n",
      "NODE_4_2_8: [2.010382890701294, -1.865517020225525]\n",
      "NODE_4_2_9: [2.252131462097168, -2.0898327827453613]\n",
      "NODE_5_0_0: [2.3825325965881348, -2.231416702270508]\n",
      "NODE_5_0_1: [2.1646015644073486, -2.0329856872558594]\n",
      "NODE_5_0_2: [1.8789130449295044, -1.7714687585830688]\n",
      "NODE_5_0_3: [1.856129765510559, -1.744287371635437]\n",
      "NODE_5_0_4: [1.8337010145187378, -1.7177494764328003]\n",
      "NODE_5_0_5: [1.8106449842453003, -1.6916059255599976]\n",
      "NODE_5_0_6: [1.7848032712936401, -1.6643280982971191]\n",
      "NODE_5_0_7: [1.7562116384506226, -1.6360658407211304]\n",
      "NODE_5_0_8: [2.0132391452789307, -1.8702902793884277]\n",
      "NODE_5_0_9: [2.2595162391662598, -2.1013731956481934]\n",
      "BLOCK_5: [2.746291160583496, -2.5497798919677734]\n",
      "NODE_5_1_0: [2.3867247104644775, -2.2380893230438232]\n",
      "NODE_5_1_1: [2.1674013137817383, -2.0396766662597656]\n",
      "NODE_5_1_2: [1.879099726676941, -1.7759889364242554]\n",
      "NODE_5_1_3: [1.8565994501113892, -1.7490848302841187]\n",
      "NODE_5_1_4: [1.8349214792251587, -1.7229342460632324]\n",
      "NODE_5_1_5: [1.8115969896316528, -1.6958515644073486]\n",
      "NODE_5_1_6: [1.7867505550384521, -1.6687626838684082]\n",
      "NODE_5_1_7: [1.7596133947372437, -1.6412891149520874]\n",
      "NODE_5_1_8: [2.0193850994110107, -1.8778733015060425]\n",
      "NODE_5_1_9: [2.2641348838806152, -2.1068248748779297]\n",
      "NODE_5_2_0: [2.39046049118042, -2.2443909645080566]\n",
      "NODE_5_2_1: [2.169783592224121, -2.046048641204834]\n",
      "NODE_5_2_2: [1.8802489042282104, -1.7815622091293335]\n",
      "NODE_5_2_3: [1.8576284646987915, -1.7544554471969604]\n",
      "NODE_5_2_4: [1.8353272676467896, -1.7277752161026]\n",
      "NODE_5_2_5: [1.8111284971237183, -1.699522614479065]\n",
      "NODE_5_2_6: [1.7852768898010254, -1.670751929283142]\n",
      "NODE_5_2_7: [1.7588000297546387, -1.6429685354232788]\n",
      "NODE_5_2_8: [2.0200724601745605, -1.8805148601531982]\n",
      "NODE_5_2_9: [2.265535593032837, -2.1094257831573486]\n",
      "NODE_6_0_0: [2.386965274810791, -2.244208574295044]\n",
      "NODE_6_0_1: [2.166396141052246, -2.0425190925598145]\n",
      "NODE_6_0_2: [1.8829401731491089, -1.7837765216827393]\n",
      "NODE_6_0_3: [1.862328290939331, -1.7584611177444458]\n",
      "NODE_6_0_4: [1.8400355577468872, -1.7318071126937866]\n",
      "NODE_6_0_5: [1.8157659769058228, -1.7040479183197021]\n",
      "NODE_6_0_6: [1.792303442955017, -1.6782218217849731]\n",
      "NODE_6_0_7: [1.7654485702514648, -1.6508982181549072]\n",
      "NODE_6_0_8: [2.023756265640259, -1.8857154846191406]\n",
      "NODE_6_0_9: [2.2722160816192627, -2.1201725006103516]\n",
      "BLOCK_6: [2.754912853240967, -2.5637056827545166]\n",
      "NODE_6_1_0: [2.3906517028808594, -2.2499890327453613]\n",
      "NODE_6_1_1: [2.169919967651367, -2.0495007038116455]\n",
      "NODE_6_1_2: [1.884413719177246, -1.7893098592758179]\n",
      "NODE_6_1_3: [1.8639860153198242, -1.7643886804580688]\n",
      "NODE_6_1_4: [1.8406554460525513, -1.7366631031036377]\n",
      "NODE_6_1_5: [1.817562222480774, -1.7091646194458008]\n",
      "NODE_6_1_6: [1.7943332195281982, -1.682878851890564]\n",
      "NODE_6_1_7: [1.7691344022750854, -1.6563462018966675]\n",
      "NODE_6_1_8: [2.0292396545410156, -1.8928056955337524]\n",
      "NODE_6_1_9: [2.2772607803344727, -2.1260907649993896]\n",
      "NODE_6_2_0: [2.395073413848877, -2.256962299346924]\n",
      "NODE_6_2_1: [2.173344612121582, -2.0568857192993164]\n",
      "NODE_6_2_2: [1.8856028318405151, -1.7949329614639282]\n",
      "NODE_6_2_3: [1.8645297288894653, -1.769295573234558]\n",
      "NODE_6_2_4: [1.8410470485687256, -1.741380214691162]\n",
      "NODE_6_2_5: [1.8183468580245972, -1.7142351865768433]\n",
      "NODE_6_2_6: [1.7945321798324585, -1.6867282390594482]\n",
      "NODE_6_2_7: [1.7691951990127563, -1.659323811531067]\n",
      "NODE_6_2_8: [2.0297577381134033, -1.8955105543136597]\n",
      "NODE_6_2_9: [2.2779324054718018, -2.1281843185424805]\n",
      "NODE_7_0_0: [2.3889458179473877, -2.2549304962158203]\n",
      "NODE_7_0_1: [2.1671130657196045, -2.05124831199646]\n",
      "NODE_7_0_2: [1.8852113485336304, -1.7945556640625]\n",
      "NODE_7_0_3: [1.8671424388885498, -1.7713475227355957]\n",
      "NODE_7_0_4: [1.8456883430480957, -1.745438814163208]\n",
      "NODE_7_0_5: [1.8215118646621704, -1.7174047231674194]\n",
      "NODE_7_0_6: [1.7977250814437866, -1.690807580947876]\n",
      "NODE_7_0_7: [1.7725460529327393, -1.664176344871521]\n",
      "NODE_7_0_8: [2.031695604324341, -1.8991507291793823]\n",
      "NODE_7_0_9: [2.2821922302246094, -2.1365818977355957]\n",
      "BLOCK_7: [2.7620062828063965, -2.576385498046875]\n",
      "NODE_7_1_0: [2.3921852111816406, -2.260047197341919]\n",
      "NODE_7_1_1: [2.1694371700286865, -2.056817054748535]\n",
      "NODE_7_1_2: [1.887018084526062, -1.8002344369888306]\n",
      "NODE_7_1_3: [1.868498682975769, -1.776941180229187]\n",
      "NODE_7_1_4: [1.8466020822525024, -1.7505892515182495]\n",
      "NODE_7_1_5: [1.8224619626998901, -1.7220934629440308]\n",
      "NODE_7_1_6: [1.8000868558883667, -1.695982575416565]\n",
      "NODE_7_1_7: [1.776558756828308, -1.6701422929763794]\n",
      "NODE_7_1_8: [2.0374889373779297, -1.906438946723938]\n",
      "NODE_7_1_9: [2.2870492935180664, -2.1423499584198]\n",
      "NODE_7_2_0: [2.3966259956359863, -2.266695022583008]\n",
      "NODE_7_2_1: [2.1738321781158447, -2.0647921562194824]\n",
      "NODE_7_2_2: [1.889205813407898, -1.8066260814666748]\n",
      "NODE_7_2_3: [1.8698451519012451, -1.782600998878479]\n",
      "NODE_7_2_4: [1.8468950986862183, -1.7552107572555542]\n",
      "NODE_7_2_5: [1.8236278295516968, -1.7274776697158813]\n",
      "NODE_7_2_6: [1.8014427423477173, -1.7009286880493164]\n",
      "NODE_7_2_7: [1.777840495109558, -1.6743316650390625]\n",
      "NODE_7_2_8: [2.03902530670166, -1.910352110862732]\n",
      "NODE_7_2_9: [2.2878284454345703, -2.1447534561157227]\n",
      "NODE_8_0_0: [2.391493320465088, -2.2662782669067383]\n",
      "NODE_8_0_1: [2.1679086685180664, -2.0603229999542236]\n",
      "NODE_8_0_2: [1.8858894109725952, -1.8040567636489868]\n",
      "NODE_8_0_3: [1.8698316812515259, -1.7824358940124512]\n",
      "NODE_8_0_4: [1.8491687774658203, -1.7570337057113647]\n",
      "NODE_8_0_5: [1.8269914388656616, -1.7305089235305786]\n",
      "NODE_8_0_6: [1.8029803037643433, -1.7032638788223267]\n",
      "NODE_8_0_7: [1.7785937786102295, -1.6768845319747925]\n",
      "NODE_8_0_8: [2.0377256870269775, -1.911367416381836]\n",
      "NODE_8_0_9: [2.2884836196899414, -2.150146484375]\n",
      "BLOCK_8: [2.767993450164795, -2.5884313583374023]\n",
      "NODE_8_1_0: [2.394643545150757, -2.2712759971618652]\n",
      "NODE_8_1_1: [2.1695504188537598, -2.0649611949920654]\n",
      "NODE_8_1_2: [1.88697350025177, -1.8087577819824219]\n",
      "NODE_8_1_3: [1.8709943294525146, -1.7875752449035645]\n",
      "NODE_8_1_4: [1.8504793643951416, -1.7625185251235962]\n",
      "NODE_8_1_5: [1.8283013105392456, -1.7359188795089722]\n",
      "NODE_8_1_6: [1.8048957586288452, -1.7083998918533325]\n",
      "NODE_8_1_7: [1.7823714017868042, -1.6828964948654175]\n",
      "NODE_8_1_8: [2.043416738510132, -1.9184346199035645]\n",
      "NODE_8_1_9: [2.2938270568847656, -2.156184673309326]\n",
      "NODE_8_2_0: [2.3986520767211914, -2.277294397354126]\n",
      "NODE_8_2_1: [2.172729253768921, -2.071685791015625]\n",
      "NODE_8_2_2: [1.889926552772522, -1.8157260417938232]\n",
      "NODE_8_2_3: [1.8722662925720215, -1.7931302785873413]\n",
      "NODE_8_2_4: [1.8514667749404907, -1.7678169012069702]\n",
      "NODE_8_2_5: [1.828826665878296, -1.7407171726226807]\n",
      "NODE_8_2_6: [1.8063840866088867, -1.7137995958328247]\n",
      "NODE_8_2_7: [1.7843949794769287, -1.687965750694275]\n",
      "NODE_8_2_8: [2.0459678173065186, -1.9236470460891724]\n",
      "NODE_8_2_9: [2.29565691947937, -2.1595265865325928]\n",
      "NODE_9_0_0: [2.3946852684020996, -2.2783734798431396]\n",
      "NODE_9_0_1: [2.1683428287506104, -2.0692830085754395]\n",
      "NODE_9_0_2: [1.886475682258606, -1.8135343790054321]\n",
      "NODE_9_0_3: [1.8695087432861328, -1.7907236814498901]\n",
      "NODE_9_0_4: [1.8515397310256958, -1.7676082849502563]\n",
      "NODE_9_0_5: [1.830899715423584, -1.7424629926681519]\n",
      "NODE_9_0_6: [1.8088504076004028, -1.7166246175765991]\n",
      "NODE_9_0_7: [1.7842293977737427, -1.6895345449447632]\n",
      "NODE_9_0_8: [2.0430848598480225, -1.9230295419692993]\n",
      "NODE_9_0_9: [2.292583465576172, -2.161637783050537]\n",
      "BLOCK_9: [2.772345542907715, -2.5992324352264404]\n",
      "NODE_9_1_0: [2.398016929626465, -2.2835376262664795]\n",
      "NODE_9_1_1: [2.169811964035034, -2.0737380981445312]\n",
      "NODE_9_1_2: [1.887062668800354, -1.8176647424697876]\n",
      "NODE_9_1_3: [1.870423674583435, -1.7954930067062378]\n",
      "NODE_9_1_4: [1.8520647287368774, -1.7722667455673218]\n",
      "NODE_9_1_5: [1.8319469690322876, -1.7476422786712646]\n",
      "NODE_9_1_6: [1.809980869293213, -1.7212711572647095]\n",
      "NODE_9_1_7: [1.7873166799545288, -1.6951700448989868]\n",
      "NODE_9_1_8: [2.0476176738739014, -1.9293969869613647]\n",
      "NODE_9_1_9: [2.29659366607666, -2.166625499725342]\n",
      "NODE_9_2_0: [2.4018361568450928, -2.289332866668701]\n",
      "NODE_9_2_1: [2.171963691711426, -2.079113006591797]\n",
      "NODE_9_2_2: [1.8892604112625122, -1.8236287832260132]\n",
      "NODE_9_2_3: [1.8727566003799438, -1.801812767982483]\n",
      "NODE_9_2_4: [1.8539594411849976, -1.7784210443496704]\n",
      "NODE_9_2_5: [1.833560585975647, -1.7534860372543335]\n",
      "NODE_9_2_6: [1.8111237287521362, -1.726581335067749]\n",
      "NODE_9_2_7: [1.78917396068573, -1.7003118991851807]\n",
      "NODE_9_2_8: [2.0501770973205566, -1.9348907470703125]\n",
      "NODE_9_2_9: [2.2985658645629883, -2.1701090335845947]\n",
      "\n",
      "Nodes isolated by NODE_1_0_4 and NODE_1_0_6:\n",
      "- NODE_1_0_3\n",
      "- NODE_1_0_5\n"
     ]
    }
   ],
   "source": [
    "failed_node1 = \"NODE_1_0_4\"\n",
    "failed_node2 = \"NODE_1_0_6\"\n",
    "    \n",
    "    # Predict isolated nodes\n",
    "isolated = predict_isolated_nodes(model, G_nx, failed_node1, failed_node2)\n",
    "print(f\"\\nNodes isolated by {failed_node1} and {failed_node2}:\")\n",
    "for node in isolated:\n",
    "        print(f\"- {node}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathBasedIsolationGNN(nn.Module):\n",
    "    \"\"\"GNN that learns to identify nodes on paths between two input nodes\"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels=64):\n",
    "        super(PathBasedIsolationGNN, self).__init__()\n",
    "        \n",
    "        # Node embedding\n",
    "        self.node_encoder = nn.Linear(in_channels, hidden_channels)\n",
    "        \n",
    "        # Message passing layers\n",
    "        self.conv1 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # Path awareness layer\n",
    "        self.path_layer = nn.Linear(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # Output classification\n",
    "        self.classifier = nn.Linear(hidden_channels, 2)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # Initial node features encoding\n",
    "        h = self.node_encoder(x)\n",
    "        \n",
    "        # Message passing to understand graph structure\n",
    "        h1 = F.relu(self.conv1(h, edge_index))\n",
    "        h2 = F.relu(self.conv2(h1, edge_index)) + h1  # Residual connection\n",
    "        h3 = F.relu(self.conv3(h2, edge_index)) + h2  # Residual connection\n",
    "        \n",
    "        # Path awareness\n",
    "        h_path = F.relu(self.path_layer(h3))\n",
    "        \n",
    "        # Final classification\n",
    "        out = self.classifier(h_path)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_based_training_data(G, num_samples=1000):\n",
    "    \"\"\"Create training data focused on graph paths\"\"\"\n",
    "    from torch_geometric.data import Data\n",
    "    from torch_geometric.utils import to_networkx\n",
    "    \n",
    "    data_list = []\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # Create edge index\n",
    "    edge_index = []\n",
    "    for u, v in G.edges():\n",
    "        edge_index.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edge_index.append([node_to_idx[v], node_to_idx[u]])  # Undirected graph\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "    \n",
    "    # Extract ring information\n",
    "    rings = {}\n",
    "    for node, data in G.nodes(data=True):\n",
    "        pr_id = data.get('pr_id')\n",
    "        lr_id = data.get('lr_id')\n",
    "        position = data.get('position')\n",
    "        \n",
    "        if pr_id is not None and lr_id is not None and position is not None:\n",
    "            if (pr_id, lr_id) not in rings:\n",
    "                rings[(pr_id, lr_id)] = []\n",
    "            \n",
    "            rings[(pr_id, lr_id)].append((node, position))\n",
    "    \n",
    "    # Generate samples\n",
    "    for _ in range(num_samples):\n",
    "        # Create base features\n",
    "        base_features = []\n",
    "        for node in node_list:\n",
    "            data = G.nodes[node]\n",
    "            features = [\n",
    "                float(data.get('is_block', False)),\n",
    "                data.get('pr_id', -1) / 10.0,\n",
    "                data.get('lr_id', -1) / 10.0,\n",
    "                data.get('position', -1) / 10.0\n",
    "            ]\n",
    "            base_features.append(features)\n",
    "        \n",
    "        # Select random ring and two nodes\n",
    "        valid_rings = [(k, v) for k, v in rings.items() if len(v) >= 3]\n",
    "        if not valid_rings:\n",
    "            continue\n",
    "            \n",
    "        ring_key, ring_nodes = random.choice(valid_rings)\n",
    "        \n",
    "        # Sort by position\n",
    "        sorted_ring_nodes = sorted(ring_nodes, key=lambda x: x[1])\n",
    "        \n",
    "        # Choose two nodes as endpoints\n",
    "        if len(sorted_ring_nodes) < 2:\n",
    "            continue\n",
    "            \n",
    "        i, j = sorted(random.sample(range(len(sorted_ring_nodes)), 2))\n",
    "        endpoint1, _ = sorted_ring_nodes[i]\n",
    "        endpoint2, _ = sorted_ring_nodes[j]\n",
    "        \n",
    "        # Mark endpoints in features\n",
    "        endpoint_markers = torch.zeros((len(node_list), 2), dtype=torch.float)\n",
    "        endpoint_markers[node_to_idx[endpoint1], 0] = 1.0\n",
    "        endpoint_markers[node_to_idx[endpoint2], 1] = 1.0\n",
    "        \n",
    "        # Combine features\n",
    "        features = torch.tensor(base_features, dtype=torch.float)\n",
    "        x = torch.cat([features, endpoint_markers], dim=1)\n",
    "        \n",
    "        # Find nodes that should be isolated (on path between endpoints in the ring)\n",
    "        path_nodes = []\n",
    "        for idx in range(i+1, j):\n",
    "            node, _ = sorted_ring_nodes[idx]\n",
    "            path_nodes.append(node_to_idx[node])\n",
    "        \n",
    "        # Create labels\n",
    "        y = torch.zeros(len(node_list), dtype=torch.long)\n",
    "        for idx in path_nodes:\n",
    "            y[idx] = 1\n",
    "        \n",
    "        # Create data object with graph structure as primary information\n",
    "        data = Data(x=x, edge_index=edge_index, y=y, \n",
    "                    endpoints=torch.tensor([node_to_idx[endpoint1], node_to_idx[endpoint2]]))\n",
    "        \n",
    "        data_list.append(data)\n",
    "    \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_path_isolation_model(G_nx, num_epochs=30, batch_size=32):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create dataset focused on graph paths\n",
    "    dataset = create_path_based_training_data(G_nx, num_samples=1000)\n",
    "    \n",
    "    # Split train/validation\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create data loaders\n",
    "    from torch_geometric.loader import DataLoader\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "    \n",
    "    # Create model\n",
    "    model = PathBasedIsolationGNN(in_channels=6).to(device)  # 4 node features + 2 endpoint markers\n",
    "    \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 10.0]).to(device))  # Weight positive class more\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    best_model = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds, train_labels = [], []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(batch)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, pred = out.max(dim=1)\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            train_preds.append(pred.cpu())\n",
    "            train_labels.append(batch.y.cpu())\n",
    "            \n",
    "            # Compute loss and backward\n",
    "            loss = criterion(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch)\n",
    "                _, pred = out.max(dim=1)\n",
    "                \n",
    "                val_preds.append(pred.cpu())\n",
    "                val_labels.append(batch.y.cpu())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "        \n",
    "        # Training metrics\n",
    "        train_preds = torch.cat(train_preds)\n",
    "        train_labels = torch.cat(train_labels)\n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        train_prec = precision_score(train_labels, train_preds, zero_division=0)\n",
    "        train_rec = recall_score(train_labels, train_preds, zero_division=0)\n",
    "        train_f1 = f1_score(train_labels, train_preds, zero_division=0)\n",
    "        \n",
    "        # Validation metrics\n",
    "        val_preds = torch.cat(val_preds)\n",
    "        val_labels = torch.cat(val_labels)\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_prec = precision_score(val_labels, val_preds, zero_division=0)\n",
    "        val_rec = recall_score(val_labels, val_preds, zero_division=0)\n",
    "        val_f1 = f1_score(val_labels, val_preds, zero_division=0)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model = model.state_dict().copy()\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train: Loss={total_loss/len(train_loader):.4f}, F1={train_f1:.4f}\")\n",
    "        print(f\"  Train: Precision={train_prec:.4f}, Recall={train_rec:.4f}\")\n",
    "        print(f\"  Val: F1={val_f1:.4f}, Precision={val_prec:.4f}, Recall={val_rec:.4f}\")\n",
    "        print(f\"  Positives: {val_preds.sum().item()}/{len(val_preds)}\")\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_path_isolation(model, G, node1, node2):\n",
    "    \"\"\"Predict nodes on the path between two specified nodes\"\"\"\n",
    "    # Setup\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Convert graph to indices\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # Check if nodes exist\n",
    "    if node1 not in node_to_idx or node2 not in node_to_idx:\n",
    "        print(f\"Error: One or both endpoints not found in graph\")\n",
    "        return []\n",
    "    \n",
    "    # Create edge index\n",
    "    edge_index = []\n",
    "    for u, v in G.edges():\n",
    "        edge_index.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edge_index.append([node_to_idx[v], node_to_idx[u]])\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().to(device)\n",
    "    \n",
    "    # Create node features\n",
    "    base_features = []\n",
    "    for node in node_list:\n",
    "        data = G.nodes[node]\n",
    "        features = [\n",
    "            float(data.get('is_block', False)),\n",
    "            data.get('pr_id', -1) / 10.0,\n",
    "            data.get('lr_id', -1) / 10.0,\n",
    "            data.get('position', -1) / 10.0\n",
    "        ]\n",
    "        base_features.append(features)\n",
    "    \n",
    "    # Create endpoint markers\n",
    "    endpoint_markers = torch.zeros((len(node_list), 2), dtype=torch.float)\n",
    "    endpoint_markers[node_to_idx[node1], 0] = 1.0\n",
    "    endpoint_markers[node_to_idx[node2], 1] = 1.0\n",
    "    \n",
    "    # Combine features\n",
    "    features = torch.tensor(base_features, dtype=torch.float).to(device)\n",
    "    x = torch.cat([features, endpoint_markers.to(device)], dim=1)\n",
    "    \n",
    "    # Create data object\n",
    "    from torch_geometric.data import Data\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        probs = torch.softmax(out, dim=1)\n",
    "        _, pred = out.max(dim=1)\n",
    "    \n",
    "    # Get predicted isolated nodes\n",
    "    isolated_indices = torch.nonzero(pred == 1).squeeze().cpu().tolist()\n",
    "    \n",
    "    # Handle case of single or no result\n",
    "    if not isinstance(isolated_indices, list):\n",
    "        isolated_indices = [isolated_indices] if isolated_indices.numel() > 0 else []\n",
    "    \n",
    "    # Convert to node names\n",
    "    isolated_nodes = [node_list[idx] for idx in isolated_indices \n",
    "                    ]\n",
    "    \n",
    "    return isolated_nodes, out, node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:\n",
      "  Train: Loss=0.4303, F1=0.0208\n",
      "  Train: Precision=0.0115, Recall=0.1088\n",
      "  Val: F1=0.0000, Precision=0.0000, Recall=0.0000\n",
      "  Positives: 0/62000\n",
      "Epoch 2/100:\n",
      "  Train: Loss=0.2829, F1=0.0000\n",
      "  Train: Precision=0.0000, Recall=0.0000\n",
      "  Val: F1=0.0000, Precision=0.0000, Recall=0.0000\n",
      "  Positives: 0/62000\n",
      "Epoch 3/100:\n",
      "  Train: Loss=0.2660, F1=0.0000\n",
      "  Train: Precision=0.0000, Recall=0.0000\n",
      "  Val: F1=0.0000, Precision=0.0000, Recall=0.0000\n",
      "  Positives: 0/62000\n",
      "Epoch 4/100:\n",
      "  Train: Loss=0.2445, F1=0.0000\n",
      "  Train: Precision=0.0000, Recall=0.0000\n",
      "  Val: F1=0.0000, Precision=0.0000, Recall=0.0000\n",
      "  Positives: 0/62000\n",
      "Epoch 5/100:\n",
      "  Train: Loss=0.2031, F1=0.0072\n",
      "  Train: Precision=0.2051, Recall=0.0037\n",
      "  Val: F1=0.1397, Precision=0.3761, Recall=0.0858\n",
      "  Positives: 117/62000\n",
      "Epoch 6/100:\n",
      "  Train: Loss=0.1487, F1=0.3605\n",
      "  Train: Precision=0.2990, Recall=0.4539\n",
      "  Val: F1=0.4321, Precision=0.3219, Recall=0.6569\n",
      "  Positives: 1047/62000\n",
      "Epoch 7/100:\n",
      "  Train: Loss=0.1215, F1=0.4570\n",
      "  Train: Precision=0.3310, Recall=0.7380\n",
      "  Val: F1=0.4650, Precision=0.3251, Recall=0.8168\n",
      "  Positives: 1289/62000\n",
      "Epoch 8/100:\n",
      "  Train: Loss=0.1158, F1=0.4626\n",
      "  Train: Precision=0.3262, Recall=0.7949\n",
      "  Val: F1=0.4434, Precision=0.3043, Recall=0.8168\n",
      "  Positives: 1377/62000\n",
      "Epoch 9/100:\n",
      "  Train: Loss=0.1103, F1=0.4560\n",
      "  Train: Precision=0.3181, Recall=0.8054\n",
      "  Val: F1=0.4415, Precision=0.3025, Recall=0.8168\n",
      "  Positives: 1385/62000\n",
      "Epoch 10/100:\n",
      "  Train: Loss=0.1061, F1=0.4513\n",
      "  Train: Precision=0.3134, Recall=0.8054\n",
      "  Val: F1=0.4344, Precision=0.2959, Recall=0.8168\n",
      "  Positives: 1416/62000\n",
      "Epoch 11/100:\n",
      "  Train: Loss=0.1023, F1=0.4451\n",
      "  Train: Precision=0.3076, Recall=0.8054\n",
      "  Val: F1=0.4397, Precision=0.2980, Recall=0.8382\n",
      "  Positives: 1443/62000\n",
      "Epoch 12/100:\n",
      "  Train: Loss=0.1001, F1=0.4494\n",
      "  Train: Precision=0.3102, Recall=0.8151\n",
      "  Val: F1=0.4450, Precision=0.3027, Recall=0.8402\n",
      "  Positives: 1424/62000\n",
      "Epoch 13/100:\n",
      "  Train: Loss=0.0974, F1=0.4557\n",
      "  Train: Precision=0.3139, Recall=0.8311\n",
      "  Val: F1=0.4515, Precision=0.3120, Recall=0.8168\n",
      "  Positives: 1343/62000\n",
      "Epoch 14/100:\n",
      "  Train: Loss=0.0997, F1=0.4620\n",
      "  Train: Precision=0.3215, Recall=0.8206\n",
      "  Val: F1=0.4439, Precision=0.3006, Recall=0.8480\n",
      "  Positives: 1447/62000\n",
      "Epoch 15/100:\n",
      "  Train: Loss=0.0926, F1=0.4624\n",
      "  Train: Precision=0.3202, Recall=0.8320\n",
      "  Val: F1=0.4501, Precision=0.3063, Recall=0.8480\n",
      "  Positives: 1420/62000\n",
      "Epoch 16/100:\n",
      "  Train: Loss=0.0885, F1=0.4673\n",
      "  Train: Precision=0.3231, Recall=0.8440\n",
      "  Val: F1=0.4725, Precision=0.3278, Recall=0.8460\n",
      "  Positives: 1324/62000\n",
      "Epoch 17/100:\n",
      "  Train: Loss=0.0860, F1=0.4794\n",
      "  Train: Precision=0.3347, Recall=0.8449\n",
      "  Val: F1=0.4621, Precision=0.3098, Recall=0.9084\n",
      "  Positives: 1504/62000\n",
      "Epoch 18/100:\n",
      "  Train: Loss=0.0825, F1=0.4931\n",
      "  Train: Precision=0.3470, Recall=0.8518\n",
      "  Val: F1=0.5061, Precision=0.3607, Recall=0.8480\n",
      "  Positives: 1206/62000\n",
      "Epoch 19/100:\n",
      "  Train: Loss=0.0784, F1=0.5275\n",
      "  Train: Precision=0.3795, Recall=0.8646\n",
      "  Val: F1=0.5063, Precision=0.3531, Recall=0.8947\n",
      "  Positives: 1300/62000\n",
      "Epoch 20/100:\n",
      "  Train: Loss=0.0760, F1=0.5465\n",
      "  Train: Precision=0.3977, Recall=0.8733\n",
      "  Val: F1=0.5373, Precision=0.3808, Recall=0.9123\n",
      "  Positives: 1229/62000\n",
      "Epoch 21/100:\n",
      "  Train: Loss=0.0711, F1=0.5597\n",
      "  Train: Precision=0.4083, Recall=0.8894\n",
      "  Val: F1=0.5622, Precision=0.4167, Recall=0.8635\n",
      "  Positives: 1063/62000\n",
      "Epoch 22/100:\n",
      "  Train: Loss=0.0673, F1=0.5662\n",
      "  Train: Precision=0.4137, Recall=0.8967\n",
      "  Val: F1=0.5670, Precision=0.4167, Recall=0.8869\n",
      "  Positives: 1092/62000\n",
      "Epoch 23/100:\n",
      "  Train: Loss=0.0615, F1=0.5942\n",
      "  Train: Precision=0.4401, Recall=0.9146\n",
      "  Val: F1=0.6020, Precision=0.4447, Recall=0.9318\n",
      "  Positives: 1075/62000\n",
      "Epoch 24/100:\n",
      "  Train: Loss=0.0557, F1=0.6287\n",
      "  Train: Precision=0.4742, Recall=0.9325\n",
      "  Val: F1=0.6557, Precision=0.5169, Recall=0.8967\n",
      "  Positives: 890/62000\n",
      "Epoch 25/100:\n",
      "  Train: Loss=0.0521, F1=0.6612\n",
      "  Train: Precision=0.5137, Recall=0.9275\n",
      "  Val: F1=0.6676, Precision=0.5195, Recall=0.9337\n",
      "  Positives: 922/62000\n",
      "Epoch 26/100:\n",
      "  Train: Loss=0.0474, F1=0.6800\n",
      "  Train: Precision=0.5344, Recall=0.9348\n",
      "  Val: F1=0.6746, Precision=0.5281, Recall=0.9337\n",
      "  Positives: 907/62000\n",
      "Epoch 27/100:\n",
      "  Train: Loss=0.0467, F1=0.7043\n",
      "  Train: Precision=0.5647, Recall=0.9358\n",
      "  Val: F1=0.7003, Precision=0.5602, Recall=0.9337\n",
      "  Positives: 855/62000\n",
      "Epoch 28/100:\n",
      "  Train: Loss=0.0463, F1=0.7129\n",
      "  Train: Precision=0.5774, Recall=0.9316\n",
      "  Val: F1=0.7133, Precision=0.5771, Recall=0.9337\n",
      "  Positives: 830/62000\n",
      "Epoch 29/100:\n",
      "  Train: Loss=0.0441, F1=0.7241\n",
      "  Train: Precision=0.5909, Recall=0.9348\n",
      "  Val: F1=0.7149, Precision=0.5792, Recall=0.9337\n",
      "  Positives: 827/62000\n",
      "Epoch 30/100:\n",
      "  Train: Loss=0.0431, F1=0.7348\n",
      "  Train: Precision=0.6049, Recall=0.9358\n",
      "  Val: F1=0.7258, Precision=0.5936, Recall=0.9337\n",
      "  Positives: 807/62000\n",
      "Epoch 31/100:\n",
      "  Train: Loss=0.0446, F1=0.7406\n",
      "  Train: Precision=0.6129, Recall=0.9358\n",
      "  Val: F1=0.7489, Precision=0.6833, Recall=0.8285\n",
      "  Positives: 622/62000\n",
      "Epoch 32/100:\n",
      "  Train: Loss=0.0443, F1=0.7274\n",
      "  Train: Precision=0.5995, Recall=0.9247\n",
      "  Val: F1=0.7214, Precision=0.5877, Recall=0.9337\n",
      "  Positives: 815/62000\n",
      "Epoch 33/100:\n",
      "  Train: Loss=0.0407, F1=0.7480\n",
      "  Train: Precision=0.6254, Recall=0.9302\n",
      "  Val: F1=0.7117, Precision=0.5750, Recall=0.9337\n",
      "  Positives: 833/62000\n",
      "Epoch 34/100:\n",
      "  Train: Loss=0.0407, F1=0.7454\n",
      "  Train: Precision=0.6196, Recall=0.9353\n",
      "  Val: F1=0.7347, Precision=0.6056, Recall=0.9337\n",
      "  Positives: 791/62000\n",
      "Epoch 35/100:\n",
      "  Train: Loss=0.0402, F1=0.7515\n",
      "  Train: Precision=0.6293, Recall=0.9325\n",
      "  Val: F1=0.7403, Precision=0.6133, Recall=0.9337\n",
      "  Positives: 781/62000\n",
      "Epoch 36/100:\n",
      "  Train: Loss=0.0394, F1=0.7599\n",
      "  Train: Precision=0.6399, Recall=0.9353\n",
      "  Val: F1=0.7751, Precision=0.6625, Recall=0.9337\n",
      "  Positives: 723/62000\n",
      "Epoch 37/100:\n",
      "  Train: Loss=0.0409, F1=0.7522\n",
      "  Train: Precision=0.6328, Recall=0.9270\n",
      "  Val: F1=0.7965, Precision=0.7293, Recall=0.8772\n",
      "  Positives: 617/62000\n",
      "Epoch 38/100:\n",
      "  Train: Loss=0.0420, F1=0.7564\n",
      "  Train: Precision=0.6402, Recall=0.9243\n",
      "  Val: F1=0.7807, Precision=0.6728, Recall=0.9298\n",
      "  Positives: 709/62000\n",
      "Epoch 39/100:\n",
      "  Train: Loss=0.0417, F1=0.7578\n",
      "  Train: Precision=0.6426, Recall=0.9234\n",
      "  Val: F1=0.7467, Precision=0.6221, Recall=0.9337\n",
      "  Positives: 770/62000\n",
      "Epoch 40/100:\n",
      "  Train: Loss=0.0377, F1=0.7656\n",
      "  Train: Precision=0.6502, Recall=0.9307\n",
      "  Val: F1=0.7726, Precision=0.6589, Recall=0.9337\n",
      "  Positives: 727/62000\n",
      "Epoch 41/100:\n",
      "  Train: Loss=0.0412, F1=0.7665\n",
      "  Train: Precision=0.6536, Recall=0.9266\n",
      "  Val: F1=0.7029, Precision=0.5635, Recall=0.9337\n",
      "  Positives: 850/62000\n",
      "Epoch 42/100:\n",
      "  Train: Loss=0.0365, F1=0.7729\n",
      "  Train: Precision=0.6608, Recall=0.9307\n",
      "  Val: F1=0.7490, Precision=0.6253, Recall=0.9337\n",
      "  Positives: 766/62000\n",
      "Epoch 43/100:\n",
      "  Train: Loss=0.0367, F1=0.7757\n",
      "  Train: Precision=0.6649, Recall=0.9307\n",
      "  Val: F1=0.7358, Precision=0.6071, Recall=0.9337\n",
      "  Positives: 789/62000\n",
      "Epoch 44/100:\n",
      "  Train: Loss=0.0366, F1=0.7762\n",
      "  Train: Precision=0.6643, Recall=0.9335\n",
      "  Val: F1=0.7967, Precision=0.6979, Recall=0.9279\n",
      "  Positives: 682/62000\n",
      "Epoch 45/100:\n",
      "  Train: Loss=0.0362, F1=0.7822\n",
      "  Train: Precision=0.6739, Recall=0.9321\n",
      "  Val: F1=0.7486, Precision=0.6257, Recall=0.9318\n",
      "  Positives: 764/62000\n",
      "Epoch 46/100:\n",
      "  Train: Loss=0.0357, F1=0.7880\n",
      "  Train: Precision=0.6820, Recall=0.9330\n",
      "  Val: F1=0.7341, Precision=0.6048, Recall=0.9337\n",
      "  Positives: 792/62000\n",
      "Epoch 47/100:\n",
      "  Train: Loss=0.0366, F1=0.7825\n",
      "  Train: Precision=0.6769, Recall=0.9270\n",
      "  Val: F1=0.7585, Precision=0.6387, Recall=0.9337\n",
      "  Positives: 750/62000\n",
      "Epoch 48/100:\n",
      "  Train: Loss=0.0347, F1=0.7920\n",
      "  Train: Precision=0.6890, Recall=0.9312\n",
      "  Val: F1=0.7421, Precision=0.6157, Recall=0.9337\n",
      "  Positives: 778/62000\n",
      "Epoch 49/100:\n",
      "  Train: Loss=0.0337, F1=0.7927\n",
      "  Train: Precision=0.6898, Recall=0.9316\n",
      "  Val: F1=0.7681, Precision=0.6592, Recall=0.9201\n",
      "  Positives: 716/62000\n",
      "Epoch 50/100:\n",
      "  Train: Loss=0.0384, F1=0.7884\n",
      "  Train: Precision=0.6851, Recall=0.9284\n",
      "  Val: F1=0.8256, Precision=0.7500, Recall=0.9181\n",
      "  Positives: 628/62000\n",
      "Epoch 51/100:\n",
      "  Train: Loss=0.0351, F1=0.8036\n",
      "  Train: Precision=0.7062, Recall=0.9321\n",
      "  Val: F1=0.7432, Precision=0.6173, Recall=0.9337\n",
      "  Positives: 776/62000\n",
      "Epoch 52/100:\n",
      "  Train: Loss=0.0353, F1=0.7966\n",
      "  Train: Precision=0.6966, Recall=0.9302\n",
      "  Val: F1=0.7091, Precision=0.5716, Recall=0.9337\n",
      "  Positives: 838/62000\n",
      "Epoch 53/100:\n",
      "  Train: Loss=0.0362, F1=0.7903\n",
      "  Train: Precision=0.6864, Recall=0.9312\n",
      "  Val: F1=0.7667, Precision=0.6543, Recall=0.9259\n",
      "  Positives: 726/62000\n",
      "Epoch 54/100:\n",
      "  Train: Loss=0.0356, F1=0.8102\n",
      "  Train: Precision=0.7175, Recall=0.9302\n",
      "  Val: F1=0.7520, Precision=0.6294, Recall=0.9337\n",
      "  Positives: 761/62000\n",
      "Epoch 55/100:\n",
      "  Train: Loss=0.0359, F1=0.7998\n",
      "  Train: Precision=0.7024, Recall=0.9284\n",
      "  Val: F1=0.7651, Precision=0.6529, Recall=0.9240\n",
      "  Positives: 726/62000\n",
      "Epoch 56/100:\n",
      "  Train: Loss=0.0336, F1=0.8139\n",
      "  Train: Precision=0.7234, Recall=0.9302\n",
      "  Val: F1=0.7247, Precision=0.5921, Recall=0.9337\n",
      "  Positives: 809/62000\n",
      "Epoch 57/100:\n",
      "  Train: Loss=0.0337, F1=0.8081\n",
      "  Train: Precision=0.7142, Recall=0.9302\n",
      "  Val: F1=0.8433, Precision=0.7798, Recall=0.9181\n",
      "  Positives: 604/62000\n",
      "Epoch 58/100:\n",
      "  Train: Loss=0.0331, F1=0.8052\n",
      "  Train: Precision=0.7119, Recall=0.9266\n",
      "  Val: F1=0.7726, Precision=0.6589, Recall=0.9337\n",
      "  Positives: 727/62000\n",
      "Epoch 59/100:\n",
      "  Train: Loss=0.0334, F1=0.8167\n",
      "  Train: Precision=0.7302, Recall=0.9266\n",
      "  Val: F1=0.7751, Precision=0.6625, Recall=0.9337\n",
      "  Positives: 723/62000\n",
      "Epoch 60/100:\n",
      "  Train: Loss=0.0332, F1=0.8032\n",
      "  Train: Precision=0.7062, Recall=0.9312\n",
      "  Val: F1=0.7758, Precision=0.6685, Recall=0.9240\n",
      "  Positives: 709/62000\n",
      "Epoch 61/100:\n",
      "  Train: Loss=0.0331, F1=0.8235\n",
      "  Train: Precision=0.7381, Recall=0.9312\n",
      "  Val: F1=0.8117, Precision=0.7262, Recall=0.9201\n",
      "  Positives: 650/62000\n",
      "Epoch 62/100:\n",
      "  Train: Loss=0.0334, F1=0.8230\n",
      "  Train: Precision=0.7412, Recall=0.9252\n",
      "  Val: F1=0.8779, Precision=0.8411, Recall=0.9181\n",
      "  Positives: 560/62000\n",
      "Epoch 63/100:\n",
      "  Train: Loss=0.0344, F1=0.8033\n",
      "  Train: Precision=0.7073, Recall=0.9293\n",
      "  Val: F1=0.7745, Precision=0.6667, Recall=0.9240\n",
      "  Positives: 711/62000\n",
      "Epoch 64/100:\n",
      "  Train: Loss=0.0328, F1=0.8141\n",
      "  Train: Precision=0.7235, Recall=0.9307\n",
      "  Val: F1=0.8989, Precision=0.8804, Recall=0.9181\n",
      "  Positives: 535/62000\n",
      "Epoch 65/100:\n",
      "  Train: Loss=0.0330, F1=0.8277\n",
      "  Train: Precision=0.7470, Recall=0.9279\n",
      "  Val: F1=0.8336, Precision=0.7634, Recall=0.9181\n",
      "  Positives: 617/62000\n",
      "Epoch 66/100:\n",
      "  Train: Loss=0.0325, F1=0.8187\n",
      "  Train: Precision=0.7308, Recall=0.9307\n",
      "  Val: F1=0.8584, Precision=0.8031, Recall=0.9220\n",
      "  Positives: 589/62000\n",
      "Epoch 67/100:\n",
      "  Train: Loss=0.0328, F1=0.8218\n",
      "  Train: Precision=0.7366, Recall=0.9293\n",
      "  Val: F1=0.8626, Precision=0.8135, Recall=0.9181\n",
      "  Positives: 579/62000\n",
      "Epoch 68/100:\n",
      "  Train: Loss=0.0324, F1=0.8227\n",
      "  Train: Precision=0.7386, Recall=0.9284\n",
      "  Val: F1=0.8763, Precision=0.8381, Recall=0.9181\n",
      "  Positives: 562/62000\n",
      "Epoch 69/100:\n",
      "  Train: Loss=0.0321, F1=0.8246\n",
      "  Train: Precision=0.7420, Recall=0.9279\n",
      "  Val: F1=0.8548, Precision=0.7997, Recall=0.9181\n",
      "  Positives: 589/62000\n",
      "Epoch 70/100:\n",
      "  Train: Loss=0.0322, F1=0.8291\n",
      "  Train: Precision=0.7482, Recall=0.9298\n",
      "  Val: F1=0.8920, Precision=0.8674, Recall=0.9181\n",
      "  Positives: 543/62000\n",
      "Epoch 71/100:\n",
      "  Train: Loss=0.0314, F1=0.8333\n",
      "  Train: Precision=0.7555, Recall=0.9289\n",
      "  Val: F1=0.8200, Precision=0.7346, Recall=0.9279\n",
      "  Positives: 648/62000\n",
      "Epoch 72/100:\n",
      "  Train: Loss=0.0314, F1=0.8230\n",
      "  Train: Precision=0.7385, Recall=0.9293\n",
      "  Val: F1=0.7926, Precision=0.6940, Recall=0.9240\n",
      "  Positives: 683/62000\n",
      "Epoch 73/100:\n",
      "  Train: Loss=0.0322, F1=0.8387\n",
      "  Train: Precision=0.7640, Recall=0.9298\n",
      "  Val: F1=0.7738, Precision=0.6607, Recall=0.9337\n",
      "  Positives: 725/62000\n",
      "Epoch 74/100:\n",
      "  Train: Loss=0.0323, F1=0.8461\n",
      "  Train: Precision=0.7755, Recall=0.9307\n",
      "  Val: F1=0.8650, Precision=0.8177, Recall=0.9181\n",
      "  Positives: 576/62000\n",
      "Epoch 75/100:\n",
      "  Train: Loss=0.0318, F1=0.8361\n",
      "  Train: Precision=0.7590, Recall=0.9307\n",
      "  Val: F1=0.8690, Precision=0.8249, Recall=0.9181\n",
      "  Positives: 571/62000\n",
      "Epoch 76/100:\n",
      "  Train: Loss=0.0317, F1=0.8424\n",
      "  Train: Precision=0.7691, Recall=0.9312\n",
      "  Val: F1=0.8178, Precision=0.7287, Recall=0.9318\n",
      "  Positives: 656/62000\n",
      "Epoch 77/100:\n",
      "  Train: Loss=0.0346, F1=0.8407\n",
      "  Train: Precision=0.7685, Recall=0.9279\n",
      "  Val: F1=0.8841, Precision=0.8492, Recall=0.9220\n",
      "  Positives: 557/62000\n",
      "Epoch 78/100:\n",
      "  Train: Loss=0.0313, F1=0.8336\n",
      "  Train: Precision=0.7552, Recall=0.9302\n",
      "  Val: F1=0.8989, Precision=0.8804, Recall=0.9181\n",
      "  Positives: 535/62000\n",
      "Epoch 79/100:\n",
      "  Train: Loss=0.0323, F1=0.8339\n",
      "  Train: Precision=0.7568, Recall=0.9284\n",
      "  Val: F1=0.8419, Precision=0.7732, Recall=0.9240\n",
      "  Positives: 613/62000\n",
      "Epoch 80/100:\n",
      "  Train: Loss=0.0340, F1=0.8448\n",
      "  Train: Precision=0.7760, Recall=0.9270\n",
      "  Val: F1=0.8396, Precision=0.7734, Recall=0.9181\n",
      "  Positives: 609/62000\n",
      "Epoch 81/100:\n",
      "  Train: Loss=0.0306, F1=0.8420\n",
      "  Train: Precision=0.7697, Recall=0.9293\n",
      "  Val: F1=0.8071, Precision=0.7130, Recall=0.9298\n",
      "  Positives: 669/62000\n",
      "Epoch 82/100:\n",
      "  Train: Loss=0.0308, F1=0.8497\n",
      "  Train: Precision=0.7822, Recall=0.9298\n",
      "  Val: F1=0.8749, Precision=0.8339, Recall=0.9201\n",
      "  Positives: 566/62000\n",
      "Epoch 83/100:\n",
      "  Train: Loss=0.0310, F1=0.8457\n",
      "  Train: Precision=0.7740, Recall=0.9321\n",
      "  Val: F1=0.9023, Precision=0.8870, Recall=0.9181\n",
      "  Positives: 531/62000\n",
      "Epoch 84/100:\n",
      "  Train: Loss=0.0316, F1=0.8482\n",
      "  Train: Precision=0.7817, Recall=0.9270\n",
      "  Val: F1=0.8050, Precision=0.7075, Recall=0.9337\n",
      "  Positives: 677/62000\n",
      "Epoch 85/100:\n",
      "  Train: Loss=0.0301, F1=0.8469\n",
      "  Train: Precision=0.7772, Recall=0.9302\n",
      "  Val: F1=0.8112, Precision=0.7171, Recall=0.9337\n",
      "  Positives: 668/62000\n",
      "Epoch 86/100:\n",
      "  Train: Loss=0.0329, F1=0.8464\n",
      "  Train: Precision=0.7761, Recall=0.9307\n",
      "  Val: F1=0.8071, Precision=0.7107, Recall=0.9337\n",
      "  Positives: 674/62000\n",
      "Epoch 87/100:\n",
      "  Train: Loss=0.0310, F1=0.8588\n",
      "  Train: Precision=0.7982, Recall=0.9293\n",
      "  Val: F1=0.8457, Precision=0.7756, Recall=0.9298\n",
      "  Positives: 615/62000\n",
      "Epoch 88/100:\n",
      "  Train: Loss=0.0307, F1=0.8523\n",
      "  Train: Precision=0.7851, Recall=0.9321\n",
      "  Val: F1=0.8105, Precision=0.7160, Recall=0.9337\n",
      "  Positives: 669/62000\n",
      "Epoch 89/100:\n",
      "  Train: Loss=0.0310, F1=0.8558\n",
      "  Train: Precision=0.7917, Recall=0.9312\n",
      "  Val: F1=0.9217, Precision=0.9253, Recall=0.9181\n",
      "  Positives: 509/62000\n",
      "Epoch 90/100:\n",
      "  Train: Loss=0.0311, F1=0.8467\n",
      "  Train: Precision=0.7776, Recall=0.9293\n",
      "  Val: F1=0.7977, Precision=0.6962, Recall=0.9337\n",
      "  Positives: 688/62000\n",
      "Epoch 91/100:\n",
      "  Train: Loss=0.0320, F1=0.8576\n",
      "  Train: Precision=0.7958, Recall=0.9298\n",
      "  Val: F1=0.8518, Precision=0.7858, Recall=0.9298\n",
      "  Positives: 607/62000\n",
      "Epoch 92/100:\n",
      "  Train: Loss=0.0310, F1=0.8526\n",
      "  Train: Precision=0.7869, Recall=0.9302\n",
      "  Val: F1=0.9039, Precision=0.9505, Recall=0.8616\n",
      "  Positives: 465/62000\n",
      "Epoch 93/100:\n",
      "  Train: Loss=0.0388, F1=0.8351\n",
      "  Train: Precision=0.7752, Recall=0.9050\n",
      "  Val: F1=0.8529, Precision=0.8070, Recall=0.9045\n",
      "  Positives: 575/62000\n",
      "Epoch 94/100:\n",
      "  Train: Loss=0.0338, F1=0.8148\n",
      "  Train: Precision=0.7251, Recall=0.9298\n",
      "  Val: F1=0.8395, Precision=0.7665, Recall=0.9279\n",
      "  Positives: 621/62000\n",
      "Epoch 95/100:\n",
      "  Train: Loss=0.0313, F1=0.8650\n",
      "  Train: Precision=0.8080, Recall=0.9307\n",
      "  Val: F1=0.7950, Precision=0.6922, Recall=0.9337\n",
      "  Positives: 692/62000\n",
      "Epoch 96/100:\n",
      "  Train: Loss=0.0315, F1=0.8492\n",
      "  Train: Precision=0.7802, Recall=0.9316\n",
      "  Val: F1=0.9023, Precision=0.8870, Recall=0.9181\n",
      "  Positives: 531/62000\n",
      "Epoch 97/100:\n",
      "  Train: Loss=0.0313, F1=0.8599\n",
      "  Train: Precision=0.7988, Recall=0.9312\n",
      "  Val: F1=0.8678, Precision=0.8151, Recall=0.9279\n",
      "  Positives: 584/62000\n",
      "Epoch 98/100:\n",
      "  Train: Loss=0.0316, F1=0.8612\n",
      "  Train: Precision=0.8031, Recall=0.9284\n",
      "  Val: F1=0.8003, Precision=0.7003, Recall=0.9337\n",
      "  Positives: 684/62000\n",
      "Epoch 99/100:\n",
      "  Train: Loss=0.0309, F1=0.8573\n",
      "  Train: Precision=0.7949, Recall=0.9302\n",
      "  Val: F1=0.9223, Precision=0.9306, Recall=0.9142\n",
      "  Positives: 504/62000\n",
      "Epoch 100/100:\n",
      "  Train: Loss=0.0330, F1=0.8467\n",
      "  Train: Precision=0.7814, Recall=0.9238\n",
      "  Val: F1=0.7846, Precision=0.6766, Recall=0.9337\n",
      "  Positives: 708/62000\n"
     ]
    }
   ],
   "source": [
    "model2 = train_path_isolation_model(G_nx, num_epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node1 = \"NODE_1_0_4\"\n",
    "node2 = \"NODE_1_0_5\"\n",
    "isolated,out,node_list = predict_path_isolation(model2, G_nx, node1, node2)\n",
    "isolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NODE_5_0_3', 'NODE_5_0_5', 'NODE_5_0_7']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NODE_3_0_3', 'NODE_3_0_5', 'NODE_3_0_7']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NODE_0_0_0: [3.255542039871216, -2.4851021766662598]\n",
      "NODE_0_0_1: [3.2551181316375732, -2.7131824493408203]\n",
      "NODE_0_0_2: [4.006722450256348, -3.4877960681915283]\n",
      "NODE_0_0_3: [2.879310369491577, -2.453397274017334]\n",
      "NODE_0_0_4: [2.2853047847747803, -1.882545828819275]\n",
      "NODE_0_0_5: [2.141958475112915, -1.7502539157867432]\n",
      "NODE_0_0_6: [2.580329656600952, -2.165238857269287]\n",
      "NODE_0_0_7: [4.0471510887146, -3.5342583656311035]\n",
      "NODE_0_0_8: [3.4005801677703857, -2.8939390182495117]\n",
      "NODE_0_0_9: [3.3036420345306396, -2.563901901245117]\n",
      "BLOCK_0: [1.7650765180587769, -0.8555459976196289]\n",
      "NODE_0_1_0: [3.281153440475464, -2.5047178268432617]\n",
      "NODE_0_1_1: [3.3073740005493164, -2.7558741569519043]\n",
      "NODE_0_1_2: [4.066165447235107, -3.5339479446411133]\n",
      "NODE_0_1_3: [2.893030881881714, -2.454338550567627]\n",
      "NODE_0_1_4: [2.3501172065734863, -1.93293035030365]\n",
      "NODE_0_1_5: [2.2769405841827393, -1.866085410118103]\n",
      "NODE_0_1_6: [2.7471938133239746, -2.3133931159973145]\n",
      "NODE_0_1_7: [4.133645057678223, -3.6034388542175293]\n",
      "NODE_0_1_8: [3.4648945331573486, -2.942474842071533]\n",
      "NODE_0_1_9: [3.3407657146453857, -2.5913360118865967]\n",
      "NODE_0_2_0: [3.307530641555786, -2.524977684020996]\n",
      "NODE_0_2_1: [3.3604536056518555, -2.7993416786193848]\n",
      "NODE_0_2_2: [4.124817848205566, -3.579464912414551]\n",
      "NODE_0_2_3: [2.90677809715271, -2.455159902572632]\n",
      "NODE_0_2_4: [2.4173367023468018, -1.9857043027877808]\n",
      "NODE_0_2_5: [2.415104866027832, -1.984924077987671]\n",
      "NODE_0_2_6: [2.9169561862945557, -2.4643242359161377]\n",
      "NODE_0_2_7: [4.220978736877441, -3.672970771789551]\n",
      "NODE_0_2_8: [3.529388666152954, -2.99112606048584]\n",
      "NODE_0_2_9: [3.3778905868530273, -2.6187703609466553]\n",
      "NODE_1_0_0: [3.3025755882263184, -2.525176763534546]\n",
      "NODE_1_0_1: [2.9419443607330322, -2.5087013244628906]\n",
      "NODE_1_0_2: [1.1105109453201294, -0.7522238492965698]\n",
      "NODE_1_0_3: [-1.5732306241989136, 1.6646358966827393]\n",
      "NODE_1_0_4: [0.722102701663971, -0.41896089911460876]\n",
      "NODE_1_0_5: [-1.0848461389541626, 1.1978315114974976]\n",
      "NODE_1_0_6: [0.6897916793823242, -0.3906174898147583]\n",
      "NODE_1_0_7: [-0.19047504663467407, 0.3657025992870331]\n",
      "NODE_1_0_8: [1.6913129091262817, -1.319128394126892]\n",
      "NODE_1_0_9: [3.045305013656616, -2.4176435470581055]\n",
      "BLOCK_1: [1.7697112560272217, -0.8570371270179749]\n",
      "NODE_1_1_0: [3.3281195163726807, -2.5447239875793457]\n",
      "NODE_1_1_1: [3.357320547103882, -2.8004584312438965]\n",
      "NODE_1_1_2: [4.134718418121338, -3.594003200531006]\n",
      "NODE_1_1_3: [2.9409985542297363, -2.494670867919922]\n",
      "NODE_1_1_4: [2.4048614501953125, -1.9798266887664795]\n",
      "NODE_1_1_5: [2.325734853744507, -1.9076812267303467]\n",
      "NODE_1_1_6: [2.7919371128082275, -2.3508872985839844]\n",
      "NODE_1_1_7: [4.200789928436279, -3.662334680557251]\n",
      "NODE_1_1_8: [3.5190751552581787, -2.992077589035034]\n",
      "NODE_1_1_9: [3.393838882446289, -2.6377756595611572]\n",
      "NODE_1_2_0: [3.3554930686950684, -2.5659565925598145]\n",
      "NODE_1_2_1: [3.411287307739258, -2.844820499420166]\n",
      "NODE_1_2_2: [4.193972110748291, -3.6400034427642822]\n",
      "NODE_1_2_3: [2.955911874771118, -2.496724843978882]\n",
      "NODE_1_2_4: [2.4718940258026123, -2.0324134826660156]\n",
      "NODE_1_2_5: [2.4658894538879395, -2.028276205062866]\n",
      "NODE_1_2_6: [2.9615542888641357, -2.5017290115356445]\n",
      "NODE_1_2_7: [4.287848472595215, -3.73203706741333]\n",
      "NODE_1_2_8: [3.5833895206451416, -3.0406126976013184]\n",
      "NODE_1_2_9: [3.430962562561035, -2.6652095317840576]\n",
      "NODE_2_0_0: [3.3494036197662354, -2.5651001930236816]\n",
      "NODE_2_0_1: [3.354605197906494, -2.8019068241119385]\n",
      "NODE_2_0_2: [4.144442558288574, -3.6084723472595215]\n",
      "NODE_2_0_3: [2.987699270248413, -2.5457959175109863]\n",
      "NODE_2_0_4: [2.4170498847961426, -1.9977186918258667]\n",
      "NODE_2_0_5: [2.272320032119751, -1.8638662099838257]\n",
      "NODE_2_0_6: [2.68571400642395, -2.255495071411133]\n",
      "NODE_2_0_7: [4.183359146118164, -3.6535561084747314]\n",
      "NODE_2_0_8: [3.509221315383911, -2.992868423461914]\n",
      "NODE_2_0_9: [3.409968614578247, -2.6568498611450195]\n",
      "BLOCK_2: [1.7747362852096558, -0.8591124415397644]\n",
      "NODE_2_1_0: [3.376192569732666, -2.5859696865081787]\n",
      "NODE_2_1_1: [3.4084959030151367, -2.846287250518799]\n",
      "NODE_2_1_2: [4.20343542098999, -3.6542320251464844]\n",
      "NODE_2_1_3: [2.9950549602508545, -2.5407583713531494]\n",
      "NODE_2_1_4: [2.464860677719116, -2.031803607940674]\n",
      "NODE_2_1_5: [2.3780860900878906, -1.9525680541992188]\n",
      "NODE_2_1_6: [2.837953805923462, -2.389617681503296]\n",
      "NODE_2_1_7: [4.268435478210449, -3.7215914726257324]\n",
      "NODE_2_1_8: [3.5733766555786133, -3.0415611267089844]\n",
      "NODE_2_1_9: [3.4469008445739746, -2.6841864585876465]\n",
      "NODE_2_2_0: [3.4067957401275635, -2.610569953918457]\n",
      "NODE_2_2_1: [3.4661459922790527, -2.894414186477661]\n",
      "NODE_2_2_2: [4.262004852294922, -3.6996331214904785]\n",
      "NODE_2_2_3: [3.0079808235168457, -2.5409481525421143]\n",
      "NODE_2_2_4: [2.5296146869659424, -2.0821893215179443]\n",
      "NODE_2_2_5: [2.5168344974517822, -2.0718564987182617]\n",
      "NODE_2_2_6: [3.007115602493286, -2.540020704269409]\n",
      "NODE_2_2_7: [4.3551812171936035, -3.791045904159546]\n",
      "NODE_2_2_8: [3.6375722885131836, -3.0902254581451416]\n",
      "NODE_2_2_9: [3.4836199283599854, -2.7114570140838623]\n",
      "NODE_3_0_0: [3.39786696434021, -2.6068687438964844]\n",
      "NODE_3_0_1: [3.40636944770813, -2.8483612537384033]\n",
      "NODE_3_0_2: [4.213037014007568, -3.6685945987701416]\n",
      "NODE_3_0_3: [3.045442581176758, -2.595367431640625]\n",
      "NODE_3_0_4: [2.487502336502075, -2.059746503829956]\n",
      "NODE_3_0_5: [2.345059394836426, -1.9276684522628784]\n",
      "NODE_3_0_6: [2.742600202560425, -2.304678440093994]\n",
      "NODE_3_0_7: [4.252617835998535, -3.7141737937927246]\n",
      "NODE_3_0_8: [3.563762664794922, -3.0423431396484375]\n",
      "NODE_3_0_9: [3.4629743099212646, -2.703216791152954]\n",
      "BLOCK_3: [1.7806780338287354, -0.8625850677490234]\n",
      "NODE_3_1_0: [3.427324056625366, -2.6304678916931152]\n",
      "NODE_3_1_1: [3.463486433029175, -2.896028757095337]\n",
      "NODE_3_1_2: [4.271627426147461, -3.714012384414673]\n",
      "NODE_3_1_3: [3.0531246662139893, -2.5906405448913574]\n",
      "NODE_3_1_4: [2.535637378692627, -2.094147205352783]\n",
      "NODE_3_1_5: [2.4515669345855713, -2.0170562267303467]\n",
      "NODE_3_1_6: [2.8948068618774414, -2.4387693405151367]\n",
      "NODE_3_1_7: [4.33751106262207, -3.7820510864257812]\n",
      "NODE_3_1_8: [3.62784743309021, -3.0909814834594727]\n",
      "NODE_3_1_9: [3.4996142387390137, -2.730451822280884]\n",
      "NODE_3_2_0: [3.4579265117645264, -2.6550676822662354]\n",
      "NODE_3_2_1: [3.520995855331421, -2.9439759254455566]\n",
      "NODE_3_2_2: [4.330197334289551, -3.759413719177246]\n",
      "NODE_3_2_3: [3.0624217987060547, -2.5874247550964355]\n",
      "NODE_3_2_4: [2.589092493057251, -2.133665084838867]\n",
      "NODE_3_2_5: [2.56813383102417, -2.115767002105713]\n",
      "NODE_3_2_6: [3.052565336227417, -2.5782055854797363]\n",
      "NODE_3_2_7: [4.422957897186279, -3.850402355194092]\n",
      "NODE_3_2_8: [3.691957712173462, -3.1396422386169434]\n",
      "NODE_3_2_9: [3.536255121231079, -2.7576870918273926]\n",
      "NODE_4_0_0: [3.448333501815796, -2.650829792022705]\n",
      "NODE_4_0_1: [3.4609458446502686, -2.8977057933807373]\n",
      "NODE_4_0_2: [4.281249523162842, -3.7283918857574463]\n",
      "NODE_4_0_3: [3.103513717651367, -2.6452512741088867]\n",
      "NODE_4_0_4: [2.558278799057007, -2.1220903396606445]\n",
      "NODE_4_0_5: [2.418184995651245, -1.9918206930160522]\n",
      "NODE_4_0_6: [2.80063796043396, -2.354980945587158]\n",
      "NODE_4_0_7: [4.322426795959473, -3.7752647399902344]\n",
      "NODE_4_0_8: [3.6196300983428955, -3.0930843353271484]\n",
      "NODE_4_0_9: [3.5168492794036865, -2.7506823539733887]\n",
      "BLOCK_4: [1.7892498970031738, -0.8693079948425293]\n",
      "NODE_4_1_0: [3.478314161300659, -2.674894094467163]\n",
      "NODE_4_1_1: [3.518359422683716, -2.945620059967041]\n",
      "NODE_4_1_2: [4.33981990814209, -3.7737934589385986]\n",
      "NODE_4_1_3: [3.1111958026885986, -2.640524387359619]\n",
      "NODE_4_1_4: [2.606414556503296, -2.156491994857788]\n",
      "NODE_4_1_5: [2.5248968601226807, -2.0813980102539062]\n",
      "NODE_4_1_6: [2.9524588584899902, -2.4886858463287354]\n",
      "NODE_4_1_7: [4.407318592071533, -3.843137502670288]\n",
      "NODE_4_1_8: [3.6839873790740967, -3.141982078552246]\n",
      "NODE_4_1_9: [3.553797960281372, -2.7782156467437744]\n",
      "NODE_4_2_0: [3.5089175701141357, -2.6994943618774414]\n",
      "NODE_4_2_1: [3.5758464336395264, -2.9935388565063477]\n",
      "NODE_4_2_2: [4.39838981628418, -3.8191943168640137]\n",
      "NODE_4_2_3: [3.1188783645629883, -2.6357970237731934]\n",
      "NODE_4_2_4: [2.6545491218566895, -2.190892457962036]\n",
      "NODE_4_2_5: [2.63136887550354, -2.1707520484924316]\n",
      "NODE_4_2_6: [3.104736328125, -2.62284779548645]\n",
      "NODE_4_2_7: [4.492152214050293, -3.910964250564575]\n",
      "NODE_4_2_8: [3.748342275619507, -3.190877676010132]\n",
      "NODE_4_2_9: [3.5907466411590576, -2.80574893951416]\n",
      "NODE_5_0_0: [3.4990036487579346, -2.6954503059387207]\n",
      "NODE_5_0_1: [3.5168545246124268, -2.9483280181884766]\n",
      "NODE_5_0_2: [4.349308967590332, -3.788062572479248]\n",
      "NODE_5_0_3: [3.161583185195923, -2.6951332092285156]\n",
      "NODE_5_0_4: [2.6290547847747803, -2.1844332218170166]\n",
      "NODE_5_0_5: [2.4913101196289062, -2.0559725761413574]\n",
      "NODE_5_0_6: [2.8584377765655518, -2.405057191848755]\n",
      "NODE_5_0_7: [4.391982555389404, -3.836160898208618]\n",
      "NODE_5_0_8: [3.6780483722686768, -3.1458446979522705]\n",
      "NODE_5_0_9: [3.572782278060913, -2.8005456924438477]\n",
      "BLOCK_5: [1.8163979053497314, -0.8928138613700867]\n",
      "NODE_5_1_0: [3.530550718307495, -2.7209036350250244]\n",
      "NODE_5_1_1: [3.5751254558563232, -2.99699330329895]\n",
      "NODE_5_1_2: [4.407772541046143, -3.833376169204712]\n",
      "NODE_5_1_3: [3.1692652702331543, -2.690406322479248]\n",
      "NODE_5_1_4: [2.677189588546753, -2.218834161758423]\n",
      "NODE_5_1_5: [2.5980217456817627, -2.1455488204956055]\n",
      "NODE_5_1_6: [3.010503053665161, -2.538994789123535]\n",
      "NODE_5_1_7: [4.4765472412109375, -3.9037375450134277]\n",
      "NODE_5_1_8: [3.7433979511260986, -3.195949077606201]\n",
      "NODE_5_1_9: [3.610630750656128, -2.8289482593536377]\n",
      "NODE_5_2_0: [3.5629324913024902, -2.747044563293457]\n",
      "NODE_5_2_1: [3.633310556411743, -3.045584201812744]\n",
      "NODE_5_2_2: [4.466336727142334, -3.8787736892700195]\n",
      "NODE_5_2_3: [3.177461862564087, -2.6861801147460938]\n",
      "NODE_5_2_4: [2.726353406906128, -2.254209518432617]\n",
      "NODE_5_2_5: [2.7043259143829346, -2.2351484298706055]\n",
      "NODE_5_2_6: [3.1617038249969482, -2.6720638275146484]\n",
      "NODE_5_2_7: [4.560903549194336, -3.971153736114502]\n",
      "NODE_5_2_8: [3.81063175201416, -3.2476840019226074]\n",
      "NODE_5_2_9: [3.6510801315307617, -2.859910488128662]\n",
      "NODE_6_0_0: [3.5542967319488525, -2.744647979736328]\n",
      "NODE_6_0_1: [3.575676679611206, -3.0016884803771973]\n",
      "NODE_6_0_2: [4.416988849639893, -3.847419261932373]\n",
      "NODE_6_0_3: [3.2196524143218994, -2.7450151443481445]\n",
      "NODE_6_0_4: [2.6998302936553955, -2.2467761039733887]\n",
      "NODE_6_0_5: [2.5644350051879883, -2.1201236248016357]\n",
      "NODE_6_0_6: [2.91623854637146, -2.4551331996917725]\n",
      "NODE_6_0_7: [4.460694313049316, -3.896347999572754]\n",
      "NODE_6_0_8: [3.7417140007019043, -3.2033491134643555]\n",
      "NODE_6_0_9: [3.63366961479187, -2.8557515144348145]\n",
      "BLOCK_6: [1.8587828874588013, -0.9301023483276367]\n",
      "NODE_6_1_0: [3.5866787433624268, -2.7707886695861816]\n",
      "NODE_6_1_1: [3.6336939334869385, -3.05013370513916]\n",
      "NODE_6_1_2: [4.475452423095703, -3.8927321434020996]\n",
      "NODE_6_1_3: [3.2273356914520264, -2.7402889728546143]\n",
      "NODE_6_1_4: [2.747967004776001, -2.2811787128448486]\n",
      "NODE_6_1_5: [2.6711485385894775, -2.2097020149230957]\n",
      "NODE_6_1_6: [3.068326234817505, -2.5890932083129883]\n",
      "NODE_6_1_7: [4.544415473937988, -3.963197708129883]\n",
      "NODE_6_1_8: [3.8116438388824463, -3.2579727172851562]\n",
      "NODE_6_1_9: [3.677741765975952, -2.890059232711792]\n",
      "NODE_6_2_0: [3.622018575668335, -2.79978609085083]\n",
      "NODE_6_2_1: [3.6932308673858643, -3.0999860763549805]\n",
      "NODE_6_2_2: [4.534273624420166, -3.9383320808410645]\n",
      "NODE_6_2_3: [3.2406165599823, -2.74102783203125]\n",
      "NODE_6_2_4: [2.802665948867798, -2.3218259811401367]\n",
      "NODE_6_2_5: [2.778867483139038, -2.301088809967041]\n",
      "NODE_6_2_6: [3.2197892665863037, -2.72235107421875]\n",
      "NODE_6_2_7: [4.62807559967041, -4.029997825622559]\n",
      "NODE_6_2_8: [3.8815579414367676, -3.312330961227417]\n",
      "NODE_6_2_9: [3.7225940227508545, -2.9250874519348145]\n",
      "NODE_7_0_0: [3.6172444820404053, -2.8016152381896973]\n",
      "NODE_7_0_1: [3.6386923789978027, -3.0592358112335205]\n",
      "NODE_7_0_2: [4.48706579208374, -3.9084689617156982]\n",
      "NODE_7_0_3: [3.277323007583618, -2.794426918029785]\n",
      "NODE_7_0_4: [2.770040273666382, -2.3084871768951416]\n",
      "NODE_7_0_5: [2.6369411945343018, -2.1836252212524414]\n",
      "NODE_7_0_6: [2.9735107421875, -2.5046191215515137]\n",
      "NODE_7_0_7: [4.530068874359131, -3.957141399383545]\n",
      "NODE_7_0_8: [3.81345534324646, -3.268577814102173]\n",
      "NODE_7_0_9: [3.7087931632995605, -2.9247803688049316]\n",
      "BLOCK_7: [1.923494815826416, -0.9891872406005859]\n",
      "NODE_7_1_0: [3.658029317855835, -2.8357536792755127]\n",
      "NODE_7_1_1: [3.7043211460113525, -3.114990234375]\n",
      "NODE_7_1_2: [4.547550201416016, -3.9555842876434326]\n",
      "NODE_7_1_3: [3.285522699356079, -2.790114402770996]\n",
      "NODE_7_1_4: [2.819450616836548, -2.3439836502075195]\n",
      "NODE_7_1_5: [2.744060516357422, -2.2735071182250977]\n",
      "NODE_7_1_6: [3.1260838508605957, -2.6389336585998535]\n",
      "NODE_7_1_7: [4.6162214279174805, -4.02612829208374]\n",
      "NODE_7_1_8: [3.886270761489868, -3.326036214828491]\n",
      "NODE_7_1_9: [3.754908323287964, -2.96104097366333]\n",
      "NODE_7_2_0: [3.6991569995880127, -2.8702075481414795]\n",
      "NODE_7_2_1: [3.767263650894165, -3.168078899383545]\n",
      "NODE_7_2_2: [4.609194755554199, -4.003668308258057]\n",
      "NODE_7_2_3: [3.306502342224121, -2.798285961151123]\n",
      "NODE_7_2_4: [2.8799850940704346, -2.390110492706299]\n",
      "NODE_7_2_5: [2.8524062633514404, -2.3657703399658203]\n",
      "NODE_7_2_6: [3.276427984237671, -2.7709782123565674]\n",
      "NODE_7_2_7: [4.7023725509643555, -4.095114707946777]\n",
      "NODE_7_2_8: [3.958847999572754, -3.3831472396850586]\n",
      "NODE_7_2_9: [3.8010241985321045, -2.9973015785217285]\n",
      "NODE_8_0_0: [3.6972925662994385, -2.8751258850097656]\n",
      "NODE_8_0_1: [3.7205021381378174, -3.134917736053467]\n",
      "NODE_8_0_2: [4.565891742706299, -3.97715425491333]\n",
      "NODE_8_0_3: [3.333237409591675, -2.8417744636535645]\n",
      "NODE_8_0_4: [2.837662935256958, -2.3673081398010254]\n",
      "NODE_8_0_5: [2.706503391265869, -2.2440309524536133]\n",
      "NODE_8_0_6: [3.028157949447632, -2.551172971725464]\n",
      "NODE_8_0_7: [4.609537124633789, -4.026804447174072]\n",
      "NODE_8_0_8: [3.89448618888855, -3.3426625728607178]\n",
      "NODE_8_0_9: [3.7892963886260986, -2.999281883239746]\n",
      "BLOCK_8: [1.999228835105896, -1.0589503049850464]\n",
      "NODE_8_1_0: [3.738419771194458, -2.909579038619995]\n",
      "NODE_8_1_1: [3.7855513095855713, -3.190110206604004]\n",
      "NODE_8_1_2: [4.626632213592529, -4.0244855880737305]\n",
      "NODE_8_1_3: [3.342912435531616, -2.8388986587524414]\n",
      "NODE_8_1_4: [2.8900229930877686, -2.405597686767578]\n",
      "NODE_8_1_5: [2.8147265911102295, -2.3352150917053223]\n",
      "NODE_8_1_6: [3.1824748516082764, -2.6871497631073]\n",
      "NODE_8_1_7: [4.6956892013549805, -4.095791339874268]\n",
      "NODE_8_1_8: [3.967576026916504, -3.400421142578125]\n",
      "NODE_8_1_9: [3.8354127407073975, -3.0355424880981445]\n",
      "NODE_8_2_0: [3.779561758041382, -2.9440999031066895]\n",
      "NODE_8_2_1: [3.846982717514038, -3.241699695587158]\n",
      "NODE_8_2_2: [4.688882827758789, -4.073072910308838]\n",
      "NODE_8_2_3: [3.3715286254882812, -2.8545339107513428]\n",
      "NODE_8_2_4: [2.9560279846191406, -2.456969738006592]\n",
      "NODE_8_2_5: [2.924652338027954, -2.4290084838867188]\n",
      "NODE_8_2_6: [3.3321244716644287, -2.8185067176818848]\n",
      "NODE_8_2_7: [4.781840801239014, -4.164778232574463]\n",
      "NODE_8_2_8: [4.040391445159912, -3.4578795433044434]\n",
      "NODE_8_2_9: [3.88152813911438, -3.071803092956543]\n",
      "NODE_9_0_0: [3.777738094329834, -2.9489641189575195]\n",
      "NODE_9_0_1: [3.802582025527954, -3.2108473777770996]\n",
      "NODE_9_0_2: [4.644963264465332, -4.046046257019043]\n",
      "NODE_9_0_3: [3.390568494796753, -2.8905019760131836]\n",
      "NODE_9_0_4: [2.908118724822998, -2.4288110733032227]\n",
      "NODE_9_0_5: [2.7777488231658936, -2.3060266971588135]\n",
      "NODE_9_0_6: [3.084479331970215, -2.599323272705078]\n",
      "NODE_9_0_7: [4.687814712524414, -4.095682144165039]\n",
      "NODE_9_0_8: [3.9746453762054443, -3.4157772064208984]\n",
      "NODE_9_0_9: [3.869859457015991, -3.0738000869750977]\n",
      "BLOCK_9: [2.081799268722534, -1.1340335607528687]\n",
      "NODE_9_1_0: [3.8187360763549805, -2.9834423065185547]\n",
      "NODE_9_1_1: [3.8651936054229736, -3.263686180114746]\n",
      "NODE_9_1_2: [4.705933570861816, -4.093569278717041]\n",
      "NODE_9_1_3: [3.403057336807251, -2.890376091003418]\n",
      "NODE_9_1_4: [2.962570905685425, -2.4691050052642822]\n",
      "NODE_9_1_5: [2.8859317302703857, -2.397458553314209]\n",
      "NODE_9_1_6: [3.2388675212860107, -2.7353670597076416]\n",
      "NODE_9_1_7: [4.7751569747924805, -4.165453910827637]\n",
      "NODE_9_1_8: [4.047829627990723, -3.473639488220215]\n",
      "NODE_9_1_9: [3.9159748554229736, -3.1100611686706543]\n",
      "NODE_9_2_0: [3.860102891921997, -3.0182743072509766]\n",
      "NODE_9_2_1: [3.9267566204071045, -3.3153867721557617]\n",
      "NODE_9_2_2: [4.768571853637695, -4.1424784660339355]\n",
      "NODE_9_2_3: [3.436556816101074, -2.910783290863037]\n",
      "NODE_9_2_4: [3.032074213027954, -2.5238325595855713]\n",
      "NODE_9_2_5: [2.9969003200531006, -2.4922473430633545]\n",
      "NODE_9_2_6: [3.3878214359283447, -2.8660361766815186]\n",
      "NODE_9_2_7: [4.861309051513672, -4.234440803527832]\n",
      "NODE_9_2_8: [4.121013641357422, -3.5315022468566895]\n",
      "NODE_9_2_9: [3.9620912075042725, -3.146322011947632]\n"
     ]
    }
   ],
   "source": [
    "for i, node_name in enumerate(node_list):  # first 10 nodes\n",
    "    print(f\"{node_name}: {out[i].tolist()}\")\n",
    "    _, pred = out.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePathGNN(nn.Module):\n",
    "    \"\"\"GNN with improved single-node path detection\"\"\"\n",
    "    def __init__(self, hidden_channels=64):\n",
    "        super(SimplePathGNN, self).__init__()\n",
    "        \n",
    "        # Input layers\n",
    "        self.conv1 = GCNConv(2, hidden_channels)\n",
    "        \n",
    "        # Direct connection layer - specifically for single-hop connections\n",
    "        self.direct_layer = GCNConv(hidden_channels, hidden_channels//2)\n",
    "        \n",
    "        # Multi-hop layers\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # Output MLP\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_channels + hidden_channels//2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Initial embedding\n",
    "        h = F.relu(self.conv1(x, edge_index))\n",
    "        \n",
    "        # Direct connection path - capture one-hop paths\n",
    "        h_direct = F.relu(self.direct_layer(h, edge_index))\n",
    "        \n",
    "        # Multi-hop path\n",
    "        h = F.relu(self.conv2(h, edge_index))\n",
    "        h = F.relu(self.conv3(h, edge_index))\n",
    "        \n",
    "        # Combine both paths\n",
    "        h_combined = torch.cat([h, h_direct], dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        out = self.classifier(h_combined)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_samples(G, num_samples=1000):\n",
    "    \"\"\"Create samples where labels are nodes on paths between two input nodes\"\"\"\n",
    "    import networkx as nx\n",
    "    from torch_geometric.data import Data\n",
    "    \n",
    "    samples = []\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # Create edge index\n",
    "    edge_index = []\n",
    "    for u, v in G.edges():\n",
    "        edge_index.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edge_index.append([node_to_idx[v], node_to_idx[u]])\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "    \n",
    "    # Group nodes by ring (PR and LR)\n",
    "    rings = {}\n",
    "    for node in G.nodes():\n",
    "        data = G.nodes[node]\n",
    "        pr_id = data.get('pr_id')\n",
    "        lr_id = data.get('lr_id')\n",
    "        \n",
    "        if pr_id is not None and lr_id is not None:\n",
    "            key = (pr_id, lr_id)\n",
    "            if key not in rings:\n",
    "                rings[key] = []\n",
    "            rings[key].append(node)\n",
    "    \n",
    "    # Filter rings with enough nodes\n",
    "    valid_rings = {key: nodes for key, nodes in rings.items() if len(nodes) >= 2}\n",
    "    \n",
    "    # Create NetworkX graph for path finding\n",
    "    G_nx = nx.Graph()\n",
    "    for u, v in G.edges():\n",
    "        G_nx.add_edge(u, v)\n",
    "    \n",
    "    # Generate samples\n",
    "    sample_count = 0\n",
    "    attempts = 0\n",
    "    max_attempts = num_samples * 5\n",
    "    \n",
    "    while sample_count < num_samples and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        \n",
    "        # CHANGED: Select a random ring and two nodes from it\n",
    "        if not valid_rings:\n",
    "            print(\"Warning: No valid rings found with at least 2 nodes\")\n",
    "            break\n",
    "            \n",
    "        ring_key = random.choice(list(valid_rings.keys()))\n",
    "        ring_nodes = valid_rings[ring_key]\n",
    "        \n",
    "        # Choose two random nodes from this ring\n",
    "        node1, node2 = random.sample(ring_nodes, 2)\n",
    "        \n",
    "        # Check if a path exists\n",
    "        if not nx.has_path(G_nx, node1, node2):\n",
    "            continue\n",
    "        \n",
    "        # Find shortest path\n",
    "        path = nx.shortest_path(G_nx, node1, node2)\n",
    "        \n",
    "        # Create input features (just the two marker nodes)\n",
    "        x = torch.zeros((len(node_list), 2), dtype=torch.float)\n",
    "        x[node_to_idx[node1], 0] = 1.0  # Mark first input node\n",
    "        x[node_to_idx[node2], 1] = 1.0  # Mark second input node\n",
    "        \n",
    "        # Create labels (nodes on the path)\n",
    "        y = torch.zeros(len(node_list), dtype=torch.long)\n",
    "        \n",
    "        # Mark nodes on the path (excluding endpoints)\n",
    "        for node in path[1:-1]:\n",
    "            y[node_to_idx[node]] = 1\n",
    "        \n",
    "        # Create data object\n",
    "        data = Data(\n",
    "            x=x, \n",
    "            edge_index=edge_index, \n",
    "            y=y,\n",
    "            path_nodes=[node_to_idx[n] for n in path]  # Store for reference\n",
    "        )\n",
    "        \n",
    "        samples.append(data)\n",
    "        sample_count += 1\n",
    "    \n",
    "    print(f\"Generated {len(samples)} samples from {len(valid_rings)} unique rings\")\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple_path_model(G, num_epochs=30, batch_size=32):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create dataset\n",
    "    print(\"Creating path samples...\")\n",
    "    dataset = create_path_samples(G, num_samples=2000)\n",
    "    \n",
    "    # Split train/val\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create loaders\n",
    "    from torch_geometric.loader import DataLoader\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "    \n",
    "    # Create model\n",
    "    model = SimplePathGNN(hidden_channels=64).to(device)\n",
    "    \n",
    "    # Optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 5.0]).to(device))\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Training model...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds, train_labels = [], []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - only need x and edge_index\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, pred = out.max(dim=1)\n",
    "            \n",
    "            # Store for metrics\n",
    "            train_preds.append(pred.cpu())\n",
    "            train_labels.append(batch.y.cpu())\n",
    "            \n",
    "            # Loss and backward\n",
    "            loss = criterion(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch.x, batch.edge_index)\n",
    "                _, pred = out.max(dim=1)\n",
    "                \n",
    "                val_preds.append(pred.cpu())\n",
    "                val_labels.append(batch.y.cpu())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "        \n",
    "        train_preds = torch.cat(train_preds)\n",
    "        train_labels = torch.cat(train_labels)\n",
    "        train_prec = precision_score(train_labels, train_preds, zero_division=0)\n",
    "        train_rec = recall_score(train_labels, train_preds, zero_division=0)\n",
    "        train_f1 = f1_score(train_labels, train_preds, zero_division=0)\n",
    "        \n",
    "        val_preds = torch.cat(val_preds)\n",
    "        val_labels = torch.cat(val_labels)\n",
    "        val_prec = precision_score(val_labels, val_preds, zero_division=0)\n",
    "        val_rec = recall_score(val_labels, val_preds, zero_division=0)\n",
    "        val_f1 = f1_score(val_labels, val_preds, zero_division=0)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train: Loss={total_loss/len(train_loader):.4f}, F1={train_f1:.4f}\")\n",
    "        print(f\"  Precision={train_prec:.4f}, Recall={train_rec:.4f}\")\n",
    "        print(f\"  Val: F1={val_f1:.4f}, Precision={val_prec:.4f}, Recall={val_rec:.4f}\")\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), \"simple_path_model.pt\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_simple_path(model, G, node1, node2):\n",
    "    \"\"\"Predict nodes on the path between two nodes\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Get node indices\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # Check nodes exist\n",
    "    if node1 not in node_to_idx or node2 not in node_to_idx:\n",
    "        print(\"Error: One or both nodes not found in graph\")\n",
    "        return []\n",
    "    \n",
    "    # Create edge index\n",
    "    edge_index = []\n",
    "    for u, v in G.edges():\n",
    "        edge_index.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edge_index.append([node_to_idx[v], node_to_idx[u]])\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().to(device)\n",
    "    \n",
    "    # Create input features (just marking the two input nodes)\n",
    "    x = torch.zeros((len(node_list), 2), dtype=torch.float).to(device)\n",
    "    x[node_to_idx[node1], 0] = 1.0\n",
    "    x[node_to_idx[node2], 1] = 1.0\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        out = model(x, edge_index)\n",
    "        probs = torch.softmax(out, dim=1)\n",
    "        _, pred = out.max(dim=1)\n",
    "    \n",
    "    # Get nodes predicted to be on the path\n",
    "    path_indices = torch.nonzero(pred == 1).squeeze().cpu().tolist()\n",
    "    \n",
    "    # Handle case of single or no result\n",
    "    if not isinstance(path_indices, list):\n",
    "        path_indices = [path_indices] if torch.is_tensor(path_indices) and path_indices.numel() > 0 else []\n",
    "    \n",
    "    # Convert to node names\n",
    "    path_nodes = [node_list[idx] for idx in path_indices \n",
    "                 if idx != node_to_idx[node1] and idx != node_to_idx[node2]]\n",
    "    \n",
    "    return path_nodes, out.cpu(), node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating path samples...\n",
      "Generated 2000 samples from 31 unique rings\n",
      "Training model...\n",
      "Epoch 1/50:\n",
      "  Train: Loss=0.5046, F1=0.1784\n",
      "  Precision=0.2711, Recall=0.1330\n",
      "  Val: F1=0.3817, Precision=0.2847, Recall=0.5791\n",
      "Epoch 2/50:\n",
      "  Train: Loss=0.0609, F1=0.4174\n",
      "  Precision=0.2813, Recall=0.8083\n",
      "  Val: F1=0.4096, Precision=0.2618, Recall=0.9411\n",
      "Epoch 3/50:\n",
      "  Train: Loss=0.0336, F1=0.4043\n",
      "  Precision=0.2558, Recall=0.9637\n",
      "  Val: F1=0.4183, Precision=0.2660, Recall=0.9779\n",
      "Epoch 4/50:\n",
      "  Train: Loss=0.0307, F1=0.4107\n",
      "  Precision=0.2601, Recall=0.9750\n",
      "  Val: F1=0.4291, Precision=0.2757, Recall=0.9669\n",
      "Epoch 5/50:\n",
      "  Train: Loss=0.0268, F1=0.4646\n",
      "  Precision=0.3059, Recall=0.9659\n",
      "  Val: F1=0.5460, Precision=0.3854, Recall=0.9362\n",
      "Epoch 6/50:\n",
      "  Train: Loss=0.0225, F1=0.6198\n",
      "  Precision=0.4615, Recall=0.9435\n",
      "  Val: F1=0.6657, Precision=0.5332, Recall=0.8859\n",
      "Epoch 7/50:\n",
      "  Train: Loss=0.0196, F1=0.6590\n",
      "  Precision=0.5124, Recall=0.9232\n",
      "  Val: F1=0.6743, Precision=0.5520, Recall=0.8663\n",
      "Epoch 8/50:\n",
      "  Train: Loss=0.0179, F1=0.6714\n",
      "  Precision=0.5288, Recall=0.9195\n",
      "  Val: F1=0.6857, Precision=0.5443, Recall=0.9264\n",
      "Epoch 9/50:\n",
      "  Train: Loss=0.0166, F1=0.6838\n",
      "  Precision=0.5414, Recall=0.9280\n",
      "  Val: F1=0.6857, Precision=0.5443, Recall=0.9264\n",
      "Epoch 10/50:\n",
      "  Train: Loss=0.0154, F1=0.6890\n",
      "  Precision=0.5468, Recall=0.9311\n",
      "  Val: F1=0.6917, Precision=0.5476, Recall=0.9387\n",
      "Epoch 11/50:\n",
      "  Train: Loss=0.0144, F1=0.7039\n",
      "  Precision=0.5641, Recall=0.9359\n",
      "  Val: F1=0.7064, Precision=0.5662, Recall=0.9387\n",
      "Epoch 12/50:\n",
      "  Train: Loss=0.0137, F1=0.7154\n",
      "  Precision=0.5782, Recall=0.9381\n",
      "  Val: F1=0.7237, Precision=0.5889, Recall=0.9387\n",
      "Epoch 13/50:\n",
      "  Train: Loss=0.0131, F1=0.7324\n",
      "  Precision=0.6022, Recall=0.9343\n",
      "  Val: F1=0.7582, Precision=0.6359, Recall=0.9387\n",
      "Epoch 14/50:\n",
      "  Train: Loss=0.0125, F1=0.7530\n",
      "  Precision=0.6281, Recall=0.9400\n",
      "  Val: F1=0.7449, Precision=0.6174, Recall=0.9387\n",
      "Epoch 15/50:\n",
      "  Train: Loss=0.0120, F1=0.7672\n",
      "  Precision=0.6486, Recall=0.9390\n",
      "  Val: F1=0.7802, Precision=0.6675, Recall=0.9387\n",
      "Epoch 16/50:\n",
      "  Train: Loss=0.0115, F1=0.7779\n",
      "  Precision=0.6603, Recall=0.9463\n",
      "  Val: F1=0.7746, Precision=0.6649, Recall=0.9276\n",
      "Epoch 17/50:\n",
      "  Train: Loss=0.0112, F1=0.7832\n",
      "  Precision=0.6673, Recall=0.9479\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 18/50:\n",
      "  Train: Loss=0.0109, F1=0.7804\n",
      "  Precision=0.6651, Recall=0.9441\n",
      "  Val: F1=0.7814, Precision=0.6638, Recall=0.9497\n",
      "Epoch 19/50:\n",
      "  Train: Loss=0.0105, F1=0.7856\n",
      "  Precision=0.6690, Recall=0.9514\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 20/50:\n",
      "  Train: Loss=0.0111, F1=0.7814\n",
      "  Precision=0.6659, Recall=0.9454\n",
      "  Val: F1=0.7689, Precision=0.6622, Recall=0.9166\n",
      "Epoch 21/50:\n",
      "  Train: Loss=0.0106, F1=0.7805\n",
      "  Precision=0.6601, Recall=0.9545\n",
      "  Val: F1=0.7802, Precision=0.6675, Recall=0.9387\n",
      "Epoch 22/50:\n",
      "  Train: Loss=0.0102, F1=0.7840\n",
      "  Precision=0.6658, Recall=0.9533\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 23/50:\n",
      "  Train: Loss=0.0099, F1=0.7863\n",
      "  Precision=0.6696, Recall=0.9523\n",
      "  Val: F1=0.7802, Precision=0.6675, Recall=0.9387\n",
      "Epoch 24/50:\n",
      "  Train: Loss=0.0096, F1=0.7864\n",
      "  Precision=0.6694, Recall=0.9529\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 25/50:\n",
      "  Train: Loss=0.0097, F1=0.7842\n",
      "  Precision=0.6670, Recall=0.9514\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 26/50:\n",
      "  Train: Loss=0.0095, F1=0.7860\n",
      "  Precision=0.6676, Recall=0.9555\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 27/50:\n",
      "  Train: Loss=0.0094, F1=0.7859\n",
      "  Precision=0.6690, Recall=0.9523\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 28/50:\n",
      "  Train: Loss=0.0097, F1=0.7836\n",
      "  Precision=0.6646, Recall=0.9545\n",
      "  Val: F1=0.7825, Precision=0.6522, Recall=0.9779\n",
      "Epoch 29/50:\n",
      "  Train: Loss=0.0093, F1=0.7856\n",
      "  Precision=0.6693, Recall=0.9510\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 30/50:\n",
      "  Train: Loss=0.0091, F1=0.7855\n",
      "  Precision=0.6670, Recall=0.9551\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 31/50:\n",
      "  Train: Loss=0.0093, F1=0.7839\n",
      "  Precision=0.6634, Recall=0.9580\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 32/50:\n",
      "  Train: Loss=0.0091, F1=0.7861\n",
      "  Precision=0.6693, Recall=0.9523\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 33/50:\n",
      "  Train: Loss=0.0092, F1=0.7830\n",
      "  Precision=0.6634, Recall=0.9551\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 34/50:\n",
      "  Train: Loss=0.0089, F1=0.7841\n",
      "  Precision=0.6629, Recall=0.9596\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 35/50:\n",
      "  Train: Loss=0.0089, F1=0.7841\n",
      "  Precision=0.6648, Recall=0.9558\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 36/50:\n",
      "  Train: Loss=0.0087, F1=0.7851\n",
      "  Precision=0.6658, Recall=0.9564\n",
      "  Val: F1=0.7825, Precision=0.6522, Recall=0.9779\n",
      "Epoch 37/50:\n",
      "  Train: Loss=0.0091, F1=0.7811\n",
      "  Precision=0.6598, Recall=0.9570\n",
      "  Val: F1=0.7825, Precision=0.6522, Recall=0.9779\n",
      "Epoch 38/50:\n",
      "  Train: Loss=0.0086, F1=0.7850\n",
      "  Precision=0.6650, Recall=0.9580\n",
      "  Val: F1=0.7836, Precision=0.6610, Recall=0.9620\n",
      "Epoch 39/50:\n",
      "  Train: Loss=0.0088, F1=0.7853\n",
      "  Precision=0.6654, Recall=0.9580\n",
      "  Val: F1=0.7836, Precision=0.6610, Recall=0.9620\n",
      "Epoch 40/50:\n",
      "  Train: Loss=0.0086, F1=0.7816\n",
      "  Precision=0.6592, Recall=0.9599\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 41/50:\n",
      "  Train: Loss=0.0085, F1=0.7857\n",
      "  Precision=0.6655, Recall=0.9589\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 42/50:\n",
      "  Train: Loss=0.0085, F1=0.7834\n",
      "  Precision=0.6627, Recall=0.9580\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 43/50:\n",
      "  Train: Loss=0.0083, F1=0.7815\n",
      "  Precision=0.6591, Recall=0.9599\n",
      "  Val: F1=0.8013, Precision=0.7024, Recall=0.9325\n",
      "Epoch 44/50:\n",
      "  Train: Loss=0.0085, F1=0.7835\n",
      "  Precision=0.6612, Recall=0.9611\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n",
      "Epoch 45/50:\n",
      "  Train: Loss=0.0088, F1=0.7847\n",
      "  Precision=0.6671, Recall=0.9526\n",
      "  Val: F1=0.7825, Precision=0.6522, Recall=0.9779\n",
      "Epoch 46/50:\n",
      "  Train: Loss=0.0084, F1=0.7824\n",
      "  Precision=0.6593, Recall=0.9621\n",
      "  Val: F1=0.7968, Precision=0.7005, Recall=0.9239\n",
      "Epoch 47/50:\n",
      "  Train: Loss=0.0082, F1=0.7843\n",
      "  Precision=0.6623, Recall=0.9615\n",
      "  Val: F1=0.7836, Precision=0.6610, Recall=0.9620\n",
      "Epoch 48/50:\n",
      "  Train: Loss=0.0081, F1=0.7815\n",
      "  Precision=0.6559, Recall=0.9665\n",
      "  Val: F1=0.7836, Precision=0.6610, Recall=0.9620\n",
      "Epoch 49/50:\n",
      "  Train: Loss=0.0080, F1=0.7801\n",
      "  Precision=0.6538, Recall=0.9668\n",
      "  Val: F1=0.7825, Precision=0.6522, Recall=0.9779\n",
      "Epoch 50/50:\n",
      "  Train: Loss=0.0081, F1=0.7819\n",
      "  Precision=0.6581, Recall=0.9630\n",
      "  Val: F1=0.7858, Precision=0.6701, Recall=0.9497\n"
     ]
    }
   ],
   "source": [
    "simple_model = train_simple_path_model(G_nx, num_epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolated nodes because of NODE_4_0_1 and NODE_4_0_4:\n",
      "- NODE_4_0_2\n",
      "- NODE_4_0_3\n",
      "- NODE_4_0_5\n"
     ]
    }
   ],
   "source": [
    "node1 = \"NODE_4_0_1\"\n",
    "node2 = \"NODE_4_0_4\"\n",
    "path_nodes, out, node_list = predict_simple_path(simple_model, G_nx, node1, node2)\n",
    "\n",
    "print(f\"Isolated nodes because of {node1} and {node2}:\")\n",
    "for node in path_nodes:\n",
    "    print(f\"- {node}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
