{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/pytz/__init__.py:31: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  match = re.match(\"^#\\s*version\\s*([0-9a-z]*)\\s*$\", line)\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_topology_data(num_rings=10, nodes_per_ring=10, logical_rings_per_physical=3):\n",
    "    \"\"\"\n",
    "    Create dummy topology data for testing\n",
    "    \n",
    "    Args:\n",
    "        num_rings: Number of physical rings to create\n",
    "        nodes_per_ring: Number of nodes per ring\n",
    "        logical_rings_per_physical: Number of logical rings per physical ring\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with topology data\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for pr_idx in range(num_rings):\n",
    "        physical_ring = f\"RING_{pr_idx}\"\n",
    "        block_name = f\"BLOCK_{pr_idx}\"  # One block per physical ring\n",
    "        \n",
    "        # Each physical ring has multiple logical rings from the same block\n",
    "        for lr_idx in range(logical_rings_per_physical):\n",
    "            logical_ring = f\"LR_{pr_idx}_{lr_idx}\"\n",
    "            \n",
    "            # Create a linear path for each logical ring (not a complete ring)\n",
    "            for i in range(nodes_per_ring - 1):  # Connect nodes in a path, not a ring\n",
    "                node_a = f\"NODE_{pr_idx}_{lr_idx}_{i}\"\n",
    "                node_b = f\"NODE_{pr_idx}_{lr_idx}_{i+1}\"\n",
    "                \n",
    "                data.append({\n",
    "                    'aendname': node_a,\n",
    "                    'bendname': node_b,\n",
    "                    'aendip': f\"10.{pr_idx}.{lr_idx}.{i}\",\n",
    "                    'bendip': f\"10.{pr_idx}.{lr_idx}.{i+1}\",\n",
    "                    'aendifIndex': i,\n",
    "                    'bendifIndex': i+1,\n",
    "                    'block_name': block_name,\n",
    "                    'physicalringname': physical_ring,\n",
    "                    'lrname': logical_ring\n",
    "                })\n",
    "            \n",
    "            # Connect the first and last nodes to the block\n",
    "            # First node connects to block\n",
    "            data.append({\n",
    "                'aendname': f\"NODE_{pr_idx}_{lr_idx}_0\",\n",
    "                'bendname': block_name,\n",
    "                'aendip': f\"10.{pr_idx}.{lr_idx}.0\",\n",
    "                'bendip': f\"10.{pr_idx}.99.99\",  # Special IP for block\n",
    "                'aendifIndex': 100 + lr_idx,\n",
    "                'bendifIndex': 100 + lr_idx,\n",
    "                'block_name': block_name,\n",
    "                'physicalringname': physical_ring,\n",
    "                'lrname': logical_ring\n",
    "            })\n",
    "            \n",
    "            # Last node connects to block\n",
    "            data.append({\n",
    "                'aendname': f\"NODE_{pr_idx}_{lr_idx}_{nodes_per_ring-1}\",\n",
    "                'bendname': block_name,\n",
    "                'aendip': f\"10.{pr_idx}.{lr_idx}.{nodes_per_ring-1}\",\n",
    "                'bendip': f\"10.{pr_idx}.99.99\",  # Special IP for block\n",
    "                'aendifIndex': 200 + lr_idx,\n",
    "                'bendifIndex': 200 + lr_idx,\n",
    "                'block_name': block_name,\n",
    "                'physicalringname': physical_ring,\n",
    "                'lrname': logical_ring\n",
    "            })\n",
    "    \n",
    "    # Add connections between blocks from different physical rings\n",
    "    # for pr_idx in range(num_rings):\n",
    "    #     if pr_idx < num_rings - 1:  # Connect to next physical ring\n",
    "    #         # Connect this block to the next ring's block\n",
    "    #         block_a = f\"BLOCK_{pr_idx}\"\n",
    "    #         block_b = f\"BLOCK_{pr_idx+1}\"\n",
    "            \n",
    "    #         data.append({\n",
    "    #             'aendname': block_a,\n",
    "    #             'bendname': block_b,\n",
    "    #             'aendip': f\"10.{pr_idx}.99.99\",\n",
    "    #             'bendip': f\"10.{pr_idx+1}.99.99\",\n",
    "    #             'aendifIndex': 300 + pr_idx,\n",
    "    #             'bendifIndex': 300 + pr_idx + 1,\n",
    "    #             'block_name': block_a,\n",
    "    #             'physicalringname': f\"RING_{pr_idx}\",\n",
    "    #             'lrname': \"INTER_BLOCK\"  # Inter-block connection\n",
    "    #         })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_data = create_dummy_topology_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_network_graph(topology_df):\n",
    "    \"\"\"Build a graph from topology_data_logical table\"\"\"\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes and edges with attributes\n",
    "    for _, row in topology_df.iterrows():\n",
    "        aend = row['aendname'].upper()\n",
    "        bend = row['bendname'].upper()\n",
    "        \n",
    "        # Add node attributes\n",
    "        G.add_node(aend, ip=row['aendip'], \n",
    "                   physicalringname=row['physicalringname'], \n",
    "                   lrname=row['lrname'],\n",
    "                   block_name=row['block_name'])\n",
    "        \n",
    "        G.add_node(bend, ip=row['bendip'], \n",
    "                   physicalringname=row['physicalringname'], \n",
    "                   lrname=row['lrname'],\n",
    "                   block_name=row['block_name'])\n",
    "        \n",
    "        # Add edge with attributes\n",
    "        G.add_edge(aend, bend, \n",
    "                  physicalringname=row['physicalringname'], \n",
    "                  lrname=row['lrname'],\n",
    "                  aendifIndex=row['aendifIndex'],\n",
    "                  bendifIndex=row['bendifIndex'])\n",
    "    \n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "G =build_network_graph(topo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyvis.network import Network\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def visualize_network(G, filename='network_graph.html'):\n",
    "    \"\"\"\n",
    "    Visualize the network graph using PyVis\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        filename: Output HTML file name\n",
    "    \"\"\"\n",
    "    # Create a PyVis network\n",
    "    net = Network(notebook=True, height=\"750px\", width=\"100%\")\n",
    "    \n",
    "    # Generate a color map for different physical rings and logical rings\n",
    "    physical_rings = set(nx.get_node_attributes(G, 'physicalringname').values())\n",
    "    logical_rings = set(nx.get_node_attributes(G, 'lrname').values())\n",
    "    \n",
    "    # Create color maps\n",
    "    pr_colors = {pr: f\"#{hash(pr) % 0xffffff:06x}\" for pr in physical_rings}\n",
    "    lr_colors = {lr: f\"#{hash(lr) % 0xffffff:06x}\" for lr in logical_rings}\n",
    "    \n",
    "    # Add nodes with attributes\n",
    "    for node in G.nodes():\n",
    "        pr = G.nodes[node]['physicalringname']\n",
    "        lr = G.nodes[node]['lrname']\n",
    "        is_block = node == G.nodes[node]['block_name']\n",
    "        \n",
    "        # Set node properties\n",
    "        node_title = f\"Node: {node}<br>IP: {G.nodes[node]['ip']}<br>PR: {pr}<br>LR: {lr}\"\n",
    "        node_color = pr_colors[pr]  # Color by physical ring\n",
    "        node_shape = 'diamond' if is_block else 'dot'\n",
    "        node_size = 25 if is_block else 15\n",
    "        \n",
    "        net.add_node(node, title=node_title, color=node_color, shape=node_shape, size=node_size, label=node)\n",
    "    \n",
    "    # Add edges with attributes\n",
    "    for u, v in G.edges():\n",
    "        pr = G.edges[u, v]['physicalringname']\n",
    "        lr = G.edges[u, v]['lrname']\n",
    "        \n",
    "        # Set edge properties\n",
    "        edge_title = f\"PR: {pr}<br>LR: {lr}\"\n",
    "        edge_color = lr_colors[lr]  # Color by logical ring\n",
    "        \n",
    "        net.add_edge(u, v, title=edge_title, color=edge_color)\n",
    "    \n",
    "    # Set physics layout\n",
    "    net.barnes_hut(spring_length=200)\n",
    "    \n",
    "    # Save and show the graph\n",
    "    net.show(filename)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "network_topology.html\n"
     ]
    }
   ],
   "source": [
    "net = visualize_network(G, 'network_topology.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_node_failure(G, node_to_fail):\n",
    "    \"\"\"\n",
    "    Simulate a node failure and identify isolated nodes\n",
    "    \n",
    "    Returns:\n",
    "        - isolated_nodes: list of nodes that become isolated\n",
    "    \"\"\"\n",
    "    # Get the node's ring information\n",
    "    if node_to_fail not in G.nodes():\n",
    "        return []\n",
    "    \n",
    "    node_pr = G.nodes[node_to_fail]['physicalringname']\n",
    "    node_lr = G.nodes[node_to_fail]['lrname']\n",
    "    block_name = G.nodes[node_to_fail]['block_name']\n",
    "    \n",
    "    # Get all connected nodes in the same ring before failure\n",
    "    before_graph = G.copy()\n",
    "    block_component = set(nx.node_connected_component(before_graph, block_name))\n",
    "    connected_before = {n for n in block_component \n",
    "                        if n in before_graph.nodes() and \n",
    "                        before_graph.nodes[n].get('physicalringname') == node_pr and \n",
    "                        before_graph.nodes[n].get('lrname') == node_lr}\n",
    "    \n",
    "    # Remove the failed node\n",
    "    after_graph = G.copy()\n",
    "    after_graph.remove_node(node_to_fail)\n",
    "    \n",
    "    # Get connected nodes after failure\n",
    "    if block_name in after_graph.nodes():\n",
    "        block_component_after = set(nx.node_connected_component(after_graph, block_name))\n",
    "        connected_after = {n for n in block_component_after \n",
    "                          if n in after_graph.nodes() and \n",
    "                          after_graph.nodes[n].get('physicalringname') == node_pr and \n",
    "                          after_graph.nodes[n].get('lrname') == node_lr}\n",
    "    else:\n",
    "        connected_after = set()\n",
    "    \n",
    "    # Find isolated nodes (nodes connected before but not after)\n",
    "    isolated_nodes = connected_before - connected_after - {node_to_fail}\n",
    "    \n",
    "    return list(isolated_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_edge_failure(G, edge_to_fail):\n",
    "    \"\"\"\n",
    "    Simulate an edge failure and identify isolated nodes\n",
    "    \n",
    "    Returns:\n",
    "        - isolated_nodes: list of nodes that become isolated\n",
    "    \"\"\"\n",
    "    u, v = edge_to_fail\n",
    "    if not G.has_edge(u, v):\n",
    "        return []\n",
    "    \n",
    "    # Get edge information\n",
    "    edge_pr = G.edges[u, v]['physicalringname']\n",
    "    edge_lr = G.edges[u, v]['lrname']\n",
    "    block_name = G.nodes[u]['block_name']  # Assuming same block for connected nodes\n",
    "    \n",
    "    # Get connected nodes before failure\n",
    "    before_graph = G.copy()\n",
    "    block_component = set(nx.node_connected_component(before_graph, block_name))\n",
    "    connected_before = {n for n in block_component \n",
    "                        if n in before_graph.nodes() and \n",
    "                        before_graph.nodes[n].get('physicalringname') == edge_pr and \n",
    "                        before_graph.nodes[n].get('lrname') == edge_lr}\n",
    "    \n",
    "    # Remove the edge\n",
    "    after_graph = G.copy()\n",
    "    after_graph.remove_edge(u, v)\n",
    "    \n",
    "    # Get connected nodes after failure\n",
    "    block_component_after = set(nx.node_connected_component(after_graph, block_name))\n",
    "    connected_after = {n for n in block_component_after \n",
    "                       if n in after_graph.nodes() and \n",
    "                       after_graph.nodes[n].get('physicalringname') == edge_pr and \n",
    "                       after_graph.nodes[n].get('lrname') == edge_lr}\n",
    "    \n",
    "    # Find isolated nodes\n",
    "    isolated_nodes = connected_before - connected_after\n",
    "    \n",
    "    return list(isolated_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NODE_0_0_0', 'NODE_0_0_1')\n",
      "('NODE_0_0_0', 'BLOCK_0')\n",
      "('NODE_0_0_1', 'NODE_0_0_2')\n",
      "('NODE_0_0_2', 'NODE_0_0_3')\n",
      "('NODE_0_0_3', 'NODE_0_0_4')\n",
      "('NODE_0_0_4', 'NODE_0_0_5')\n",
      "('NODE_0_0_5', 'NODE_0_0_6')\n",
      "('NODE_0_0_6', 'NODE_0_0_7')\n",
      "('NODE_0_0_7', 'NODE_0_0_8')\n",
      "('NODE_0_0_8', 'NODE_0_0_9')\n",
      "('NODE_0_0_9', 'BLOCK_0')\n",
      "('BLOCK_0', 'NODE_0_1_0')\n",
      "('BLOCK_0', 'NODE_0_1_9')\n",
      "('BLOCK_0', 'NODE_0_2_0')\n",
      "('BLOCK_0', 'NODE_0_2_9')\n",
      "('NODE_0_1_0', 'NODE_0_1_1')\n",
      "('NODE_0_1_1', 'NODE_0_1_2')\n",
      "('NODE_0_1_2', 'NODE_0_1_3')\n",
      "('NODE_0_1_3', 'NODE_0_1_4')\n",
      "('NODE_0_1_4', 'NODE_0_1_5')\n",
      "('NODE_0_1_5', 'NODE_0_1_6')\n",
      "('NODE_0_1_6', 'NODE_0_1_7')\n",
      "('NODE_0_1_7', 'NODE_0_1_8')\n",
      "('NODE_0_1_8', 'NODE_0_1_9')\n",
      "('NODE_0_2_0', 'NODE_0_2_1')\n",
      "('NODE_0_2_1', 'NODE_0_2_2')\n",
      "('NODE_0_2_2', 'NODE_0_2_3')\n",
      "('NODE_0_2_3', 'NODE_0_2_4')\n",
      "('NODE_0_2_4', 'NODE_0_2_5')\n",
      "('NODE_0_2_5', 'NODE_0_2_6')\n",
      "('NODE_0_2_6', 'NODE_0_2_7')\n",
      "('NODE_0_2_7', 'NODE_0_2_8')\n",
      "('NODE_0_2_8', 'NODE_0_2_9')\n",
      "('NODE_1_0_0', 'NODE_1_0_1')\n",
      "('NODE_1_0_0', 'BLOCK_1')\n",
      "('NODE_1_0_1', 'NODE_1_0_2')\n",
      "('NODE_1_0_2', 'NODE_1_0_3')\n",
      "('NODE_1_0_3', 'NODE_1_0_4')\n",
      "('NODE_1_0_4', 'NODE_1_0_5')\n",
      "('NODE_1_0_5', 'NODE_1_0_6')\n",
      "('NODE_1_0_6', 'NODE_1_0_7')\n",
      "('NODE_1_0_7', 'NODE_1_0_8')\n",
      "('NODE_1_0_8', 'NODE_1_0_9')\n",
      "('NODE_1_0_9', 'BLOCK_1')\n",
      "('BLOCK_1', 'NODE_1_1_0')\n",
      "('BLOCK_1', 'NODE_1_1_9')\n",
      "('BLOCK_1', 'NODE_1_2_0')\n",
      "('BLOCK_1', 'NODE_1_2_9')\n",
      "('NODE_1_1_0', 'NODE_1_1_1')\n",
      "('NODE_1_1_1', 'NODE_1_1_2')\n",
      "('NODE_1_1_2', 'NODE_1_1_3')\n",
      "('NODE_1_1_3', 'NODE_1_1_4')\n",
      "('NODE_1_1_4', 'NODE_1_1_5')\n",
      "('NODE_1_1_5', 'NODE_1_1_6')\n",
      "('NODE_1_1_6', 'NODE_1_1_7')\n",
      "('NODE_1_1_7', 'NODE_1_1_8')\n",
      "('NODE_1_1_8', 'NODE_1_1_9')\n",
      "('NODE_1_2_0', 'NODE_1_2_1')\n",
      "('NODE_1_2_1', 'NODE_1_2_2')\n",
      "('NODE_1_2_2', 'NODE_1_2_3')\n",
      "('NODE_1_2_3', 'NODE_1_2_4')\n",
      "('NODE_1_2_4', 'NODE_1_2_5')\n",
      "('NODE_1_2_5', 'NODE_1_2_6')\n",
      "('NODE_1_2_6', 'NODE_1_2_7')\n",
      "('NODE_1_2_7', 'NODE_1_2_8')\n",
      "('NODE_1_2_8', 'NODE_1_2_9')\n",
      "('NODE_2_0_0', 'NODE_2_0_1')\n",
      "('NODE_2_0_0', 'BLOCK_2')\n",
      "('NODE_2_0_1', 'NODE_2_0_2')\n",
      "('NODE_2_0_2', 'NODE_2_0_3')\n",
      "('NODE_2_0_3', 'NODE_2_0_4')\n",
      "('NODE_2_0_4', 'NODE_2_0_5')\n",
      "('NODE_2_0_5', 'NODE_2_0_6')\n",
      "('NODE_2_0_6', 'NODE_2_0_7')\n",
      "('NODE_2_0_7', 'NODE_2_0_8')\n",
      "('NODE_2_0_8', 'NODE_2_0_9')\n",
      "('NODE_2_0_9', 'BLOCK_2')\n",
      "('BLOCK_2', 'NODE_2_1_0')\n",
      "('BLOCK_2', 'NODE_2_1_9')\n",
      "('BLOCK_2', 'NODE_2_2_0')\n",
      "('BLOCK_2', 'NODE_2_2_9')\n",
      "('NODE_2_1_0', 'NODE_2_1_1')\n",
      "('NODE_2_1_1', 'NODE_2_1_2')\n",
      "('NODE_2_1_2', 'NODE_2_1_3')\n",
      "('NODE_2_1_3', 'NODE_2_1_4')\n",
      "('NODE_2_1_4', 'NODE_2_1_5')\n",
      "('NODE_2_1_5', 'NODE_2_1_6')\n",
      "('NODE_2_1_6', 'NODE_2_1_7')\n",
      "('NODE_2_1_7', 'NODE_2_1_8')\n",
      "('NODE_2_1_8', 'NODE_2_1_9')\n",
      "('NODE_2_2_0', 'NODE_2_2_1')\n",
      "('NODE_2_2_1', 'NODE_2_2_2')\n",
      "('NODE_2_2_2', 'NODE_2_2_3')\n",
      "('NODE_2_2_3', 'NODE_2_2_4')\n",
      "('NODE_2_2_4', 'NODE_2_2_5')\n",
      "('NODE_2_2_5', 'NODE_2_2_6')\n",
      "('NODE_2_2_6', 'NODE_2_2_7')\n",
      "('NODE_2_2_7', 'NODE_2_2_8')\n",
      "('NODE_2_2_8', 'NODE_2_2_9')\n",
      "('NODE_3_0_0', 'NODE_3_0_1')\n",
      "('NODE_3_0_0', 'BLOCK_3')\n",
      "('NODE_3_0_1', 'NODE_3_0_2')\n",
      "('NODE_3_0_2', 'NODE_3_0_3')\n",
      "('NODE_3_0_3', 'NODE_3_0_4')\n",
      "('NODE_3_0_4', 'NODE_3_0_5')\n",
      "('NODE_3_0_5', 'NODE_3_0_6')\n",
      "('NODE_3_0_6', 'NODE_3_0_7')\n",
      "('NODE_3_0_7', 'NODE_3_0_8')\n",
      "('NODE_3_0_8', 'NODE_3_0_9')\n",
      "('NODE_3_0_9', 'BLOCK_3')\n",
      "('BLOCK_3', 'NODE_3_1_0')\n",
      "('BLOCK_3', 'NODE_3_1_9')\n",
      "('BLOCK_3', 'NODE_3_2_0')\n",
      "('BLOCK_3', 'NODE_3_2_9')\n",
      "('NODE_3_1_0', 'NODE_3_1_1')\n",
      "('NODE_3_1_1', 'NODE_3_1_2')\n",
      "('NODE_3_1_2', 'NODE_3_1_3')\n",
      "('NODE_3_1_3', 'NODE_3_1_4')\n",
      "('NODE_3_1_4', 'NODE_3_1_5')\n",
      "('NODE_3_1_5', 'NODE_3_1_6')\n",
      "('NODE_3_1_6', 'NODE_3_1_7')\n",
      "('NODE_3_1_7', 'NODE_3_1_8')\n",
      "('NODE_3_1_8', 'NODE_3_1_9')\n",
      "('NODE_3_2_0', 'NODE_3_2_1')\n",
      "('NODE_3_2_1', 'NODE_3_2_2')\n",
      "('NODE_3_2_2', 'NODE_3_2_3')\n",
      "('NODE_3_2_3', 'NODE_3_2_4')\n",
      "('NODE_3_2_4', 'NODE_3_2_5')\n",
      "('NODE_3_2_5', 'NODE_3_2_6')\n",
      "('NODE_3_2_6', 'NODE_3_2_7')\n",
      "('NODE_3_2_7', 'NODE_3_2_8')\n",
      "('NODE_3_2_8', 'NODE_3_2_9')\n",
      "('NODE_4_0_0', 'NODE_4_0_1')\n",
      "('NODE_4_0_0', 'BLOCK_4')\n",
      "('NODE_4_0_1', 'NODE_4_0_2')\n",
      "('NODE_4_0_2', 'NODE_4_0_3')\n",
      "('NODE_4_0_3', 'NODE_4_0_4')\n",
      "('NODE_4_0_4', 'NODE_4_0_5')\n",
      "('NODE_4_0_5', 'NODE_4_0_6')\n",
      "('NODE_4_0_6', 'NODE_4_0_7')\n",
      "('NODE_4_0_7', 'NODE_4_0_8')\n",
      "('NODE_4_0_8', 'NODE_4_0_9')\n",
      "('NODE_4_0_9', 'BLOCK_4')\n",
      "('BLOCK_4', 'NODE_4_1_0')\n",
      "('BLOCK_4', 'NODE_4_1_9')\n",
      "('BLOCK_4', 'NODE_4_2_0')\n",
      "('BLOCK_4', 'NODE_4_2_9')\n",
      "('NODE_4_1_0', 'NODE_4_1_1')\n",
      "('NODE_4_1_1', 'NODE_4_1_2')\n",
      "('NODE_4_1_2', 'NODE_4_1_3')\n",
      "('NODE_4_1_3', 'NODE_4_1_4')\n",
      "('NODE_4_1_4', 'NODE_4_1_5')\n",
      "('NODE_4_1_5', 'NODE_4_1_6')\n",
      "('NODE_4_1_6', 'NODE_4_1_7')\n",
      "('NODE_4_1_7', 'NODE_4_1_8')\n",
      "('NODE_4_1_8', 'NODE_4_1_9')\n",
      "('NODE_4_2_0', 'NODE_4_2_1')\n",
      "('NODE_4_2_1', 'NODE_4_2_2')\n",
      "('NODE_4_2_2', 'NODE_4_2_3')\n",
      "('NODE_4_2_3', 'NODE_4_2_4')\n",
      "('NODE_4_2_4', 'NODE_4_2_5')\n",
      "('NODE_4_2_5', 'NODE_4_2_6')\n",
      "('NODE_4_2_6', 'NODE_4_2_7')\n",
      "('NODE_4_2_7', 'NODE_4_2_8')\n",
      "('NODE_4_2_8', 'NODE_4_2_9')\n",
      "('NODE_5_0_0', 'NODE_5_0_1')\n",
      "('NODE_5_0_0', 'BLOCK_5')\n",
      "('NODE_5_0_1', 'NODE_5_0_2')\n",
      "('NODE_5_0_2', 'NODE_5_0_3')\n",
      "('NODE_5_0_3', 'NODE_5_0_4')\n",
      "('NODE_5_0_4', 'NODE_5_0_5')\n",
      "('NODE_5_0_5', 'NODE_5_0_6')\n",
      "('NODE_5_0_6', 'NODE_5_0_7')\n",
      "('NODE_5_0_7', 'NODE_5_0_8')\n",
      "('NODE_5_0_8', 'NODE_5_0_9')\n",
      "('NODE_5_0_9', 'BLOCK_5')\n",
      "('BLOCK_5', 'NODE_5_1_0')\n",
      "('BLOCK_5', 'NODE_5_1_9')\n",
      "('BLOCK_5', 'NODE_5_2_0')\n",
      "('BLOCK_5', 'NODE_5_2_9')\n",
      "('NODE_5_1_0', 'NODE_5_1_1')\n",
      "('NODE_5_1_1', 'NODE_5_1_2')\n",
      "('NODE_5_1_2', 'NODE_5_1_3')\n",
      "('NODE_5_1_3', 'NODE_5_1_4')\n",
      "('NODE_5_1_4', 'NODE_5_1_5')\n",
      "('NODE_5_1_5', 'NODE_5_1_6')\n",
      "('NODE_5_1_6', 'NODE_5_1_7')\n",
      "('NODE_5_1_7', 'NODE_5_1_8')\n",
      "('NODE_5_1_8', 'NODE_5_1_9')\n",
      "('NODE_5_2_0', 'NODE_5_2_1')\n",
      "('NODE_5_2_1', 'NODE_5_2_2')\n",
      "('NODE_5_2_2', 'NODE_5_2_3')\n",
      "('NODE_5_2_3', 'NODE_5_2_4')\n",
      "('NODE_5_2_4', 'NODE_5_2_5')\n",
      "('NODE_5_2_5', 'NODE_5_2_6')\n",
      "('NODE_5_2_6', 'NODE_5_2_7')\n",
      "('NODE_5_2_7', 'NODE_5_2_8')\n",
      "('NODE_5_2_8', 'NODE_5_2_9')\n",
      "('NODE_6_0_0', 'NODE_6_0_1')\n",
      "('NODE_6_0_0', 'BLOCK_6')\n",
      "('NODE_6_0_1', 'NODE_6_0_2')\n",
      "('NODE_6_0_2', 'NODE_6_0_3')\n",
      "('NODE_6_0_3', 'NODE_6_0_4')\n",
      "('NODE_6_0_4', 'NODE_6_0_5')\n",
      "('NODE_6_0_5', 'NODE_6_0_6')\n",
      "('NODE_6_0_6', 'NODE_6_0_7')\n",
      "('NODE_6_0_7', 'NODE_6_0_8')\n",
      "('NODE_6_0_8', 'NODE_6_0_9')\n",
      "('NODE_6_0_9', 'BLOCK_6')\n",
      "('BLOCK_6', 'NODE_6_1_0')\n",
      "('BLOCK_6', 'NODE_6_1_9')\n",
      "('BLOCK_6', 'NODE_6_2_0')\n",
      "('BLOCK_6', 'NODE_6_2_9')\n",
      "('NODE_6_1_0', 'NODE_6_1_1')\n",
      "('NODE_6_1_1', 'NODE_6_1_2')\n",
      "('NODE_6_1_2', 'NODE_6_1_3')\n",
      "('NODE_6_1_3', 'NODE_6_1_4')\n",
      "('NODE_6_1_4', 'NODE_6_1_5')\n",
      "('NODE_6_1_5', 'NODE_6_1_6')\n",
      "('NODE_6_1_6', 'NODE_6_1_7')\n",
      "('NODE_6_1_7', 'NODE_6_1_8')\n",
      "('NODE_6_1_8', 'NODE_6_1_9')\n",
      "('NODE_6_2_0', 'NODE_6_2_1')\n",
      "('NODE_6_2_1', 'NODE_6_2_2')\n",
      "('NODE_6_2_2', 'NODE_6_2_3')\n",
      "('NODE_6_2_3', 'NODE_6_2_4')\n",
      "('NODE_6_2_4', 'NODE_6_2_5')\n",
      "('NODE_6_2_5', 'NODE_6_2_6')\n",
      "('NODE_6_2_6', 'NODE_6_2_7')\n",
      "('NODE_6_2_7', 'NODE_6_2_8')\n",
      "('NODE_6_2_8', 'NODE_6_2_9')\n",
      "('NODE_7_0_0', 'NODE_7_0_1')\n",
      "('NODE_7_0_0', 'BLOCK_7')\n",
      "('NODE_7_0_1', 'NODE_7_0_2')\n",
      "('NODE_7_0_2', 'NODE_7_0_3')\n",
      "('NODE_7_0_3', 'NODE_7_0_4')\n",
      "('NODE_7_0_4', 'NODE_7_0_5')\n",
      "('NODE_7_0_5', 'NODE_7_0_6')\n",
      "('NODE_7_0_6', 'NODE_7_0_7')\n",
      "('NODE_7_0_7', 'NODE_7_0_8')\n",
      "('NODE_7_0_8', 'NODE_7_0_9')\n",
      "('NODE_7_0_9', 'BLOCK_7')\n",
      "('BLOCK_7', 'NODE_7_1_0')\n",
      "('BLOCK_7', 'NODE_7_1_9')\n",
      "('BLOCK_7', 'NODE_7_2_0')\n",
      "('BLOCK_7', 'NODE_7_2_9')\n",
      "('NODE_7_1_0', 'NODE_7_1_1')\n",
      "('NODE_7_1_1', 'NODE_7_1_2')\n",
      "('NODE_7_1_2', 'NODE_7_1_3')\n",
      "('NODE_7_1_3', 'NODE_7_1_4')\n",
      "('NODE_7_1_4', 'NODE_7_1_5')\n",
      "('NODE_7_1_5', 'NODE_7_1_6')\n",
      "('NODE_7_1_6', 'NODE_7_1_7')\n",
      "('NODE_7_1_7', 'NODE_7_1_8')\n",
      "('NODE_7_1_8', 'NODE_7_1_9')\n",
      "('NODE_7_2_0', 'NODE_7_2_1')\n",
      "('NODE_7_2_1', 'NODE_7_2_2')\n",
      "('NODE_7_2_2', 'NODE_7_2_3')\n",
      "('NODE_7_2_3', 'NODE_7_2_4')\n",
      "('NODE_7_2_4', 'NODE_7_2_5')\n",
      "('NODE_7_2_5', 'NODE_7_2_6')\n",
      "('NODE_7_2_6', 'NODE_7_2_7')\n",
      "('NODE_7_2_7', 'NODE_7_2_8')\n",
      "('NODE_7_2_8', 'NODE_7_2_9')\n",
      "('NODE_8_0_0', 'NODE_8_0_1')\n",
      "('NODE_8_0_0', 'BLOCK_8')\n",
      "('NODE_8_0_1', 'NODE_8_0_2')\n",
      "('NODE_8_0_2', 'NODE_8_0_3')\n",
      "('NODE_8_0_3', 'NODE_8_0_4')\n",
      "('NODE_8_0_4', 'NODE_8_0_5')\n",
      "('NODE_8_0_5', 'NODE_8_0_6')\n",
      "('NODE_8_0_6', 'NODE_8_0_7')\n",
      "('NODE_8_0_7', 'NODE_8_0_8')\n",
      "('NODE_8_0_8', 'NODE_8_0_9')\n",
      "('NODE_8_0_9', 'BLOCK_8')\n",
      "('BLOCK_8', 'NODE_8_1_0')\n",
      "('BLOCK_8', 'NODE_8_1_9')\n",
      "('BLOCK_8', 'NODE_8_2_0')\n",
      "('BLOCK_8', 'NODE_8_2_9')\n",
      "('NODE_8_1_0', 'NODE_8_1_1')\n",
      "('NODE_8_1_1', 'NODE_8_1_2')\n",
      "('NODE_8_1_2', 'NODE_8_1_3')\n",
      "('NODE_8_1_3', 'NODE_8_1_4')\n",
      "('NODE_8_1_4', 'NODE_8_1_5')\n",
      "('NODE_8_1_5', 'NODE_8_1_6')\n",
      "('NODE_8_1_6', 'NODE_8_1_7')\n",
      "('NODE_8_1_7', 'NODE_8_1_8')\n",
      "('NODE_8_1_8', 'NODE_8_1_9')\n",
      "('NODE_8_2_0', 'NODE_8_2_1')\n",
      "('NODE_8_2_1', 'NODE_8_2_2')\n",
      "('NODE_8_2_2', 'NODE_8_2_3')\n",
      "('NODE_8_2_3', 'NODE_8_2_4')\n",
      "('NODE_8_2_4', 'NODE_8_2_5')\n",
      "('NODE_8_2_5', 'NODE_8_2_6')\n",
      "('NODE_8_2_6', 'NODE_8_2_7')\n",
      "('NODE_8_2_7', 'NODE_8_2_8')\n",
      "('NODE_8_2_8', 'NODE_8_2_9')\n",
      "('NODE_9_0_0', 'NODE_9_0_1')\n",
      "('NODE_9_0_0', 'BLOCK_9')\n",
      "('NODE_9_0_1', 'NODE_9_0_2')\n",
      "('NODE_9_0_2', 'NODE_9_0_3')\n",
      "('NODE_9_0_3', 'NODE_9_0_4')\n",
      "('NODE_9_0_4', 'NODE_9_0_5')\n",
      "('NODE_9_0_5', 'NODE_9_0_6')\n",
      "('NODE_9_0_6', 'NODE_9_0_7')\n",
      "('NODE_9_0_7', 'NODE_9_0_8')\n",
      "('NODE_9_0_8', 'NODE_9_0_9')\n",
      "('NODE_9_0_9', 'BLOCK_9')\n",
      "('BLOCK_9', 'NODE_9_1_0')\n",
      "('BLOCK_9', 'NODE_9_1_9')\n",
      "('BLOCK_9', 'NODE_9_2_0')\n",
      "('BLOCK_9', 'NODE_9_2_9')\n",
      "('NODE_9_1_0', 'NODE_9_1_1')\n",
      "('NODE_9_1_1', 'NODE_9_1_2')\n",
      "('NODE_9_1_2', 'NODE_9_1_3')\n",
      "('NODE_9_1_3', 'NODE_9_1_4')\n",
      "('NODE_9_1_4', 'NODE_9_1_5')\n",
      "('NODE_9_1_5', 'NODE_9_1_6')\n",
      "('NODE_9_1_6', 'NODE_9_1_7')\n",
      "('NODE_9_1_7', 'NODE_9_1_8')\n",
      "('NODE_9_1_8', 'NODE_9_1_9')\n",
      "('NODE_9_2_0', 'NODE_9_2_1')\n",
      "('NODE_9_2_1', 'NODE_9_2_2')\n",
      "('NODE_9_2_2', 'NODE_9_2_3')\n",
      "('NODE_9_2_3', 'NODE_9_2_4')\n",
      "('NODE_9_2_4', 'NODE_9_2_5')\n",
      "('NODE_9_2_5', 'NODE_9_2_6')\n",
      "('NODE_9_2_6', 'NODE_9_2_7')\n",
      "('NODE_9_2_7', 'NODE_9_2_8')\n",
      "('NODE_9_2_8', 'NODE_9_2_9')\n"
     ]
    }
   ],
   "source": [
    "# Print all edges in the graph\n",
    "for edge in G.edges():\n",
    "    print(edge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NODE_1_2_6',\n",
       " 'NODE_1_2_2',\n",
       " 'NODE_1_2_4',\n",
       " 'NODE_1_2_9',\n",
       " 'NODE_1_2_7',\n",
       " 'NODE_1_2_1',\n",
       " 'NODE_1_2_3',\n",
       " 'NODE_1_2_0',\n",
       " 'NODE_1_2_5',\n",
       " 'NODE_1_2_8']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isolated_nodes = simulate_node_failure(G,'BLOCK_1')\n",
    "isolated_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_multiple_failures(G, elements_to_fail, failure_type='node'):\n",
    "    \"\"\"\n",
    "    Simulate multiple node or edge failures and identify isolated nodes\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        elements_to_fail: List of nodes or edges to fail\n",
    "        failure_type: 'node' or 'edge'\n",
    "    \n",
    "    Returns:\n",
    "        List of isolated nodes\n",
    "    \"\"\"\n",
    "    # Create a copy of the graph \n",
    "    after_graph = G.copy()\n",
    "    \n",
    "    # Remove all failed elements\n",
    "    if failure_type == 'node':\n",
    "        for node in elements_to_fail:\n",
    "            if node in after_graph:\n",
    "                after_graph.remove_node(node)\n",
    "    else:  # edge\n",
    "        for edge in elements_to_fail:\n",
    "            if after_graph.has_edge(*edge):\n",
    "                after_graph.remove_edge(*edge)\n",
    "    \n",
    "    # Find isolated nodes (nodes that can't reach their block)\n",
    "    isolated_nodes = []\n",
    "    \n",
    "    # Group nodes by physical ring and logical ring\n",
    "    nodes_by_ring = {}\n",
    "    for node in G.nodes():\n",
    "        if 'BLOCK' in node:\n",
    "            continue\n",
    "            \n",
    "        pr = G.nodes[node]['physicalringname']\n",
    "        lr = G.nodes[node]['lrname']\n",
    "        block = G.nodes[node]['block_name']\n",
    "        \n",
    "        key = (pr, lr, block)\n",
    "        if key not in nodes_by_ring:\n",
    "            nodes_by_ring[key] = []\n",
    "        nodes_by_ring[key].append(node)\n",
    "    \n",
    "    # Check connectivity for each group\n",
    "    for (pr, lr, block), nodes in nodes_by_ring.items():\n",
    "        if block not in after_graph:\n",
    "            # If block is gone, all nodes in this group are isolated\n",
    "            isolated_nodes.extend(nodes)\n",
    "            continue\n",
    "            \n",
    "        for node in nodes:\n",
    "            if node in elements_to_fail or node not in after_graph:\n",
    "                continue  # Skip failed nodes\n",
    "                \n",
    "            # Check if node can reach its block\n",
    "            try:\n",
    "                path = nx.shortest_path(after_graph, node, block)\n",
    "            except nx.NetworkXNoPath:\n",
    "                isolated_nodes.append(node)\n",
    "    \n",
    "    return isolated_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NODE_1_0_0',\n",
       " 'NODE_1_0_1',\n",
       " 'NODE_1_0_2',\n",
       " 'NODE_1_0_3',\n",
       " 'NODE_1_0_4',\n",
       " 'NODE_1_0_5',\n",
       " 'NODE_1_0_6',\n",
       " 'NODE_1_0_7',\n",
       " 'NODE_1_0_8',\n",
       " 'NODE_1_0_9',\n",
       " 'NODE_1_1_0',\n",
       " 'NODE_1_1_1',\n",
       " 'NODE_1_1_2',\n",
       " 'NODE_1_1_3',\n",
       " 'NODE_1_1_4',\n",
       " 'NODE_1_1_5',\n",
       " 'NODE_1_1_6',\n",
       " 'NODE_1_1_7',\n",
       " 'NODE_1_1_8',\n",
       " 'NODE_1_1_9',\n",
       " 'NODE_1_2_0',\n",
       " 'NODE_1_2_1',\n",
       " 'NODE_1_2_2',\n",
       " 'NODE_1_2_3',\n",
       " 'NODE_1_2_4',\n",
       " 'NODE_1_2_5',\n",
       " 'NODE_1_2_6',\n",
       " 'NODE_1_2_7',\n",
       " 'NODE_1_2_8',\n",
       " 'NODE_1_2_9']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isolated_nodes = simulate_multiple_failures(G,['BLOCK_1'],'node')\n",
    "isolated_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_isolation_prediction_dataset(topology_df, num_simulations=1000, max_failures=3, balance_ratio=2):\n",
    "    \"\"\"\n",
    "    Create an optimized dataset for predicting node isolations after failures\n",
    "    \n",
    "    Args:\n",
    "        topology_df: DataFrame with network topology information\n",
    "        num_simulations: Total number of examples to generate\n",
    "        max_failures: Maximum number of simultaneous failures to simulate\n",
    "        balance_ratio: Maximum ratio of non-isolation to isolation examples\n",
    "        \n",
    "    Returns:\n",
    "        data_list: List of PyG Data objects\n",
    "        node_list: List of all node names\n",
    "        node_to_idx: Dictionary mapping node names to indices\n",
    "    \"\"\"\n",
    "    print(\"Building network graph...\")\n",
    "    G = build_network_graph(topology_df)\n",
    "    \n",
    "    # Create node mapping\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # Identify regular nodes (non-block nodes)\n",
    "    regular_nodes = [n for n in node_list if \"BLOCK\" not in n]\n",
    "    \n",
    "    print(\"Computing network vulnerability metrics...\")\n",
    "    # Pre-compute network properties for isolation prediction\n",
    "    articulation_points = set(nx.articulation_points(G))\n",
    "    bridges = list(nx.bridges(G))\n",
    "    \n",
    "    # Group nodes by physical ring and logical ring\n",
    "    nodes_by_ring = {}\n",
    "    for node in regular_nodes:\n",
    "        if node not in G.nodes():\n",
    "            continue\n",
    "            \n",
    "        pr = G.nodes[node]['physicalringname']\n",
    "        lr = G.nodes[node]['lrname']\n",
    "        block = G.nodes[node]['block_name']\n",
    "        key = (pr, lr, block)\n",
    "        \n",
    "        if key not in nodes_by_ring:\n",
    "            nodes_by_ring[key] = []\n",
    "        nodes_by_ring[key].append(node)\n",
    "    \n",
    "    # Helper function to calculate path redundancy\n",
    "    def get_path_redundancy(G, node, block):\n",
    "        \"\"\"Return number of edge-disjoint paths from node to block\"\"\"\n",
    "        try:\n",
    "            return len(list(nx.edge_disjoint_paths(G, node, block)))\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    # Helper function to calculate vulnerability score\n",
    "    def calculate_vulnerability_score(G, node, block, articulation_points, bridges):\n",
    "        \"\"\"Calculate a node's vulnerability to isolation\"\"\"\n",
    "        vulnerability = 0.0\n",
    "        \n",
    "        # Factor 1: Few paths to block\n",
    "        paths = get_path_redundancy(G, node, block)\n",
    "        if paths == 0:\n",
    "            vulnerability += 1.0  # Already isolated\n",
    "        elif paths == 1:\n",
    "            vulnerability += 0.6  # Critical - only one path\n",
    "        elif paths == 2:\n",
    "            vulnerability += 0.3  # Vulnerable - two paths\n",
    "        \n",
    "        # Factor 2: Node is articulation point\n",
    "        if node in articulation_points:\n",
    "            vulnerability += 0.2\n",
    "        \n",
    "        # Factor 3: Connected to bridges\n",
    "        bridge_connections = 0\n",
    "        for u, v in bridges:\n",
    "            if node == u or node == v:\n",
    "                bridge_connections += 1\n",
    "        vulnerability += min(0.3, bridge_connections * 0.1)\n",
    "        \n",
    "        # Factor 4: Path to block goes through articulation points\n",
    "        try:\n",
    "            path = nx.shortest_path(G, node, block)\n",
    "            critical_on_path = sum(1 for n in path[1:-1] if n in articulation_points)\n",
    "            vulnerability += min(0.3, critical_on_path * 0.1)\n",
    "        except:\n",
    "            vulnerability += 0.2  # No path exists\n",
    "        \n",
    "        return min(1.0, vulnerability)  # Cap at 1.0\n",
    "    \n",
    "    print(\"Creating node features...\")\n",
    "    # Create isolation-focused node features\n",
    "    node_features = []\n",
    "    for node in node_list:\n",
    "        features = []\n",
    "        \n",
    "        # Basic features\n",
    "        pr = G.nodes[node]['physicalringname']\n",
    "        pr_hash = hash(pr) % 10\n",
    "        features.append(pr_hash/10.0)\n",
    "        \n",
    "        lr = G.nodes[node]['lrname']\n",
    "        lr_hash = hash(lr) % 10\n",
    "        features.append(lr_hash/10.0)\n",
    "        \n",
    "        is_block = 1.0 if \"BLOCK\" in node else 0.0\n",
    "        features.append(is_block)\n",
    "        \n",
    "        # Skip advanced features for block nodes\n",
    "        if is_block:\n",
    "            # Add placeholder values for block nodes to match feature dimensions\n",
    "            features.extend([0.0] * 7)  # 7 extra features for non-block nodes\n",
    "        else:\n",
    "            # Get block for this node\n",
    "            block = G.nodes[node]['block_name']\n",
    "            \n",
    "            # 1. Path redundancy - key indicator of isolation risk\n",
    "            path_redundancy = get_path_redundancy(G, node, block) / 5.0\n",
    "            features.append(path_redundancy)\n",
    "            \n",
    "            # 2. Critical point features\n",
    "            is_articulation = 1.0 if node in articulation_points else 0.0\n",
    "            has_bridge = any((node == u or node == v) for u, v in bridges)\n",
    "            features.extend([is_articulation, float(has_bridge)])\n",
    "            \n",
    "            # 3. Path to block features\n",
    "            try:\n",
    "                path_length = nx.shortest_path_length(G, node, block) / 10.0\n",
    "                path = nx.shortest_path(G, node, block)\n",
    "                critical_on_path = len(set(path[1:-1]).intersection(articulation_points)) / 5.0\n",
    "            except:\n",
    "                path_length = 1.0\n",
    "                critical_on_path = 1.0\n",
    "            features.extend([path_length, critical_on_path])\n",
    "            \n",
    "            # 4. Vulnerability score (combined metric)\n",
    "            vulnerability = calculate_vulnerability_score(G, node, block, articulation_points, bridges)\n",
    "            features.append(vulnerability)\n",
    "            \n",
    "            # 5. Network centrality\n",
    "            degree = G.degree(node) / len(G)\n",
    "            features.append(degree)\n",
    "        \n",
    "        node_features.append(features)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Create edge index\n",
    "    edges = []\n",
    "    for u, v in G.edges():\n",
    "        edges.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edges.append([node_to_idx[v], node_to_idx[u]])  # Add reverse edge for undirected graph\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Track dataset statistics\n",
    "    examples_with_isolations = []\n",
    "    examples_without_isolations = []\n",
    "    isolation_counts = []\n",
    "    failure_counts = []\n",
    "    \n",
    "    print(\"Generating dataset examples...\")\n",
    "    # PHASE 1: Generate examples that are likely to have isolations (50% of dataset)\n",
    "    target_isolation_examples = num_simulations // 2\n",
    "    phase1_attempts = 0\n",
    "    \n",
    "    while len(examples_with_isolations) < target_isolation_examples and phase1_attempts < num_simulations * 2:\n",
    "        phase1_attempts += 1\n",
    "        \n",
    "        # Choose failure type (node or edge)\n",
    "        failure_type = random.choice(['node', 'edge'])\n",
    "        \n",
    "        if failure_type == 'node':\n",
    "            # STRATEGY: Target critical nodes or nodes in the same ring\n",
    "            strategy = random.choice(['critical', 'same_ring', 'random'])\n",
    "            \n",
    "            if strategy == 'critical':\n",
    "                # Target articulation points and their neighbors\n",
    "                potential_targets = list(articulation_points.intersection(set(regular_nodes)))\n",
    "                if len(potential_targets) == 0:\n",
    "                    potential_targets = regular_nodes\n",
    "                \n",
    "                # Select 1-3 critical nodes\n",
    "                num_failures = random.randint(1, min(max_failures, len(potential_targets)))\n",
    "                nodes_to_fail = random.sample(potential_targets, num_failures)\n",
    "                \n",
    "            elif strategy == 'same_ring':\n",
    "                # Target multiple nodes in the same ring\n",
    "                if not nodes_by_ring:\n",
    "                    # Fallback to random\n",
    "                    nodes_to_fail = random.sample(regular_nodes, random.randint(1, min(max_failures, len(regular_nodes))))\n",
    "                else:\n",
    "                    # Pick a random ring\n",
    "                    ring_key = random.choice(list(nodes_by_ring.keys()))\n",
    "                    ring_nodes = nodes_by_ring[ring_key]\n",
    "                    \n",
    "                    # Select 1-3 nodes from this ring\n",
    "                    num_failures = random.randint(1, min(max_failures, len(ring_nodes)))\n",
    "                    nodes_to_fail = random.sample(ring_nodes, num_failures)\n",
    "            \n",
    "            else:  # random strategy\n",
    "                num_failures = random.randint(1, min(max_failures, len(regular_nodes)))\n",
    "                nodes_to_fail = random.sample(regular_nodes, num_failures)\n",
    "            \n",
    "            # Simulate failures\n",
    "            isolated_nodes = simulate_multiple_failures(G, nodes_to_fail, 'node')\n",
    "            \n",
    "            # Create target tensor\n",
    "            y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "            for node in isolated_nodes:\n",
    "                if node in node_to_idx:\n",
    "                    y[node_to_idx[node]] = 1.0\n",
    "            \n",
    "            # Create edge mask\n",
    "            valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "            for node in nodes_to_fail:\n",
    "                node_idx = node_to_idx[node]\n",
    "                invalid_edges = (edge_index[0] == node_idx) | (edge_index[1] == node_idx)\n",
    "                valid_edge_mask[invalid_edges] = False\n",
    "            \n",
    "            # Create data object\n",
    "            data = Data(\n",
    "                x=node_features,\n",
    "                edge_index=edge_index,\n",
    "                y=y,\n",
    "                valid_edge_mask=valid_edge_mask,\n",
    "                failure_type=torch.tensor([0]),\n",
    "                num_failures=torch.tensor([len(nodes_to_fail)], dtype=torch.long)\n",
    "            )\n",
    "            \n",
    "        else:  # edge failure\n",
    "            # STRATEGY: Target bridge edges or edges in the same ring\n",
    "            strategy = random.choice(['bridges', 'same_ring', 'random'])\n",
    "            \n",
    "            if strategy == 'bridges':\n",
    "                # Target bridge edges\n",
    "                potential_targets = [(u, v) for u, v in bridges if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                if len(potential_targets) == 0:\n",
    "                    # Fallback to random edges\n",
    "                    all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                    potential_targets = all_edges\n",
    "                \n",
    "                # Select 1-3 bridge edges\n",
    "                num_failures = random.randint(1, min(max_failures, len(potential_targets)))\n",
    "                edges_to_fail = random.sample(potential_targets, num_failures)\n",
    "                \n",
    "            elif strategy == 'same_ring':\n",
    "                # Target multiple edges in the same ring\n",
    "                ring_edges = {}\n",
    "                for u, v in G.edges():\n",
    "                    if \"BLOCK\" in u or \"BLOCK\" in v:\n",
    "                        continue\n",
    "                        \n",
    "                    try:\n",
    "                        pr_u = G.nodes[u]['physicalringname']\n",
    "                        lr_u = G.nodes[u]['lrname']\n",
    "                        pr_v = G.nodes[v]['physicalringname']\n",
    "                        lr_v = G.nodes[v]['lrname']\n",
    "                        \n",
    "                        if pr_u == pr_v and lr_u == lr_v:\n",
    "                            key = (pr_u, lr_u)\n",
    "                            if key not in ring_edges:\n",
    "                                ring_edges[key] = []\n",
    "                            ring_edges[key].append((u, v))\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if not ring_edges:\n",
    "                    # Fallback to random\n",
    "                    all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                    num_failures = random.randint(1, min(max_failures, len(all_edges)))\n",
    "                    edges_to_fail = random.sample(all_edges, num_failures)\n",
    "                else:\n",
    "                    # Pick a random ring\n",
    "                    ring_key = random.choice(list(ring_edges.keys()))\n",
    "                    ring_edge_list = ring_edges[ring_key]\n",
    "                    \n",
    "                    # Select 1-3 edges from this ring\n",
    "                    num_failures = random.randint(1, min(max_failures, len(ring_edge_list)))\n",
    "                    edges_to_fail = random.sample(ring_edge_list, num_failures)\n",
    "            \n",
    "            else:  # random strategy\n",
    "                all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                num_failures = random.randint(1, min(max_failures, len(all_edges)))\n",
    "                edges_to_fail = random.sample(all_edges, num_failures)\n",
    "            \n",
    "            # Simulate failures\n",
    "            isolated_nodes = simulate_multiple_failures(G, edges_to_fail, 'edge')\n",
    "            \n",
    "            # Create target tensor\n",
    "            y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "            for node in isolated_nodes:\n",
    "                if node in node_to_idx:\n",
    "                    y[node_to_idx[node]] = 1.0\n",
    "            \n",
    "            # Create edge mask\n",
    "            valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "            for u, v in edges_to_fail:\n",
    "                u_idx, v_idx = node_to_idx[u], node_to_idx[v]\n",
    "                \n",
    "                for i in range(edge_index.size(1)):\n",
    "                    e_u, e_v = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "                    if (e_u == u_idx and e_v == v_idx) or (e_u == v_idx and e_v == u_idx):\n",
    "                        valid_edge_mask[i] = False\n",
    "            \n",
    "            # Create data object\n",
    "            data = Data(\n",
    "                x=node_features,\n",
    "                edge_index=edge_index,\n",
    "                y=y,\n",
    "                valid_edge_mask=valid_edge_mask,\n",
    "                failure_type=torch.tensor([1]),\n",
    "                num_failures=torch.tensor([len(edges_to_fail)], dtype=torch.long)\n",
    "            )\n",
    "        \n",
    "        # Track isolation information\n",
    "        isolation_count = y.sum().item()\n",
    "        isolation_counts.append(isolation_count)\n",
    "        failure_counts.append(len(nodes_to_fail if failure_type == 'node' else edges_to_fail))\n",
    "        \n",
    "        # Add to appropriate list\n",
    "        if isolation_count > 0:\n",
    "            examples_with_isolations.append(data)\n",
    "        else:\n",
    "            examples_without_isolations.append(data)\n",
    "            \n",
    "        # Report progress\n",
    "        if phase1_attempts % 100 == 0:\n",
    "            print(f\"Phase 1: Generated {len(examples_with_isolations)} examples with isolations after {phase1_attempts} attempts\")\n",
    "    \n",
    "    # PHASE 2: Generate remaining examples randomly to complete the dataset\n",
    "    remaining = num_simulations - len(examples_with_isolations) - len(examples_without_isolations)\n",
    "    \n",
    "    if remaining > 0:\n",
    "        print(f\"Generating {remaining} additional examples for Phase 2...\")\n",
    "        \n",
    "        for _ in range(remaining):\n",
    "            # Choose failure type (node or edge)\n",
    "            failure_type = random.choice(['node', 'edge'])\n",
    "            \n",
    "            if failure_type == 'node':\n",
    "                # Choose random nodes\n",
    "                num_failures = random.randint(1, min(max_failures, len(regular_nodes)))\n",
    "                nodes_to_fail = random.sample(regular_nodes, num_failures)\n",
    "                \n",
    "                # Simulate failures\n",
    "                isolated_nodes = simulate_multiple_failures(G, nodes_to_fail, 'node')\n",
    "                \n",
    "                # Create target tensor\n",
    "                y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "                for node in isolated_nodes:\n",
    "                    if node in node_to_idx:\n",
    "                        y[node_to_idx[node]] = 1.0\n",
    "                \n",
    "                # Create edge mask\n",
    "                valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "                for node in nodes_to_fail:\n",
    "                    node_idx = node_to_idx[node]\n",
    "                    invalid_edges = (edge_index[0] == node_idx) | (edge_index[1] == node_idx)\n",
    "                    valid_edge_mask[invalid_edges] = False\n",
    "                \n",
    "                # Create data object\n",
    "                data = Data(\n",
    "                    x=node_features,\n",
    "                    edge_index=edge_index,\n",
    "                    y=y,\n",
    "                    valid_edge_mask=valid_edge_mask,\n",
    "                    failure_type=torch.tensor([0]),\n",
    "                    num_failures=torch.tensor([len(nodes_to_fail)], dtype=torch.long)\n",
    "                )\n",
    "                \n",
    "            else:  # edge failure\n",
    "                # Choose random edges\n",
    "                all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                num_failures = random.randint(1, min(max_failures, len(all_edges)))\n",
    "                edges_to_fail = random.sample(all_edges, num_failures)\n",
    "                \n",
    "                # Simulate failures\n",
    "                isolated_nodes = simulate_multiple_failures(G, edges_to_fail, 'edge')\n",
    "                \n",
    "                # Create target tensor\n",
    "                y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "                for node in isolated_nodes:\n",
    "                    if node in node_to_idx:\n",
    "                        y[node_to_idx[node]] = 1.0\n",
    "                \n",
    "                # Create edge mask\n",
    "                valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "                for u, v in edges_to_fail:\n",
    "                    u_idx, v_idx = node_to_idx[u], node_to_idx[v]\n",
    "                    \n",
    "                    for i in range(edge_index.size(1)):\n",
    "                        e_u, e_v = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "                        if (e_u == u_idx and e_v == v_idx) or (e_u == v_idx and e_v == u_idx):\n",
    "                            valid_edge_mask[i] = False\n",
    "                \n",
    "                # Create data object\n",
    "                data = Data(\n",
    "                    x=node_features,\n",
    "                    edge_index=edge_index,\n",
    "                    y=y,\n",
    "                    valid_edge_mask=valid_edge_mask,\n",
    "                    failure_type=torch.tensor([1]),\n",
    "                    num_failures=torch.tensor([len(edges_to_fail)], dtype=torch.long)\n",
    "                )\n",
    "            \n",
    "            # Track isolation information\n",
    "            isolation_count = y.sum().item()\n",
    "            isolation_counts.append(isolation_count)\n",
    "            failure_counts.append(len(nodes_to_fail if failure_type == 'node' else edges_to_fail))\n",
    "            \n",
    "            # Add to appropriate list\n",
    "            if isolation_count > 0:\n",
    "                examples_with_isolations.append(data)\n",
    "            else:\n",
    "                examples_without_isolations.append(data)\n",
    "    \n",
    "    # Balance the dataset using the specified ratio\n",
    "    print(f\"Balancing dataset with isolation:non-isolation ratio of 1:{balance_ratio}\")\n",
    "    if len(examples_without_isolations) > len(examples_with_isolations) * balance_ratio:\n",
    "        # Downsample negative examples\n",
    "        target_non_isolations = len(examples_with_isolations) * balance_ratio\n",
    "        examples_without_isolations = random.sample(examples_without_isolations, int(target_non_isolations))\n",
    "    \n",
    "    # Combine and shuffle the final dataset\n",
    "    final_data_list = examples_with_isolations + examples_without_isolations\n",
    "    random.shuffle(final_data_list)\n",
    "    \n",
    "    # Print dataset statistics\n",
    "    total_examples = len(final_data_list)\n",
    "    total_isolations = len(examples_with_isolations)\n",
    "    total_non_isolations = len(examples_without_isolations)\n",
    "    \n",
    "    print(\"\\n=== Dataset Statistics ===\")\n",
    "    print(f\"Total examples: {total_examples}\")\n",
    "    print(f\"Examples with isolations: {total_isolations} ({total_isolations/total_examples*100:.1f}%)\")\n",
    "    print(f\"Examples without isolations: {total_non_isolations} ({total_non_isolations/total_examples*100:.1f}%)\")\n",
    "    print(f\"Average isolations per positive example: {sum(c for c in isolation_counts if c > 0)/max(1, total_isolations):.2f}\")\n",
    "    print(f\"Average failures per example: {sum(failure_counts)/len(failure_counts):.2f}\")\n",
    "    \n",
    "    # Detailed breakdown by failure type\n",
    "    node_isolations = sum(1 for data in examples_with_isolations if data.failure_type.item() == 0)\n",
    "    edge_isolations = sum(1 for data in examples_with_isolations if data.failure_type.item() == 1)\n",
    "    \n",
    "    print(\"\\nIsolation examples by failure type:\")\n",
    "    print(f\"  Node failures: {node_isolations} ({node_isolations/max(1, total_isolations)*100:.1f}%)\")\n",
    "    print(f\"  Edge failures: {edge_isolations} ({edge_isolations/max(1, total_isolations)*100:.1f}%)\")\n",
    "    \n",
    "    # Breakdown by number of failures\n",
    "    failure_counts_dist = {}\n",
    "    for data in final_data_list:\n",
    "        num_fails = data.num_failures.item()\n",
    "        has_isolations = data.y.sum().item() > 0\n",
    "        \n",
    "        key = f\"{num_fails}_{'iso' if has_isolations else 'non'}\"\n",
    "        if key not in failure_counts_dist:\n",
    "            failure_counts_dist[key] = 0\n",
    "        failure_counts_dist[key] += 1\n",
    "    \n",
    "    print(\"\\nExamples by number of failures:\")\n",
    "    for k in sorted(failure_counts_dist.keys()):\n",
    "        num, iso_status = k.split('_')\n",
    "        status = \"with isolations\" if iso_status == 'iso' else \"without isolations\"\n",
    "        print(f\"  {num} failures {status}: {failure_counts_dist[k]}\")\n",
    "    \n",
    "    return final_data_list, node_list, node_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationPredictionGNN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, edge_dim=None):\n",
    "        super(IsolationPredictionGNN, self).__init__()\n",
    "        # Increase hidden dimensions\n",
    "        hidden_dim = 128\n",
    "        \n",
    "        # Pre-failure feature extraction\n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(num_node_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Edge features processing if available\n",
    "        self.use_edge_features = edge_dim is not None\n",
    "        if self.use_edge_features:\n",
    "            self.edge_encoder = nn.Sequential(\n",
    "                nn.Linear(edge_dim, hidden_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        # Initial structure learning (before failure)\n",
    "        self.conv1 = GATConv(hidden_dim, hidden_dim//4, heads=4, concat=True, dropout=0.3)\n",
    "        self.conv2 = GATConv(hidden_dim, hidden_dim//4, heads=4, concat=True, dropout=0.3)\n",
    "        \n",
    "        # Post-failure processing with separate convolutions\n",
    "        self.post_failure_conv1 = GATConv(hidden_dim, hidden_dim//4, heads=4, concat=True, dropout=0.3)\n",
    "        self.post_failure_conv2 = GATConv(hidden_dim, hidden_dim//4, heads=4, concat=True, dropout=0.3)\n",
    "        \n",
    "        # Explicit isolation prediction with deeper network\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim//2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim//2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # Get node features\n",
    "        x = self.node_encoder(x)\n",
    "        \n",
    "        # Process edge features if available\n",
    "        edge_attr = None\n",
    "        if self.use_edge_features and hasattr(data, 'edge_attr'):\n",
    "            edge_attr = self.edge_encoder(data.edge_attr)\n",
    "        \n",
    "        # Learn network structure BEFORE failure\n",
    "        h1 = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        h1 = F.dropout(h1, p=0.3, training=self.training)\n",
    "        h_before = F.relu(self.conv2(h1, edge_index, edge_attr))\n",
    "        \n",
    "        # Apply failure mask and re-process\n",
    "        if hasattr(data, 'valid_edge_mask') and data.valid_edge_mask is not None:\n",
    "            # Use only valid edges (those not failed)\n",
    "            valid_edges = edge_index[:, data.valid_edge_mask]\n",
    "            \n",
    "            if self.use_edge_features and edge_attr is not None:\n",
    "                valid_edge_attr = edge_attr[data.valid_edge_mask]\n",
    "            else:\n",
    "                valid_edge_attr = None\n",
    "                \n",
    "            # Process AFTER failure with the post-failure convolutions\n",
    "            if valid_edges.size(1) > 0:  # Only if we have valid edges\n",
    "                h2 = F.relu(self.post_failure_conv1(h_before, valid_edges, valid_edge_attr))\n",
    "                h2 = F.dropout(h2, p=0.3, training=self.training)\n",
    "                h_after = F.relu(self.post_failure_conv2(h2, valid_edges, valid_edge_attr))\n",
    "            else:\n",
    "                # If all edges failed, just use the node features\n",
    "                h_after = h_before  # Cannot propagate anything\n",
    "        else:\n",
    "            # No failures applied, use the same edges\n",
    "            h_after = h_before\n",
    "        \n",
    "        # Concatenate before and after state for each node\n",
    "        # This explicitly models the change from pre-failure to post-failure\n",
    "        combined = torch.cat([h_before, h_after], dim=1)\n",
    "        \n",
    "        # Predict isolation probability\n",
    "        return self.predictor(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_isolation_prediction_dataset(topology_df, num_simulations=5000, max_failures=5, min_isolation_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Create a high-quality dataset for training a GNN to predict node isolations after failures.\n",
    "    \n",
    "    Args:\n",
    "        topology_df: DataFrame with network topology information\n",
    "        num_simulations: Total number of examples to generate\n",
    "        max_failures: Maximum number of simultaneous failures to simulate\n",
    "        min_isolation_ratio: Minimum ratio of examples that must contain isolations\n",
    "        \n",
    "    Returns:\n",
    "        data_list: List of PyG Data objects\n",
    "        node_list: List of all node names\n",
    "        node_to_idx: Dictionary mapping node names to indices\n",
    "    \"\"\"\n",
    "    print(\"Building network graph...\")\n",
    "    G = build_network_graph(topology_df)\n",
    "    \n",
    "    # Create node mapping\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # Identify regular nodes (non-block nodes)\n",
    "    regular_nodes = [n for n in node_list if \"BLOCK\" not in n]\n",
    "    \n",
    "    print(\"Computing network vulnerability metrics...\")\n",
    "    # Pre-compute network properties for better feature engineering\n",
    "    articulation_points = set(nx.articulation_points(G))\n",
    "    bridges = list(nx.bridges(G))\n",
    "    \n",
    "    # Pre-compute edge betweenness - identifying critical edges\n",
    "    edge_betweenness = nx.edge_betweenness_centrality(G)\n",
    "    \n",
    "    # Group nodes by physical ring and logical ring\n",
    "    nodes_by_ring = {}\n",
    "    for node in regular_nodes:\n",
    "        if node not in G.nodes():\n",
    "            continue\n",
    "            \n",
    "        pr = G.nodes[node].get('physicalringname', 'unknown')\n",
    "        lr = G.nodes[node].get('lrname', 'unknown')\n",
    "        block = G.nodes[node].get('block_name', 'unknown')\n",
    "        key = (pr, lr, block)\n",
    "        \n",
    "        if key not in nodes_by_ring:\n",
    "            nodes_by_ring[key] = []\n",
    "        nodes_by_ring[key].append(node)\n",
    "    \n",
    "    print(\"Calculating node vulnerability scores...\")\n",
    "    # Calculate vulnerability score for each node\n",
    "    vulnerability_scores = {}\n",
    "    for node in regular_nodes:\n",
    "        if \"BLOCK\" in node:\n",
    "            vulnerability_scores[node] = 0.0\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            block = G.nodes[node].get('block_name', None)\n",
    "            if not block:\n",
    "                vulnerability_scores[node] = 0.5\n",
    "                continue\n",
    "                \n",
    "            # Factor 1: Path redundancy to block\n",
    "            try:\n",
    "                paths = len(list(nx.edge_disjoint_paths(G, node, block)))\n",
    "                if paths == 0:\n",
    "                    path_score = 1.0  # Already isolated\n",
    "                elif paths == 1: \n",
    "                    path_score = 0.8  # Critical - only one path\n",
    "                elif paths == 2:\n",
    "                    path_score = 0.5  # Vulnerable - two paths\n",
    "                else:\n",
    "                    path_score = 0.1  # Low risk\n",
    "            except:\n",
    "                path_score = 1.0  # Unable to calculate paths\n",
    "                \n",
    "            # Factor 2: Is it an articulation point?\n",
    "            is_articulation = 1.0 if node in articulation_points else 0.0\n",
    "            \n",
    "            # Factor 3: Connected to bridges\n",
    "            bridge_connections = sum(1 for u, v in bridges if node in (u, v))\n",
    "            bridge_score = min(1.0, bridge_connections * 0.2)\n",
    "            \n",
    "            # Factor 4: Betweenness centrality - how important is it for connecting other nodes\n",
    "            betweenness = nx.betweenness_centrality(G, k=min(100, len(G))).get(node, 0)\n",
    "            \n",
    "            # Factor 5: Closeness to block\n",
    "            try:\n",
    "                closeness = 1.0 / max(1.0, nx.shortest_path_length(G, node, block))\n",
    "            except:\n",
    "                closeness = 0.0\n",
    "                \n",
    "            # Combined score (weighted average)\n",
    "            vulnerability = (\n",
    "                0.4 * path_score + \n",
    "                0.2 * is_articulation + \n",
    "                0.2 * bridge_score + \n",
    "                0.1 * betweenness +\n",
    "                0.1 * closeness\n",
    "            )\n",
    "            \n",
    "            vulnerability_scores[node] = min(1.0, vulnerability)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating vulnerability for {node}: {e}\")\n",
    "            vulnerability_scores[node] = 0.5\n",
    "    \n",
    "    print(\"Creating node features...\")\n",
    "    # Create more informative node features\n",
    "    node_features = []\n",
    "    for node in node_list:\n",
    "        features = []\n",
    "        \n",
    "        # Basic features\n",
    "        pr = G.nodes[node].get('physicalringname', 'unknown')\n",
    "        pr_hash = int(hashlib.md5(pr.encode()).hexdigest(), 16) % 10\n",
    "        features.append(pr_hash/10.0)\n",
    "        \n",
    "        lr = G.nodes[node].get('lrname', 'unknown')\n",
    "        lr_hash = int(hashlib.md5(lr.encode()).hexdigest(), 16) % 10\n",
    "        features.append(lr_hash/10.0)\n",
    "        \n",
    "        is_block = 1.0 if \"BLOCK\" in node else 0.0\n",
    "        features.append(is_block)\n",
    "        \n",
    "        # Skip advanced features for block nodes\n",
    "        if is_block:\n",
    "            # Add placeholder values for block nodes\n",
    "            features.extend([0.0] * 8)  # 8 extra features \n",
    "        else:\n",
    "            # Topological importance features\n",
    "            degree = G.degree(node) / max(1, max(dict(G.degree()).values()))\n",
    "            features.append(degree)\n",
    "            \n",
    "            clustering = nx.clustering(G, node)\n",
    "            features.append(clustering)\n",
    "            \n",
    "            is_articulation = 1.0 if node in articulation_points else 0.0\n",
    "            features.append(is_articulation)\n",
    "            \n",
    "            # Edge connectivity features\n",
    "            bridges_connected = min(1.0, sum(1 for u, v in bridges if node in (u, v)) / 5.0)\n",
    "            features.append(bridges_connected)\n",
    "            \n",
    "            # Block connectivity\n",
    "            block = G.nodes[node].get('block_name', None)\n",
    "            if block:\n",
    "                try:\n",
    "                    path_length = nx.shortest_path_length(G, node, block) / 10.0\n",
    "                    disjoint_paths = len(list(nx.edge_disjoint_paths(G, node, block))) / 5.0\n",
    "                except:\n",
    "                    path_length = 1.0\n",
    "                    disjoint_paths = 0.0\n",
    "            else:\n",
    "                path_length = 1.0\n",
    "                disjoint_paths = 0.0\n",
    "            features.append(path_length)\n",
    "            features.append(disjoint_paths)\n",
    "            \n",
    "            # Centrality metrics\n",
    "            betweenness = nx.betweenness_centrality(G, k=min(100, len(G))).get(node, 0)\n",
    "            features.append(betweenness)\n",
    "            \n",
    "            # Pre-computed vulnerability score\n",
    "            vulnerability = vulnerability_scores.get(node, 0.5)\n",
    "            features.append(vulnerability)\n",
    "        \n",
    "        node_features.append(features)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Create edge index\n",
    "    edges = []\n",
    "    for u, v in G.edges():\n",
    "        edges.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edges.append([node_to_idx[v], node_to_idx[u]])  # Undirected graph\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Create edge features\n",
    "    edge_features = []\n",
    "    for u, v in G.edges():\n",
    "        # Compute various edge features\n",
    "        # 1. Edge betweenness\n",
    "        betweenness = edge_betweenness.get((u, v), edge_betweenness.get((v, u), 0))\n",
    "        \n",
    "        # 2. Same physical ring\n",
    "        same_pr = int(G.nodes[u].get('physicalringname') == G.nodes[v].get('physicalringname'))\n",
    "        \n",
    "        # 3. Same logical ring\n",
    "        same_lr = int(G.nodes[u].get('lrname') == G.nodes[v].get('lrname'))\n",
    "        \n",
    "        # 4. Is this a bridge?\n",
    "        is_bridge = 1.0 if (u, v) in bridges or (v, u) in bridges else 0.0\n",
    "        \n",
    "        # Duplicate for reverse edge\n",
    "        edge_features.append([betweenness, same_pr, same_lr, is_bridge])\n",
    "        edge_features.append([betweenness, same_pr, same_lr, is_bridge])\n",
    "    \n",
    "    edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "    \n",
    "    # Track dataset statistics\n",
    "    examples_with_isolations = []\n",
    "    examples_without_isolations = []\n",
    "    \n",
    "    # Generate dataset with strategic failure selection\n",
    "    print(\"Generating dataset examples...\")\n",
    "    \n",
    "    # PHASE 1: Generate examples targeting vulnerabilities\n",
    "    phase1_target = int(num_simulations * 0.7)  # 70% from targeted strategies\n",
    "    \n",
    "    while len(examples_with_isolations) + len(examples_without_isolations) < phase1_target:\n",
    "        # Choose failure type (node or edge with 70/30 split)\n",
    "        failure_type = 'node' if random.random() < 0.7 else 'edge'\n",
    "        \n",
    "        # Strategies weighted by effectiveness\n",
    "        if failure_type == 'node':\n",
    "            # Node failure strategies\n",
    "            strategies = [\n",
    "                ('high_vulnerability', 0.4),  # Target vulnerable nodes\n",
    "                ('articulation', 0.3),        # Target articulation points\n",
    "                ('same_ring', 0.2),           # Target nodes in same ring\n",
    "                ('random', 0.1)               # Random targets\n",
    "            ]\n",
    "            \n",
    "            strategy = random.choices([s[0] for s in strategies], \n",
    "                                     [s[1] for s in strategies])[0]\n",
    "            \n",
    "            if strategy == 'high_vulnerability':\n",
    "                # Target high vulnerability nodes\n",
    "                sorted_nodes = sorted([(n, vulnerability_scores.get(n, 0)) \n",
    "                                      for n in regular_nodes if \"BLOCK\" not in n],\n",
    "                                     key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                # Take from top 20% vulnerable nodes\n",
    "                top_vulnerable = sorted_nodes[:max(1, int(len(sorted_nodes) * 0.2))]\n",
    "                potential_targets = [n[0] for n in top_vulnerable]\n",
    "                \n",
    "                if not potential_targets:\n",
    "                    potential_targets = regular_nodes\n",
    "                \n",
    "                # Select 1-N failures\n",
    "                num_failures = random.randint(1, min(max_failures, len(potential_targets)))\n",
    "                nodes_to_fail = random.sample(potential_targets, num_failures)\n",
    "                \n",
    "            elif strategy == 'articulation':\n",
    "                # Target articulation points\n",
    "                potential_targets = list(articulation_points & set(regular_nodes))\n",
    "                if len(potential_targets) < 2:\n",
    "                    potential_targets = regular_nodes\n",
    "                    \n",
    "                num_failures = random.randint(1, min(max_failures, len(potential_targets)))\n",
    "                nodes_to_fail = random.sample(potential_targets, num_failures)\n",
    "                \n",
    "            elif strategy == 'same_ring':\n",
    "                # Select nodes in the same ring \n",
    "                if not nodes_by_ring:\n",
    "                    # Fallback to random\n",
    "                    nodes_to_fail = random.sample(regular_nodes, \n",
    "                                                 random.randint(1, min(max_failures, len(regular_nodes))))\n",
    "                else:\n",
    "                    # Pick a random ring group\n",
    "                    rings_with_enough_nodes = [key for key, nodes in nodes_by_ring.items() \n",
    "                                              if len(nodes) >= 2]\n",
    "                    \n",
    "                    if not rings_with_enough_nodes:\n",
    "                        # Fallback to random  \n",
    "                        nodes_to_fail = random.sample(regular_nodes,\n",
    "                                                     random.randint(1, min(max_failures, len(regular_nodes))))\n",
    "                    else:\n",
    "                        ring_key = random.choice(rings_with_enough_nodes)\n",
    "                        ring_nodes = nodes_by_ring[ring_key]\n",
    "                        \n",
    "                        # Select multiple nodes from this ring\n",
    "                        num_failures = random.randint(1, min(max_failures, len(ring_nodes)))\n",
    "                        nodes_to_fail = random.sample(ring_nodes, num_failures)\n",
    "            \n",
    "            else:  # random strategy\n",
    "                num_failures = random.randint(1, min(max_failures, len(regular_nodes)))\n",
    "                nodes_to_fail = random.sample(regular_nodes, num_failures)\n",
    "            \n",
    "            # Simulate failures\n",
    "            isolated_nodes = simulate_multiple_failures(G, nodes_to_fail, 'node')\n",
    "            \n",
    "            # Create target tensor\n",
    "            y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "            for node in isolated_nodes:\n",
    "                if node in node_to_idx:\n",
    "                    y[node_to_idx[node]] = 1.0\n",
    "            \n",
    "            # Create edge mask\n",
    "            valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "            for node in nodes_to_fail:\n",
    "                node_idx = node_to_idx[node]\n",
    "                invalid_edges = (edge_index[0] == node_idx) | (edge_index[1] == node_idx)\n",
    "                valid_edge_mask[invalid_edges] = False\n",
    "            \n",
    "            # Create data object\n",
    "            data = Data(\n",
    "                x=node_features,\n",
    "                edge_index=edge_index,\n",
    "                edge_attr=edge_attr,\n",
    "                y=y,\n",
    "                valid_edge_mask=valid_edge_mask,\n",
    "                failure_type=torch.tensor([0]),\n",
    "                failed_elements=[node_to_idx[n] for n in nodes_to_fail],\n",
    "                num_failures=torch.tensor([len(nodes_to_fail)], dtype=torch.long)\n",
    "            )\n",
    "            \n",
    "        else:  # edge failure\n",
    "            # Edge failure strategies\n",
    "            strategies = [\n",
    "                ('bridges', 0.4),       # Target bridge edges \n",
    "                ('high_betweenness', 0.3),  # Target high betweenness edges\n",
    "                ('same_ring', 0.2),     # Target edges in the same ring\n",
    "                ('random', 0.1)         # Random edges\n",
    "            ]\n",
    "            \n",
    "            strategy = random.choices([s[0] for s in strategies], \n",
    "                                     [s[1] for s in strategies])[0]\n",
    "            \n",
    "            if strategy == 'bridges':\n",
    "                # Target bridge edges\n",
    "                potential_targets = [(u, v) for u, v in bridges if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                if len(potential_targets) == 0:\n",
    "                    # Fallback to random edges\n",
    "                    all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                    potential_targets = all_edges\n",
    "                \n",
    "                # Select multiple bridge edges\n",
    "                num_failures = random.randint(1, min(max_failures, len(potential_targets)))\n",
    "                edges_to_fail = random.sample(potential_targets, num_failures)\n",
    "                \n",
    "            elif strategy == 'high_betweenness':\n",
    "                # Target high betweenness edges\n",
    "                sorted_edges = sorted([(u, v) for u, v in edge_betweenness.keys() \n",
    "                                     if \"BLOCK\" not in u and \"BLOCK\" not in v],\n",
    "                                    key=lambda e: edge_betweenness.get(e, 0), \n",
    "                                    reverse=True)\n",
    "                \n",
    "                # Take top 20% high betweenness edges\n",
    "                potential_targets = sorted_edges[:max(1, int(len(sorted_edges) * 0.2))]\n",
    "                \n",
    "                if not potential_targets:\n",
    "                    # Fallback to random\n",
    "                    all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                    potential_targets = all_edges\n",
    "                    \n",
    "                num_failures = random.randint(1, min(max_failures, len(potential_targets)))\n",
    "                edges_to_fail = random.sample(potential_targets, num_failures)\n",
    "                \n",
    "            elif strategy == 'same_ring':\n",
    "                # Target multiple edges in the same ring\n",
    "                ring_edges = {}\n",
    "                for u, v in G.edges():\n",
    "                    if \"BLOCK\" in u or \"BLOCK\" in v:\n",
    "                        continue\n",
    "                        \n",
    "                    try:\n",
    "                        pr_u = G.nodes[u].get('physicalringname')\n",
    "                        lr_u = G.nodes[u].get('lrname')\n",
    "                        pr_v = G.nodes[v].get('physicalringname')\n",
    "                        lr_v = G.nodes[v].get('lrname')\n",
    "                        \n",
    "                        if pr_u == pr_v and lr_u == lr_v:\n",
    "                            key = (pr_u, lr_u)\n",
    "                            if key not in ring_edges:\n",
    "                                ring_edges[key] = []\n",
    "                            ring_edges[key].append((u, v))\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if not ring_edges:\n",
    "                    # Fallback to random\n",
    "                    all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                    num_failures = random.randint(1, min(max_failures, len(all_edges)))\n",
    "                    edges_to_fail = random.sample(all_edges, num_failures)\n",
    "                else:\n",
    "                    # Pick a random ring\n",
    "                    ring_key = random.choice(list(ring_edges.keys()))\n",
    "                    ring_edge_list = ring_edges[ring_key]\n",
    "                    \n",
    "                    # Select multiple edges from this ring\n",
    "                    num_failures = random.randint(1, min(max_failures, len(ring_edge_list)))\n",
    "                    edges_to_fail = random.sample(ring_edge_list, num_failures)\n",
    "            \n",
    "            else:  # random strategy\n",
    "                all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                num_failures = random.randint(1, min(max_failures, len(all_edges)))\n",
    "                edges_to_fail = random.sample(all_edges, num_failures)\n",
    "            \n",
    "            # Simulate failures\n",
    "            isolated_nodes = simulate_multiple_failures(G, edges_to_fail, 'edge')\n",
    "            \n",
    "            # Create target tensor\n",
    "            y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "            for node in isolated_nodes:\n",
    "                if node in node_to_idx:\n",
    "                    y[node_to_idx[node]] = 1.0\n",
    "            \n",
    "            # Create edge mask\n",
    "            valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "            failed_edge_indices = []\n",
    "            \n",
    "            for u, v in edges_to_fail:\n",
    "                u_idx, v_idx = node_to_idx[u], node_to_idx[v]\n",
    "                \n",
    "                for i in range(edge_index.size(1)):\n",
    "                    e_u, e_v = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "                    if (e_u == u_idx and e_v == v_idx) or (e_u == v_idx and e_v == u_idx):\n",
    "                        valid_edge_mask[i] = False\n",
    "                        failed_edge_indices.append(i)\n",
    "            \n",
    "            # Create data object\n",
    "            data = Data(\n",
    "                x=node_features,\n",
    "                edge_index=edge_index,\n",
    "                edge_attr=edge_attr,\n",
    "                y=y,\n",
    "                valid_edge_mask=valid_edge_mask,\n",
    "                failure_type=torch.tensor([1]),\n",
    "                failed_elements=failed_edge_indices,\n",
    "                num_failures=torch.tensor([len(edges_to_fail)], dtype=torch.long)\n",
    "            )\n",
    "        \n",
    "        # Add to appropriate list\n",
    "        if y.sum().item() > 0:\n",
    "            examples_with_isolations.append(data)\n",
    "        else:\n",
    "            examples_without_isolations.append(data)\n",
    "            \n",
    "        # Report progress\n",
    "        total_examples = len(examples_with_isolations) + len(examples_without_isolations)\n",
    "        if total_examples % 100 == 0:\n",
    "            isolation_ratio = len(examples_with_isolations) / max(1, total_examples)\n",
    "            print(f\"Generated {total_examples} examples, {len(examples_with_isolations)} with isolations ({isolation_ratio:.2f})\")\n",
    "    \n",
    "    # PHASE 2: Generate remaining examples with random approach\n",
    "    phase2_target = num_simulations - len(examples_with_isolations) - len(examples_without_isolations)\n",
    "    \n",
    "    if phase2_target > 0:\n",
    "        print(f\"Generating {phase2_target} additional examples for Phase 2...\")\n",
    "        \n",
    "        for _ in range(phase2_target):\n",
    "            # Choose failure type (node or edge)\n",
    "            failure_type = random.choice(['node', 'edge'])\n",
    "            \n",
    "            if failure_type == 'node':\n",
    "                # Choose random nodes\n",
    "                num_failures = random.randint(1, min(max_failures, len(regular_nodes)))\n",
    "                nodes_to_fail = random.sample(regular_nodes, num_failures)\n",
    "                \n",
    "                # Simulate failures\n",
    "                isolated_nodes = simulate_multiple_failures(G, nodes_to_fail, 'node')\n",
    "                \n",
    "                # Create target tensor\n",
    "                y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "                for node in isolated_nodes:\n",
    "                    if node in node_to_idx:\n",
    "                        y[node_to_idx[node]] = 1.0\n",
    "                \n",
    "                # Create edge mask\n",
    "                valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "                for node in nodes_to_fail:\n",
    "                    node_idx = node_to_idx[node]\n",
    "                    invalid_edges = (edge_index[0] == node_idx) | (edge_index[1] == node_idx)\n",
    "                    valid_edge_mask[invalid_edges] = False\n",
    "                \n",
    "                # Create data object\n",
    "                data = Data(\n",
    "                    x=node_features,\n",
    "                    edge_index=edge_index,\n",
    "                    edge_attr=edge_attr,\n",
    "                    y=y,\n",
    "                    valid_edge_mask=valid_edge_mask,\n",
    "                    failure_type=torch.tensor([0]),\n",
    "                    failed_elements=[node_to_idx[n] for n in nodes_to_fail],\n",
    "                    num_failures=torch.tensor([len(nodes_to_fail)], dtype=torch.long)\n",
    "                )\n",
    "                \n",
    "            else:  # edge failure\n",
    "                # Choose random edges\n",
    "                all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                num_failures = random.randint(1, min(max_failures, len(all_edges)))\n",
    "                edges_to_fail = random.sample(all_edges, num_failures)\n",
    "                \n",
    "                # Simulate failures\n",
    "                isolated_nodes = simulate_multiple_failures(G, edges_to_fail, 'edge')\n",
    "                \n",
    "                # Create target tensor\n",
    "                y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "                for node in isolated_nodes:\n",
    "                    if node in node_to_idx:\n",
    "                        y[node_to_idx[node]] = 1.0\n",
    "                \n",
    "                # Create edge mask\n",
    "                valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "                failed_edge_indices = []\n",
    "                \n",
    "                for u, v in edges_to_fail:\n",
    "                    u_idx, v_idx = node_to_idx[u], node_to_idx[v]\n",
    "                    \n",
    "                    for i in range(edge_index.size(1)):\n",
    "                        e_u, e_v = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "                        if (e_u == u_idx and e_v == v_idx) or (e_u == v_idx and e_v == u_idx):\n",
    "                            valid_edge_mask[i] = False\n",
    "                            failed_edge_indices.append(i)\n",
    "                \n",
    "                # Create data object\n",
    "                data = Data(\n",
    "                    x=node_features,\n",
    "                    edge_index=edge_index,\n",
    "                    edge_attr=edge_attr,\n",
    "                    y=y,\n",
    "                    valid_edge_mask=valid_edge_mask,\n",
    "                    failure_type=torch.tensor([1]),\n",
    "                    failed_elements=failed_edge_indices,\n",
    "                    num_failures=torch.tensor([len(edges_to_fail)], dtype=torch.long)\n",
    "                )\n",
    "            \n",
    "            # Add to appropriate list\n",
    "            if y.sum().item() > 0:\n",
    "                examples_with_isolations.append(data)\n",
    "            else:\n",
    "                examples_without_isolations.append(data)\n",
    "    \n",
    "    # Balance the dataset based on min_isolation_ratio\n",
    "    isolation_count = len(examples_with_isolations)\n",
    "    total_count = isolation_count + len(examples_without_isolations)\n",
    "    current_ratio = isolation_count / total_count\n",
    "    \n",
    "    print(f\"Current isolation ratio: {current_ratio:.2f}, target: {min_isolation_ratio:.2f}\")\n",
    "    \n",
    "    if current_ratio < min_isolation_ratio and len(examples_without_isolations) > 0:\n",
    "        # Need to remove some non-isolation examples\n",
    "        target_non_isolation = int(isolation_count * (1 - min_isolation_ratio) / min_isolation_ratio)\n",
    "        if target_non_isolation < len(examples_without_isolations):\n",
    "            print(f\"Downsampling non-isolation examples from {len(examples_without_isolations)} to {target_non_isolation}\")\n",
    "            examples_without_isolations = random.sample(examples_without_isolations, target_non_isolation)\n",
    "    \n",
    "    # Combine and shuffle the final dataset\n",
    "    final_data_list = examples_with_isolations + examples_without_isolations\n",
    "    random.shuffle(final_data_list)\n",
    "    \n",
    "    # Print dataset statistics\n",
    "    total_examples = len(final_data_list)\n",
    "    total_isolations = len(examples_with_isolations)\n",
    "    total_non_isolations = len(examples_without_isolations)\n",
    "    \n",
    "    print(\"\\n=== Dataset Statistics ===\")\n",
    "    print(f\"Total examples: {total_examples}\")\n",
    "    print(f\"Examples with isolations: {total_isolations} ({total_isolations/total_examples*100:.1f}%)\")\n",
    "    print(f\"Examples without isolations: {total_non_isolations} ({total_non_isolations/total_examples*100:.1f}%)\")\n",
    "    \n",
    "    # Calculate average isolations per positive example\n",
    "    avg_isolations = sum(data.y.sum().item() for data in examples_with_isolations) / max(1, total_isolations)\n",
    "    print(f\"Average isolated nodes per positive example: {avg_isolations:.2f}\")\n",
    "    \n",
    "    # Detailed breakdown by failure type\n",
    "    node_failures = sum(1 for data in final_data_list if data.failure_type.item() == 0)\n",
    "    edge_failures = sum(1 for data in final_data_list if data.failure_type.item() == 1)\n",
    "    \n",
    "    node_isolations = sum(1 for data in examples_with_isolations if data.failure_type.item() == 0)\n",
    "    edge_isolations = sum(1 for data in examples_with_isolations if data.failure_type.item() == 1)\n",
    "    \n",
    "    print(\"\\nExamples by failure type:\")\n",
    "    print(f\"  Node failures: {node_failures} ({node_failures/total_examples*100:.1f}%)\")\n",
    "    print(f\"  Edge failures: {edge_failures} ({edge_failures/total_examples*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nIsolation examples by failure type:\")\n",
    "    print(f\"  Node failures with isolations: {node_isolations} ({node_isolations/total_isolations*100:.1f}%)\")\n",
    "    print(f\"  Edge failures with isolations: {edge_isolations} ({edge_isolations/total_isolations*100:.1f}%)\")\n",
    "    \n",
    "    # Breakdown by number of failures\n",
    "    failure_counts = {}\n",
    "    for data in final_data_list:\n",
    "        num_fails = data.num_failures.item()\n",
    "        has_isolations = data.y.sum().item() > 0\n",
    "        \n",
    "        key = f\"{num_fails}_{'iso' if has_isolations else 'non'}\"\n",
    "        if key not in failure_counts:\n",
    "            failure_counts[key] = 0\n",
    "        failure_counts[key] += 1\n",
    "    \n",
    "    print(\"\\nExamples by number of failures:\")\n",
    "    for k in sorted(failure_counts.keys()):\n",
    "        num, iso_status = k.split('_')\n",
    "        status = \"with isolations\" if iso_status == 'iso' else \"without isolations\"\n",
    "        print(f\"  {num} failures {status}: {failure_counts[k]}\")\n",
    "    \n",
    "    return final_data_list, node_list, node_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def train_isolation_model(data_list, epochs=50):\n",
    "    # Split data\n",
    "    total = len(data_list)\n",
    "    train_size = int(0.7 * total)\n",
    "    val_size = int(0.15 * total)\n",
    "    test_size = total - train_size - val_size\n",
    "    \n",
    "    # Shuffle and split\n",
    "    random.shuffle(data_list)\n",
    "    train_data = data_list[:train_size]\n",
    "    val_data = data_list[train_size:train_size+val_size]\n",
    "    test_data = data_list[train_size+val_size:]\n",
    "    \n",
    "    # Count positives in training set\n",
    "    positive_count = sum(data.y.sum().item() > 0 for data in train_data)\n",
    "    positive_rate = positive_count / len(train_data)\n",
    "    print(f\"Training set: {len(train_data)} examples, {positive_count} with isolations ({positive_rate:.2%})\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Initialize model with edge features if available\n",
    "    edge_dim = train_data[0].edge_attr.size(1) if hasattr(train_data[0], 'edge_attr') else None\n",
    "    model = IsolationPredictionGNN(num_node_features=train_data[0].x.size(1), edge_dim=edge_dim)\n",
    "    \n",
    "    # Calculate positive weight based on class imbalance\n",
    "    # This is crucial! Use a large value (50.0) to force the model to predict positives\n",
    "    pos_weight = torch.tensor(30.0)  # Very high weight for rare isolation cases\n",
    "    \n",
    "    # You could also calculate from your data:\n",
    "    # pos_samples = sum(d.y.sum().item() for d in train_data)\n",
    "    # neg_samples = sum((~d.y.bool()).sum().item() for d in train_data)\n",
    "    # pos_weight = torch.tensor(neg_samples / max(1, pos_samples))\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    # Use Adam with weight decay and a lower learning rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_f1 = 0\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            \n",
    "            # Flatten predictions and targets\n",
    "            pred = out.squeeze()\n",
    "            target = batch.y\n",
    "            \n",
    "            loss = criterion(pred, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent explosion\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                out = model(batch)\n",
    "                pred = out.squeeze()\n",
    "                target = batch.y\n",
    "                \n",
    "                loss = criterion(pred, target)\n",
    "                val_loss += loss.item() * batch.num_graphs\n",
    "                \n",
    "                # Use a MUCH LOWER threshold (0.1 or 0.2) to detect isolations\n",
    "                # This counteracts the model's tendency to predict all negatives\n",
    "                binary_pred = (torch.sigmoid(pred) > 0.2).float()\n",
    "                \n",
    "                all_preds.append(binary_pred)\n",
    "                all_targets.append(target)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        # Concatenate predictions and targets\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_targets = torch.cat(all_targets, dim=0)\n",
    "        \n",
    "        # Calculate metrics - but handle the case of no positive predictions\n",
    "        tp = ((all_preds == 1) & (all_targets == 1)).sum().item()\n",
    "        fp = ((all_preds == 1) & (all_targets == 0)).sum().item()\n",
    "        fn = ((all_preds == 0) & (all_targets == 1)).sum().item()\n",
    "        tn = ((all_preds == 0) & (all_targets == 0)).sum().item()\n",
    "        \n",
    "        precision = tp / max(tp + fp, 1)\n",
    "        recall = tp / max(tp + fn, 1)\n",
    "        f1 = 2 * precision * recall / max(precision + recall, 1e-8)\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        \n",
    "        # Update learning rate based on validation loss\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Early stopping based on F1 score\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_isolation_model.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best F1: {best_f1:.4f}\")\n",
    "                model.load_state_dict(torch.load(\"best_isolation_model.pt\"))\n",
    "                break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_example(G, node_list, node_to_idx, failure_type, elements_to_fail):\n",
    "    \"\"\"\n",
    "    Create a PyG Data object for testing the model with specific failures.\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        node_list: List of all nodes in the graph\n",
    "        node_to_idx: Mapping from node names to indices\n",
    "        failure_type: 'node' or 'edge'\n",
    "        elements_to_fail: List of nodes or edges to fail\n",
    "        \n",
    "    Returns:\n",
    "        PyG Data object ready for model inference\n",
    "    \"\"\"\n",
    "    # Create node features - must match the exact features used in training\n",
    "    node_features = []\n",
    "    for node in node_list:\n",
    "        features = []\n",
    "        \n",
    "        # Basic features\n",
    "        pr = G.nodes[node].get('physicalringname', 'unknown')\n",
    "        pr_hash = int(hashlib.md5(pr.encode()).hexdigest(), 16) % 10\n",
    "        features.append(pr_hash/10.0)\n",
    "        \n",
    "        lr = G.nodes[node].get('lrname', 'unknown')\n",
    "        lr_hash = int(hashlib.md5(lr.encode()).hexdigest(), 16) % 10\n",
    "        features.append(lr_hash/10.0)\n",
    "        \n",
    "        is_block = 1.0 if \"BLOCK\" in node else 0.0\n",
    "        features.append(is_block)\n",
    "        \n",
    "        # Add the same advanced features used in your training dataset\n",
    "        if is_block:\n",
    "            # Add placeholder values for block nodes\n",
    "            features.extend([0.0] * 8)  # Must match the number used in training\n",
    "        else:\n",
    "            # Calculate same advanced features\n",
    "            degree = G.degree(node) / max(1, max(dict(G.degree()).values()))\n",
    "            features.append(degree)\n",
    "            \n",
    "            clustering = nx.clustering(G, node)\n",
    "            features.append(clustering)\n",
    "            \n",
    "            # Precompute these if needed for efficiency\n",
    "            articulation_points = set(nx.articulation_points(G))\n",
    "            bridges = list(nx.bridges(G))\n",
    "            \n",
    "            is_articulation = 1.0 if node in articulation_points else 0.0\n",
    "            features.append(is_articulation)\n",
    "            \n",
    "            bridges_connected = min(1.0, sum(1 for u, v in bridges if node in (u, v)) / 5.0)\n",
    "            features.append(bridges_connected)\n",
    "            \n",
    "            # Add the rest of your advanced features here\n",
    "            # For example:\n",
    "            block = G.nodes[node].get('block_name', None)\n",
    "            if block:\n",
    "                try:\n",
    "                    path_length = nx.shortest_path_length(G, node, block) / 10.0\n",
    "                    disjoint_paths = len(list(nx.edge_disjoint_paths(G, node, block))) / 5.0\n",
    "                    features.append(path_length)\n",
    "                    features.append(disjoint_paths)\n",
    "                except:\n",
    "                    features.append(1.0)  # Max path length\n",
    "                    features.append(0.0)  # No paths\n",
    "            else:\n",
    "                features.append(1.0)\n",
    "                features.append(0.0)\n",
    "                \n",
    "            # Isolation risk features\n",
    "            single_path_vulnerability = 1.0 if disjoint_paths <= 1 else 0.0\n",
    "            features.append(single_path_vulnerability)\n",
    "            \n",
    "            # Add other isolation risk features\n",
    "            neighbors = list(G.neighbors(node))\n",
    "            connects_to_articulation = 1.0 if any(n in articulation_points for n in neighbors) else 0.0\n",
    "            features.append(connects_to_articulation)\n",
    "            \n",
    "            isolation_risk = max(single_path_vulnerability, is_articulation, bridges_connected)\n",
    "            features.append(isolation_risk)\n",
    "            \n",
    "        node_features.append(features)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Create edge index\n",
    "    edge_list = []\n",
    "    for u, v in G.edges():\n",
    "        u_idx, v_idx = node_to_idx[u], node_to_idx[v]\n",
    "        edge_list.append([u_idx, v_idx])\n",
    "        edge_list.append([v_idx, u_idx])  # Add both directions\n",
    "    \n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Create edge features if used in your model\n",
    "    edge_attr = None\n",
    "    if hasattr(G.edges[list(G.edges())[0]], 'features'):\n",
    "        edge_attr = []\n",
    "        for i, (u, v) in enumerate(edge_list):\n",
    "            u_name, v_name = node_list[u], node_list[v]\n",
    "            if G.has_edge(u_name, v_name):\n",
    "                edge_attr.append(G.edges[u_name, v_name].get('features', [0.0]))\n",
    "            else:\n",
    "                edge_attr.append([0.0])\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    \n",
    "    # Create edge mask for failure simulation\n",
    "    valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "    \n",
    "    if failure_type == 'node':\n",
    "        for node in elements_to_fail:\n",
    "            node_idx = node_to_idx[node]\n",
    "            invalid_edges = (edge_index[0] == node_idx) | (edge_index[1] == node_idx)\n",
    "            valid_edge_mask[invalid_edges] = False\n",
    "    else:  # edge failure\n",
    "        for u, v in elements_to_fail:\n",
    "            u_idx, v_idx = node_to_idx[u], node_to_idx[v]\n",
    "            for i in range(edge_index.size(1)):\n",
    "                e_u, e_v = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "                if (e_u == u_idx and e_v == v_idx) or (e_u == v_idx and e_v == u_idx):\n",
    "                    valid_edge_mask[i] = False\n",
    "    \n",
    "    # Create Data object\n",
    "    data = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        valid_edge_mask=valid_edge_mask,\n",
    "        failure_type=torch.tensor([0 if failure_type == 'node' else 1]),\n",
    "        failed_elements=[node_to_idx[n] if failure_type == 'node' \n",
    "                        else [node_to_idx[u], node_to_idx[v]] for n in elements_to_fail]\n",
    "    )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network graph...\n",
      "Computing network vulnerability metrics...\n",
      "Calculating node vulnerability scores...\n",
      "Creating node features...\n",
      "Generating dataset examples...\n",
      "Generated 100 examples, 41 with isolations (0.41)\n",
      "Generated 200 examples, 67 with isolations (0.34)\n",
      "Generated 300 examples, 104 with isolations (0.35)\n",
      "Generated 400 examples, 142 with isolations (0.35)\n",
      "Generated 500 examples, 192 with isolations (0.38)\n",
      "Generated 600 examples, 230 with isolations (0.38)\n",
      "Generated 700 examples, 267 with isolations (0.38)\n",
      "Generated 800 examples, 294 with isolations (0.37)\n",
      "Generated 900 examples, 329 with isolations (0.37)\n",
      "Generated 1000 examples, 366 with isolations (0.37)\n",
      "Generated 1100 examples, 408 with isolations (0.37)\n",
      "Generated 1200 examples, 443 with isolations (0.37)\n",
      "Generated 1300 examples, 483 with isolations (0.37)\n",
      "Generated 1400 examples, 522 with isolations (0.37)\n",
      "Generated 1500 examples, 563 with isolations (0.38)\n",
      "Generated 1600 examples, 605 with isolations (0.38)\n",
      "Generated 1700 examples, 645 with isolations (0.38)\n",
      "Generated 1800 examples, 676 with isolations (0.38)\n",
      "Generated 1900 examples, 722 with isolations (0.38)\n",
      "Generated 2000 examples, 757 with isolations (0.38)\n",
      "Generated 2100 examples, 807 with isolations (0.38)\n",
      "Generated 2200 examples, 839 with isolations (0.38)\n",
      "Generated 2300 examples, 881 with isolations (0.38)\n",
      "Generated 2400 examples, 921 with isolations (0.38)\n",
      "Generated 2500 examples, 958 with isolations (0.38)\n",
      "Generated 2600 examples, 993 with isolations (0.38)\n",
      "Generated 2700 examples, 1021 with isolations (0.38)\n",
      "Generated 2800 examples, 1054 with isolations (0.38)\n",
      "Generated 2900 examples, 1085 with isolations (0.37)\n",
      "Generated 3000 examples, 1119 with isolations (0.37)\n",
      "Generated 3100 examples, 1152 with isolations (0.37)\n",
      "Generated 3200 examples, 1186 with isolations (0.37)\n",
      "Generated 3300 examples, 1230 with isolations (0.37)\n",
      "Generated 3400 examples, 1263 with isolations (0.37)\n",
      "Generated 3500 examples, 1304 with isolations (0.37)\n",
      "Generating 1500 additional examples for Phase 2...\n",
      "Current isolation ratio: 0.36, target: 0.40\n",
      "Downsampling non-isolation examples from 3212 to 2681\n",
      "\n",
      "=== Dataset Statistics ===\n",
      "Total examples: 4469\n",
      "Examples with isolations: 1788 (40.0%)\n",
      "Examples without isolations: 2681 (60.0%)\n",
      "Average isolated nodes per positive example: 5.08\n",
      "\n",
      "Examples by failure type:\n",
      "  Node failures: 2874 (64.3%)\n",
      "  Edge failures: 1595 (35.7%)\n",
      "\n",
      "Isolation examples by failure type:\n",
      "  Node failures with isolations: 1116 (62.4%)\n",
      "  Edge failures with isolations: 672 (37.6%)\n",
      "\n",
      "Examples by number of failures:\n",
      "  10 failures with isolations: 298\n",
      "  10 failures without isolations: 146\n",
      "  1 failures without isolations: 415\n",
      "  2 failures with isolations: 72\n",
      "  2 failures without isolations: 414\n",
      "  3 failures with isolations: 107\n",
      "  3 failures without isolations: 339\n",
      "  4 failures with isolations: 128\n",
      "  4 failures without isolations: 322\n",
      "  5 failures with isolations: 173\n",
      "  5 failures without isolations: 286\n",
      "  6 failures with isolations: 193\n",
      "  6 failures without isolations: 233\n",
      "  7 failures with isolations: 253\n",
      "  7 failures without isolations: 212\n",
      "  8 failures with isolations: 268\n",
      "  8 failures without isolations: 171\n",
      "  9 failures with isolations: 296\n",
      "  9 failures without isolations: 143\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import copy\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "\n",
    "# 1. Create better dataset with proper balance\n",
    "data_list, node_list, node_to_idx = create_isolation_prediction_dataset(\n",
    "    topo_data,\n",
    "    num_simulations=5000,\n",
    "    max_failures=10,\n",
    "    min_isolation_ratio=0.4 # Force 50% of examples to have isolations\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 3128 examples, 1284 with isolations (41.05%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nms/.local/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.8430, Val Loss: 0.7388, Precision: 0.0059, Recall: 1.0000, F1: 0.0118, Accuracy: 0.0059\n",
      "Epoch 1: Train Loss: 0.7217, Val Loss: 0.6036, Precision: 0.0059, Recall: 1.0000, F1: 0.0118, Accuracy: 0.0091\n",
      "Epoch 2: Train Loss: 0.5831, Val Loss: 0.4293, Precision: 0.0113, Recall: 1.0000, F1: 0.0224, Accuracy: 0.4846\n",
      "Epoch 3: Train Loss: 0.4686, Val Loss: 0.3550, Precision: 0.0148, Recall: 0.9829, F1: 0.0291, Accuracy: 0.6123\n",
      "Epoch 4: Train Loss: 0.4037, Val Loss: 0.2935, Precision: 0.1104, Recall: 0.6979, F1: 0.1907, Accuracy: 0.9650\n",
      "Epoch 5: Train Loss: 0.3554, Val Loss: 0.2766, Precision: 0.0821, Recall: 0.6783, F1: 0.1464, Accuracy: 0.9532\n",
      "Epoch 6: Train Loss: 0.3345, Val Loss: 0.2672, Precision: 0.1303, Recall: 0.6498, F1: 0.2171, Accuracy: 0.9723\n",
      "Epoch 7: Train Loss: 0.3201, Val Loss: 0.2712, Precision: 0.1302, Recall: 0.6466, F1: 0.2168, Accuracy: 0.9724\n",
      "Epoch 8: Train Loss: 0.3136, Val Loss: 0.2677, Precision: 0.1302, Recall: 0.6466, F1: 0.2168, Accuracy: 0.9724\n",
      "Epoch 9: Train Loss: 0.3059, Val Loss: 0.2735, Precision: 0.1302, Recall: 0.6466, F1: 0.2168, Accuracy: 0.9724\n",
      "Epoch 10: Train Loss: 0.3036, Val Loss: 0.2707, Precision: 0.1305, Recall: 0.6946, F1: 0.2197, Accuracy: 0.9708\n",
      "Epoch 11: Train Loss: 0.2997, Val Loss: 0.2680, Precision: 0.1302, Recall: 0.6466, F1: 0.2168, Accuracy: 0.9724\n",
      "Epoch 12: Train Loss: 0.2944, Val Loss: 0.2676, Precision: 0.1302, Recall: 0.6466, F1: 0.2168, Accuracy: 0.9724\n",
      "Epoch 13: Train Loss: 0.2933, Val Loss: 0.2659, Precision: 0.1295, Recall: 0.6564, F1: 0.2163, Accuracy: 0.9719\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Train with the improved loss function\n",
    "model = train_isolation_model(data_list, epochs=100)\n",
    "\n",
    "# 3. Test with a known isolation case (lower threshold!)\n",
    "def test_model_with_known_case():\n",
    "    # Find a node that causes isolations when it fails\n",
    "    for node in G.nodes():\n",
    "        if \"BLOCK\" not in node:\n",
    "            isolated = simulate_multiple_failures(G, [node], 'node')\n",
    "            if len(isolated) > 0:\n",
    "                print(f\"Testing with node {node} which should cause {len(isolated)} isolations\")\n",
    "                \n",
    "                # Create test data\n",
    "                test_data = create_test_example(G, node_list, node_to_idx, 'node', [node])\n",
    "                \n",
    "                # Predict\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    pred = model(test_data)\n",
    "                    # Use lower threshold here!\n",
    "                    binary_pred = (torch.sigmoid(pred.squeeze()) > 0.2).float()\n",
    "                \n",
    "                # Check results\n",
    "                predicted_nodes = [node_list[i] for i in range(len(node_list)) if binary_pred[i] > 0.5]\n",
    "                print(f\"Actual isolated: {isolated}\")\n",
    "                print(f\"Predicted isolated: {predicted_nodes}\")\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_isolated_nodes(model, G, node_list, node_to_idx, failure_type, elements_to_fail, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Predict which nodes will become isolated after failures using a trained GNN model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained GNN model\n",
    "        G: NetworkX graph\n",
    "        node_list: List of all nodes in the graph\n",
    "        node_to_idx: Mapping from node names to indices\n",
    "        failure_type: 'node' or 'edge'\n",
    "        elements_to_fail: Node(s) or edge(s) to fail\n",
    "        threshold: Probability threshold for considering a node isolated (lower is more sensitive)\n",
    "    \n",
    "    Returns:\n",
    "        List of node names predicted to be isolated\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Pre-compute network properties for better feature engineering\n",
    "    articulation_points = set(nx.articulation_points(G))\n",
    "    bridges = list(nx.bridges(G))\n",
    "    \n",
    "    # Create node features\n",
    "    node_features = []\n",
    "    for node in node_list:\n",
    "        # Feature 1-2: Physical and logical ring\n",
    "        pr = G.nodes[node].get('physicalringname', 'unknown')\n",
    "        pr_hash = hash(pr) % 10\n",
    "        \n",
    "        lr = G.nodes[node].get('lrname', 'unknown')\n",
    "        lr_hash = hash(lr) % 10\n",
    "        \n",
    "        # Feature 3: Is it a block node?\n",
    "        is_block = 1.0 if \"BLOCK\" in node else 0.0\n",
    "        \n",
    "        # Feature 4-5: Network properties\n",
    "        degree = G.degree(node) / max(1, len(G))\n",
    "        clustering = nx.clustering(G, node)\n",
    "        \n",
    "        # Feature 6: Is articulation point?\n",
    "        is_articulation = 1.0 if node in articulation_points else 0.0\n",
    "        \n",
    "        # Feature 7: Connected to bridges\n",
    "        bridges_connected = min(1.0, sum(1 for u, v in bridges if node in (u, v)) / 5.0)\n",
    "        \n",
    "        # Features 8-9: Path to block\n",
    "        block = G.nodes[node].get('block_name', None)\n",
    "        if block and not is_block:\n",
    "            try:\n",
    "                path_length = nx.shortest_path_length(G, node, block) / 10.0\n",
    "                disjoint_paths = len(list(nx.edge_disjoint_paths(G, node, block))) / 5.0\n",
    "            except:\n",
    "                path_length = 1.0\n",
    "                disjoint_paths = 0.0\n",
    "        else:\n",
    "            path_length = 0.0 if is_block else 1.0\n",
    "            disjoint_paths = 1.0 if is_block else 0.0\n",
    "        \n",
    "        # Feature 10: Single path vulnerability\n",
    "        single_path_vulnerability = 1.0 if not is_block and disjoint_paths <= 0.2 else 0.0\n",
    "        \n",
    "        # Feature 11: Isolation risk\n",
    "        isolation_risk = max(single_path_vulnerability, is_articulation, bridges_connected) if not is_block else 0.0\n",
    "        \n",
    "        # Combine all features\n",
    "        node_features.append([\n",
    "            pr_hash/10.0, \n",
    "            lr_hash/10.0, \n",
    "            is_block, \n",
    "            degree, \n",
    "            clustering, \n",
    "            is_articulation, \n",
    "            bridges_connected, \n",
    "            path_length, \n",
    "            disjoint_paths, \n",
    "            single_path_vulnerability, \n",
    "            isolation_risk\n",
    "        ])\n",
    "    \n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Create edge index\n",
    "    edges = []\n",
    "    for u, v in G.edges():\n",
    "        edges.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edges.append([node_to_idx[v], node_to_idx[u]])  # Add reverse edge for undirected graph\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Create edge attributes - FIXED to have exactly 4 features per edge\n",
    "    edge_attr = []\n",
    "    for i in range(edge_index.size(1)):\n",
    "        u_idx, v_idx = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "        u_node, v_node = node_list[u_idx], node_list[v_idx]\n",
    "        \n",
    "        # Edge features\n",
    "        # 1. Same physical ring\n",
    "        same_pr = 1.0 if G.nodes[u_node].get('physicalringname') == G.nodes[v_node].get('physicalringname') else 0.0\n",
    "        \n",
    "        # 2. Same logical ring\n",
    "        same_lr = 1.0 if G.nodes[u_node].get('lrname') == G.nodes[v_node].get('lrname') else 0.0\n",
    "        \n",
    "        # 3. Is this a bridge?\n",
    "        is_bridge = 1.0 if (u_node, v_node) in bridges or (v_node, u_node) in bridges else 0.0\n",
    "        \n",
    "        # 4. Block connection\n",
    "        block_connection = 1.0 if (\"BLOCK\" in u_node) != (\"BLOCK\" in v_node) else 0.0\n",
    "        \n",
    "        # IMPORTANT: Use exactly 4 features for edge_attr\n",
    "        edge_attr.append([same_pr, same_lr, is_bridge, block_connection])\n",
    "    \n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    \n",
    "    # Convert elements_to_fail to list if it's a single element\n",
    "    if not isinstance(elements_to_fail, list):\n",
    "        elements_to_fail = [elements_to_fail]\n",
    "    \n",
    "    # Create edge mask for failure simulation\n",
    "    valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "    \n",
    "    if failure_type == 'node':\n",
    "        # Mark all edges connected to failed nodes as invalid\n",
    "        for node in elements_to_fail:\n",
    "            node_idx = node_to_idx[node]\n",
    "            invalid_edges = (edge_index[0] == node_idx) | (edge_index[1] == node_idx)\n",
    "            valid_edge_mask[invalid_edges] = False\n",
    "    else:  # Edge failure\n",
    "        for failed_edge in elements_to_fail:\n",
    "            u_idx, v_idx = node_to_idx[failed_edge[0]], node_to_idx[failed_edge[1]]\n",
    "            \n",
    "            for i in range(edge_index.size(1)):\n",
    "                e_u, e_v = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "                if (e_u == u_idx and e_v == v_idx) or (e_u == v_idx and e_v == u_idx):\n",
    "                    valid_edge_mask[i] = False\n",
    "    \n",
    "    # Create PyG Data object for the model\n",
    "    data = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        valid_edge_mask=valid_edge_mask,\n",
    "        failure_type=torch.tensor([0 if failure_type == 'node' else 1]),\n",
    "        num_failures=torch.tensor([len(elements_to_fail)], dtype=torch.long)\n",
    "    )\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred_probs = torch.sigmoid(out.squeeze())\n",
    "        \n",
    "        # Handle different output shapes (batch vs single)\n",
    "        if len(pred_probs.shape) > 0:\n",
    "            pred_probs = pred_probs.numpy()\n",
    "        else:\n",
    "            pred_probs = pred_probs.item()\n",
    "    \n",
    "    # Find nodes predicted to be isolated\n",
    "    threshold = 0.7\n",
    "    predicted_isolated_indices = np.where(pred_probs > threshold)[0]\n",
    "    predicted_isolated_nodes = [node_list[idx] for idx in predicted_isolated_indices]\n",
    "    \n",
    "    return predicted_isolated_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NODE_2_2_1', 'NODE_2_2_2', 'NODE_2_2_3']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements_to_fail = ['NODE_2_2_0','NODE_2_2_4']\n",
    "actual_isolated_nodes = simulate_multiple_failures(G, elements_to_fail, failure_type='node')\n",
    "actual_isolated_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_isolated_nodes = predict_isolated_nodes(model, G, node_list, node_to_idx, 'node', elements_to_fail)\n",
    "predicted_isolated_nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(G, model, node_list, node_to_idx, failure_type, elements_to_fail):\n",
    "    \"\"\"\n",
    "    Evaluate how well the model predicts isolated nodes compared to actual simulation\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        model: Trained GNN model\n",
    "        node_list: List of all nodes\n",
    "        node_to_idx: Mapping from node names to indices\n",
    "        failure_type: 'node' or 'edge'\n",
    "        elements_to_fail: List of nodes or edges to fail\n",
    "        \n",
    "    Returns:\n",
    "        Dict with evaluation metrics\n",
    "    \"\"\"\n",
    "    # Get actual isolated nodes via simulation\n",
    "    if failure_type == 'node':\n",
    "        actual_isolated = simulate_multiple_failures(G, elements_to_fail, 'node')\n",
    "    else:\n",
    "        actual_isolated = simulate_multiple_failures(G, elements_to_fail, 'edge')\n",
    "    \n",
    "    # Get predicted isolated nodes from model\n",
    "    predicted_isolated = predict_isolated_nodes(model, G, node_list, node_to_idx, \n",
    "                                               failure_type, elements_to_fail)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    true_positives = len(set(actual_isolated) & set(predicted_isolated))\n",
    "    false_positives = len(set(predicted_isolated) - set(actual_isolated))\n",
    "    false_negatives = len(set(actual_isolated) - set(predicted_isolated))\n",
    "    \n",
    "    # Handle division by zero\n",
    "    if true_positives + false_positives > 0:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "    else:\n",
    "        precision = 1.0 if len(actual_isolated) == 0 else 0.0\n",
    "        \n",
    "    if true_positives + false_negatives > 0:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "    else:\n",
    "        recall = 1.0 if len(predicted_isolated) == 0 else 0.0\n",
    "    \n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    \n",
    "    accuracy = len(set(actual_isolated) & set(predicted_isolated)) / max(1, len(set(actual_isolated) | set(predicted_isolated)))\n",
    "    \n",
    "    # Print results\n",
    "    if failure_type == 'node':\n",
    "        print(f\"Node failure: {', '.join(elements_to_fail)}\")\n",
    "    else:\n",
    "        print(f\"Edge failure: {', '.join([f'({e[0]}-{e[1]})' for e in elements_to_fail])}\")\n",
    "    \n",
    "    print(f\"Actual isolated nodes: {', '.join(actual_isolated) if actual_isolated else 'None'}\")\n",
    "    print(f\"Predicted isolated nodes: {', '.join(predicted_isolated) if predicted_isolated else 'None'}\")\n",
    "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}, Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'actual_isolated': actual_isolated,\n",
    "        'predicted_isolated': predicted_isolated,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TESTING SINGLE NODE FAILURES ===\n",
      "Node failure: NODE_0_0_4\n",
      "Actual isolated nodes: None\n",
      "Predicted isolated nodes: None\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00, Accuracy: 0.00\n",
      "Node failure: NODE_2_2_3\n",
      "Actual isolated nodes: None\n",
      "Predicted isolated nodes: None\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00, Accuracy: 0.00\n",
      "Node failure: NODE_1_2_1\n",
      "Actual isolated nodes: None\n",
      "Predicted isolated nodes: None\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00, Accuracy: 0.00\n",
      "\n",
      "=== TESTING DOUBLE NODE FAILURES ===\n",
      "Node failure: NODE_0_0_1, NODE_0_0_2\n",
      "Actual isolated nodes: None\n",
      "Predicted isolated nodes: None\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00, Accuracy: 0.00\n",
      "Node failure: NODE_1_0_1, NODE_1_0_2\n",
      "Actual isolated nodes: None\n",
      "Predicted isolated nodes: None\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00, Accuracy: 0.00\n",
      "\n",
      "=== TESTING SINGLE EDGE FAILURES ===\n",
      "Edge failure: (NODE_0_1_3-NODE_0_1_4)\n",
      "Actual isolated nodes: None\n",
      "Predicted isolated nodes: None\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00, Accuracy: 0.00\n",
      "Edge failure: (NODE_1_0_0-BLOCK_1)\n",
      "Actual isolated nodes: None\n",
      "Predicted isolated nodes: None\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00, Accuracy: 0.00\n",
      "Edge failure: (NODE_1_0_1-NODE_1_0_2)\n",
      "Actual isolated nodes: None\n",
      "Predicted isolated nodes: None\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00, Accuracy: 0.00\n",
      "\n",
      "=== TESTING DOUBLE EDGE FAILURES ===\n",
      "Edge failure: (NODE_0_0_0-NODE_0_0_1), (NODE_0_0_0-BLOCK_0)\n",
      "Actual isolated nodes: NODE_0_0_0\n",
      "Predicted isolated nodes: None\n",
      "Precision: 0.00, Recall: 0.00, F1: 0.00, Accuracy: 0.00\n",
      "Edge failure: (NODE_1_0_0-NODE_1_0_1), (NODE_1_0_0-BLOCK_1)\n",
      "Actual isolated nodes: NODE_1_0_0\n",
      "Predicted isolated nodes: None\n",
      "Precision: 0.00, Recall: 0.00, F1: 0.00, Accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Test single node failures\n",
    "print(\"\\n=== TESTING SINGLE NODE FAILURES ===\")\n",
    "for node in random.sample(node_list, 3):  # Test 3 random nodes\n",
    "    if \"BLOCK\" not in node:  # Skip block nodes\n",
    "        evaluate_prediction(G, model, node_list, node_to_idx, 'node', [node])\n",
    "\n",
    "# Test double node failures (more likely to cause isolations)\n",
    "print(\"\\n=== TESTING DOUBLE NODE FAILURES ===\")\n",
    "# Find nodes in the same logical ring\n",
    "for pr_idx in range(2):\n",
    "    for lr_idx in range(2):\n",
    "        ring_nodes = [n for n in G.nodes() \n",
    "                     if 'physicalringname' in G.nodes[n] and \n",
    "                        G.nodes[n]['physicalringname'] == f\"RING_{pr_idx}\" and\n",
    "                        'lrname' in G.nodes[n] and\n",
    "                        G.nodes[n]['lrname'] == f\"LR_{pr_idx}_{lr_idx}\" and\n",
    "                        \"BLOCK\" not in n]\n",
    "        \n",
    "        if len(ring_nodes) >= 3:\n",
    "            # Pick two adjacent nodes in the ring\n",
    "            nodes_to_fail = [ring_nodes[1], ring_nodes[2]]\n",
    "            evaluate_prediction(G, model, node_list, node_to_idx, 'node', nodes_to_fail)\n",
    "            break\n",
    "\n",
    "# Test single edge failures\n",
    "print(\"\\n=== TESTING SINGLE EDGE FAILURES ===\")\n",
    "for edge in random.sample(list(G.edges()), 3):\n",
    "    evaluate_prediction(G, model, node_list, node_to_idx, 'edge', [edge])\n",
    "\n",
    "# Test double edge failures\n",
    "print(\"\\n=== TESTING DOUBLE EDGE FAILURES ===\")\n",
    "# Find edges in the same logical ring\n",
    "edge_pairs = []\n",
    "for pr_idx in range(2):\n",
    "    for lr_idx in range(2):\n",
    "        ring_edges = [(u, v) for u, v in G.edges() \n",
    "                      if 'physicalringname' in G.nodes[u] and \n",
    "                         G.nodes[u]['physicalringname'] == f\"RING_{pr_idx}\" and\n",
    "                         'lrname' in G.nodes[u] and \n",
    "                         G.nodes[u]['lrname'] == f\"LR_{pr_idx}_{lr_idx}\"]\n",
    "        \n",
    "        if len(ring_edges) >= 2:\n",
    "            edges_to_fail = [ring_edges[0], ring_edges[1]]\n",
    "            evaluate_prediction(G, model, node_list, node_to_idx, 'edge', edges_to_fail)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'failed_edge_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m      3\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Users\\My PC\\Documents\\CDOT-NMS\\RCA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32me:\\Users\\My PC\\Documents\\CDOT-NMS\\RCA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32me:\\Users\\My PC\\Documents\\CDOT-NMS\\RCA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Users\\My PC\\Documents\\CDOT-NMS\\RCA\\.venv\\Lib\\site-packages\\torch_geometric\\loader\\dataloader.py:27\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     25\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[1;32me:\\Users\\My PC\\Documents\\CDOT-NMS\\RCA\\.venv\\Lib\\site-packages\\torch_geometric\\data\\batch.py:97\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[1;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_data_list\u001b[39m(\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m     exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32me:\\Users\\My PC\\Documents\\CDOT-NMS\\RCA\\.venv\\Lib\\site-packages\\torch_geometric\\data\\collate.py:95\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m exclude_keys:  \u001b[38;5;66;03m# Do not include top-level attribute.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstores\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# The `num_nodes` attribute needs special treatment, as we need to\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# sum their values up instead of merging them to a list:\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32me:\\Users\\My PC\\Documents\\CDOT-NMS\\RCA\\.venv\\Lib\\site-packages\\torch_geometric\\data\\collate.py:95\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m exclude_keys:  \u001b[38;5;66;03m# Do not include top-level attribute.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m values \u001b[38;5;241m=\u001b[39m [\u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m stores]\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# The `num_nodes` attribute needs special treatment, as we need to\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# sum their values up instead of merging them to a list:\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32me:\\Users\\My PC\\Documents\\CDOT-NMS\\RCA\\.venv\\Lib\\site-packages\\torch_geometric\\data\\storage.py:118\u001b[0m, in \u001b[0;36mBaseStorage.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'failed_edge_mask'"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = criterion(out.squeeze(), batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                out = model(batch)\n",
    "                val_loss += criterion(out.squeeze(), batch.y).item()\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gnn_dataset(topology_df, num_simulations=100):\n",
    "    \"\"\"\n",
    "    Create a dataset for training a GNN to predict isolated nodes\n",
    "    \n",
    "    Returns:\n",
    "        - List of PyTorch Geometric Data objects\n",
    "    \"\"\"\n",
    "    # Build the graph\n",
    "    G = build_network_graph(topology_df)\n",
    "    \n",
    "    # Node mapping (for creating numerical indices)\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # Create node features\n",
    "    node_features = []\n",
    "    for node in node_list:\n",
    "        # Feature 1: One-hot encoded physical ring\n",
    "        pr = G.nodes[node]['physicalringname']\n",
    "        pr_hash = hash(pr) % 10  # Simple encoding\n",
    "        \n",
    "        # Feature 2: One-hot encoded logical ring\n",
    "        lr = G.nodes[node]['lrname']\n",
    "        lr_hash = hash(lr) % 10  # Simple encoding\n",
    "        \n",
    "        # Feature 3: Is it a block node?\n",
    "        is_block = 1.0 if node == G.nodes[node]['block_name'] else 0.0\n",
    "        \n",
    "        # Feature 4-5: Degree centrality and clustering coefficient\n",
    "        degree = G.degree(node) / len(G)\n",
    "        clustering = nx.clustering(G, node)\n",
    "        \n",
    "        node_features.append([pr_hash/10.0, lr_hash/10.0, is_block, degree, clustering])\n",
    "    \n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Create edge index for PyG\n",
    "    edges = []\n",
    "    for u, v in G.edges():\n",
    "        edges.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edges.append([node_to_idx[v], node_to_idx[u]])  # Add reverse edge for undirected graph\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Create dataset entries\n",
    "    data_list = []\n",
    "    \n",
    "    # Simulate node failures\n",
    "    for _ in range(num_simulations // 2):  # Half node failures, half edge failures\n",
    "        # Randomly select a node to fail\n",
    "        node_to_fail = np.random.choice(node_list)\n",
    "        \n",
    "        # Find isolated nodes\n",
    "        isolated_nodes = simulate_node_failure(G, node_to_fail)\n",
    "        \n",
    "        # Create target tensor (1 for isolated nodes, 0 for others)\n",
    "        y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "        for node in isolated_nodes:\n",
    "            y[node_to_idx[node]] = 1.0\n",
    "        \n",
    "        # Create node failure mask\n",
    "        node_mask = torch.zeros(len(node_list), dtype=torch.bool)\n",
    "        node_mask[node_to_idx[node_to_fail]] = True\n",
    "        \n",
    "        # Create PyG Data object\n",
    "        data = Data(\n",
    "            x=node_features,\n",
    "            edge_index=edge_index,\n",
    "            y=y,\n",
    "            failed_node_mask=node_mask,\n",
    "            failed_edge_mask=None\n",
    "        )\n",
    "        \n",
    "        data_list.append(data)\n",
    "    \n",
    "    # Simulate edge failures\n",
    "    edge_list = list(G.edges())\n",
    "    for _ in range(num_simulations // 2):\n",
    "        # Randomly select an edge to fail\n",
    "        edge_to_fail = edge_list[np.random.randint(0, len(edge_list))]\n",
    "        \n",
    "        # Find isolated nodes\n",
    "        isolated_nodes = simulate_edge_failure(G, edge_to_fail)\n",
    "        \n",
    "        # Create target tensor\n",
    "        y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "        for node in isolated_nodes:\n",
    "            y[node_to_idx[node]] = 1.0\n",
    "        \n",
    "        # Create edge failure mask\n",
    "        edge_mask = torch.zeros(edge_index.size(1), dtype=torch.bool)\n",
    "        u_idx, v_idx = node_to_idx[edge_to_fail[0]], node_to_idx[edge_to_fail[1]]\n",
    "        \n",
    "        for i in range(edge_index.size(1)):\n",
    "            e_u, e_v = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "            if (e_u == u_idx and e_v == v_idx) or (e_u == v_idx and e_v == u_idx):\n",
    "                edge_mask[i] = True\n",
    "        \n",
    "        # Create PyG Data object\n",
    "        data = Data(\n",
    "            x=node_features,\n",
    "            edge_index=edge_index,\n",
    "            y=y,\n",
    "            failed_node_mask=None,\n",
    "            failed_edge_mask=edge_mask\n",
    "        )\n",
    "        \n",
    "        data_list.append(data)\n",
    "    \n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def load_topology_data(config):\n",
    "    \"\"\"Load topology data from MySQL database\"\"\"\n",
    "    connection = mysql.connector.connect(**config)\n",
    "    cursor = connection.cursor(dictionary=True)\n",
    "    \n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "           aendname, \n",
    "           bendname, \n",
    "           aendip, \n",
    "           bendip, \n",
    "           aendifIndex,\n",
    "           bendifIndex,\n",
    "           block_name, \n",
    "           physicalringname, \n",
    "           lrname \n",
    "        FROM topology_data_logical\n",
    "    \"\"\"\n",
    "    cursor.execute(query)\n",
    "    rows = cursor.fetchall()\n",
    "    connection.close()\n",
    "    \n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Pakhanjur'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m topology_df = load_topology_data(db_config)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Create dataset for GNN training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m data_list = \u001b[43mcreate_gnn_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopology_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m data entries\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mcreate_gnn_dataset\u001b[39m\u001b[34m(topology_df, num_simulations)\u001b[39m\n\u001b[32m     51\u001b[39m node_to_fail = np.random.choice(node_list)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Find isolated nodes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m isolated_nodes = \u001b[43msimulate_node_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_to_fail\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Create target tensor (1 for isolated nodes, 0 for others)\u001b[39;00m\n\u001b[32m     57\u001b[39m y = torch.zeros(\u001b[38;5;28mlen\u001b[39m(node_list), dtype=torch.float)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36msimulate_node_failure\u001b[39m\u001b[34m(G, node_to_fail)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Get all connected nodes in the same ring before failure\u001b[39;00m\n\u001b[32m     17\u001b[39m before_graph = G.copy()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m block_component = \u001b[38;5;28mset\u001b[39m(\u001b[43mnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnode_connected_component\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbefore_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_name\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     19\u001b[39m connected_before = {n \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m block_component \n\u001b[32m     20\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m before_graph.nodes() \u001b[38;5;129;01mand\u001b[39;00m \n\u001b[32m     21\u001b[39m                     before_graph.nodes[n].get(\u001b[33m'\u001b[39m\u001b[33mphysicalringname\u001b[39m\u001b[33m'\u001b[39m) == node_pr \u001b[38;5;129;01mand\u001b[39;00m \n\u001b[32m     22\u001b[39m                     before_graph.nodes[n].get(\u001b[33m'\u001b[39m\u001b[33mlrname\u001b[39m\u001b[33m'\u001b[39m) == node_lr}\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Remove the failed node\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/networkx/utils/decorators.py:788\u001b[39m, in \u001b[36margmap.__call__.<locals>.func\u001b[39m\u001b[34m(_argmap__wrapper, *args, **kwargs)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc\u001b[39m(*args, __wrapper=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43margmap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_lazy_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__wrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<class 'networkx.utils.decorators.argmap'> compilation 13:4\u001b[39m, in \u001b[36margmap_node_connected_component_9\u001b[39m\u001b[34m(G, n, backend, **backend_kwargs)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/networkx/utils/backends.py:967\u001b[39m, in \u001b[36m_dispatchable.__call__\u001b[39m\u001b[34m(self, backend, *args, **kwargs)\u001b[39m\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m backend != \u001b[33m\"\u001b[39m\u001b[33mnetworkx\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    966\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m backend is not installed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`.\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[38;5;66;03m# This is purely for aesthetics and to make it easier to search for this\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;66;03m# variable since \"backend\" is used in many comments and log/error messages.\u001b[39;00m\n\u001b[32m    972\u001b[39m backend_name = backend\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/networkx/algorithms/components/connected.py:198\u001b[39m, in \u001b[36mnode_connected_component\u001b[39m\u001b[34m(G, n)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;129m@not_implemented_for\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdirected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    161\u001b[39m \u001b[38;5;129m@nx\u001b[39m._dispatchable\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnode_connected_component\u001b[39m(G, n):\n\u001b[32m    163\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the set of nodes in the component of graph containing node n.\u001b[39;00m\n\u001b[32m    164\u001b[39m \n\u001b[32m    165\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    196\u001b[39m \n\u001b[32m    197\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_plain_bfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/networkx/algorithms/components/connected.py:210\u001b[39m, in \u001b[36m_plain_bfs\u001b[39m\u001b[34m(G, n, source)\u001b[39m\n\u001b[32m    208\u001b[39m nextlevel = []\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m thislevel:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[43madj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[32m    211\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m seen:\n\u001b[32m    212\u001b[39m             seen.add(w)\n",
      "\u001b[31mKeyError\u001b[39m: 'Pakhanjur'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Database configuration\n",
    "db_config = {\n",
    "    \"host\": \"192.168.30.15\",\n",
    "    \"user\": \"nms\",\n",
    "    \"password\": \"Nms@1234\",\n",
    "    \"database\": \"cnmsip\"\n",
    "}\n",
    "\n",
    "# Load topology data\n",
    "topology_df = load_topology_data(db_config)\n",
    "\n",
    "# Create dataset for GNN training\n",
    "data_list = create_gnn_dataset(topology_df, num_simulations=200)\n",
    "print(f\"Created {len(data_list)} data entries\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
