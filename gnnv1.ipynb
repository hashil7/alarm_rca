{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_topology_data(num_rings=10, nodes_per_ring=10, logical_rings_per_physical=3):\n",
    "    \"\"\"\n",
    "    Create dummy topology data for testing\n",
    "    \n",
    "    Args:\n",
    "        num_rings: Number of physical rings to create\n",
    "        nodes_per_ring: Number of nodes per ring\n",
    "        logical_rings_per_physical: Number of logical rings per physical ring\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with topology data\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for pr_idx in range(num_rings):\n",
    "        physical_ring = f\"RING_{pr_idx}\"\n",
    "        block_name = f\"BLOCK_{pr_idx}\"  # One block per physical ring\n",
    "        \n",
    "        # Each physical ring has multiple logical rings from the same block\n",
    "        for lr_idx in range(logical_rings_per_physical):\n",
    "            logical_ring = f\"LR_{pr_idx}_{lr_idx}\"\n",
    "            \n",
    "            # Create a linear path for each logical ring (not a complete ring)\n",
    "            for i in range(nodes_per_ring - 1):  # Connect nodes in a path, not a ring\n",
    "                node_a = f\"NODE_{pr_idx}_{lr_idx}_{i}\"\n",
    "                node_b = f\"NODE_{pr_idx}_{lr_idx}_{i+1}\"\n",
    "                \n",
    "                data.append({\n",
    "                    'aendname': node_a,\n",
    "                    'bendname': node_b,\n",
    "                    'aendip': f\"10.{pr_idx}.{lr_idx}.{i}\",\n",
    "                    'bendip': f\"10.{pr_idx}.{lr_idx}.{i+1}\",\n",
    "                    'aendifIndex': i,\n",
    "                    'bendifIndex': i+1,\n",
    "                    'block_name': block_name,\n",
    "                    'physicalringname': physical_ring,\n",
    "                    'lrname': logical_ring\n",
    "                })\n",
    "            \n",
    "            # Connect the first and last nodes to the block\n",
    "            # First node connects to block\n",
    "            data.append({\n",
    "                'aendname': f\"NODE_{pr_idx}_{lr_idx}_0\",\n",
    "                'bendname': block_name,\n",
    "                'aendip': f\"10.{pr_idx}.{lr_idx}.0\",\n",
    "                'bendip': f\"10.{pr_idx}.99.99\",  # Special IP for block\n",
    "                'aendifIndex': 100 + lr_idx,\n",
    "                'bendifIndex': 100 + lr_idx,\n",
    "                'block_name': block_name,\n",
    "                'physicalringname': physical_ring,\n",
    "                'lrname': logical_ring\n",
    "            })\n",
    "            \n",
    "            # Last node connects to block\n",
    "            data.append({\n",
    "                'aendname': f\"NODE_{pr_idx}_{lr_idx}_{nodes_per_ring-1}\",\n",
    "                'bendname': block_name,\n",
    "                'aendip': f\"10.{pr_idx}.{lr_idx}.{nodes_per_ring-1}\",\n",
    "                'bendip': f\"10.{pr_idx}.99.99\",  # Special IP for block\n",
    "                'aendifIndex': 200 + lr_idx,\n",
    "                'bendifIndex': 200 + lr_idx,\n",
    "                'block_name': block_name,\n",
    "                'physicalringname': physical_ring,\n",
    "                'lrname': logical_ring\n",
    "            })\n",
    "    \n",
    "    # Add connections between blocks from different physical rings\n",
    "    # for pr_idx in range(num_rings):\n",
    "    #     if pr_idx < num_rings - 1:  # Connect to next physical ring\n",
    "    #         # Connect this block to the next ring's block\n",
    "    #         block_a = f\"BLOCK_{pr_idx}\"\n",
    "    #         block_b = f\"BLOCK_{pr_idx+1}\"\n",
    "            \n",
    "    #         data.append({\n",
    "    #             'aendname': block_a,\n",
    "    #             'bendname': block_b,\n",
    "    #             'aendip': f\"10.{pr_idx}.99.99\",\n",
    "    #             'bendip': f\"10.{pr_idx+1}.99.99\",\n",
    "    #             'aendifIndex': 300 + pr_idx,\n",
    "    #             'bendifIndex': 300 + pr_idx + 1,\n",
    "    #             'block_name': block_a,\n",
    "    #             'physicalringname': f\"RING_{pr_idx}\",\n",
    "    #             'lrname': \"INTER_BLOCK\"  # Inter-block connection\n",
    "    #         })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_data = create_dummy_topology_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_network_graph(topology_df):\n",
    "    \"\"\"Build a graph from topology_data_logical table\"\"\"\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes and edges with attributes\n",
    "    for _, row in topology_df.iterrows():\n",
    "        aend = row['aendname'].upper()\n",
    "        bend = row['bendname'].upper()\n",
    "        \n",
    "        # Add node attributes\n",
    "        G.add_node(aend, ip=row['aendip'], \n",
    "                   physicalringname=row['physicalringname'], \n",
    "                   lrname=row['lrname'],\n",
    "                   block_name=row['block_name'])\n",
    "        \n",
    "        G.add_node(bend, ip=row['bendip'], \n",
    "                   physicalringname=row['physicalringname'], \n",
    "                   lrname=row['lrname'],\n",
    "                   block_name=row['block_name'])\n",
    "        \n",
    "        # Add edge with attributes\n",
    "        G.add_edge(aend, bend, \n",
    "                  physicalringname=row['physicalringname'], \n",
    "                  lrname=row['lrname'],\n",
    "                  aendifIndex=row['aendifIndex'],\n",
    "                  bendifIndex=row['bendifIndex'])\n",
    "    \n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "G =build_network_graph(topo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyvis.network import Network\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def visualize_network(G, filename='network_graph.html'):\n",
    "    \"\"\"\n",
    "    Visualize the network graph using PyVis\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        filename: Output HTML file name\n",
    "    \"\"\"\n",
    "    # Create a PyVis network\n",
    "    net = Network(notebook=True, height=\"750px\", width=\"100%\")\n",
    "    \n",
    "    # Generate a color map for different physical rings and logical rings\n",
    "    physical_rings = set(nx.get_node_attributes(G, 'physicalringname').values())\n",
    "    logical_rings = set(nx.get_node_attributes(G, 'lrname').values())\n",
    "    \n",
    "    # Create color maps\n",
    "    pr_colors = {pr: f\"#{hash(pr) % 0xffffff:06x}\" for pr in physical_rings}\n",
    "    lr_colors = {lr: f\"#{hash(lr) % 0xffffff:06x}\" for lr in logical_rings}\n",
    "    \n",
    "    # Add nodes with attributes\n",
    "    for node in G.nodes():\n",
    "        pr = G.nodes[node]['physicalringname']\n",
    "        lr = G.nodes[node]['lrname']\n",
    "        is_block = node == G.nodes[node]['block_name']\n",
    "        \n",
    "        # Set node properties\n",
    "        node_title = f\"Node: {node}<br>IP: {G.nodes[node]['ip']}<br>PR: {pr}<br>LR: {lr}\"\n",
    "        node_color = pr_colors[pr]  # Color by physical ring\n",
    "        node_shape = 'diamond' if is_block else 'dot'\n",
    "        node_size = 25 if is_block else 15\n",
    "        \n",
    "        net.add_node(node, title=node_title, color=node_color, shape=node_shape, size=node_size, label=node)\n",
    "    \n",
    "    # Add edges with attributes\n",
    "    for u, v in G.edges():\n",
    "        pr = G.edges[u, v]['physicalringname']\n",
    "        lr = G.edges[u, v]['lrname']\n",
    "        \n",
    "        # Set edge properties\n",
    "        edge_title = f\"PR: {pr}<br>LR: {lr}\"\n",
    "        edge_color = lr_colors[lr]  # Color by logical ring\n",
    "        \n",
    "        net.add_edge(u, v, title=edge_title, color=edge_color)\n",
    "    \n",
    "    # Set physics layout\n",
    "    net.barnes_hut(spring_length=200)\n",
    "    \n",
    "    # Save and show the graph\n",
    "    net.show(filename)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "network_topology.html\n"
     ]
    }
   ],
   "source": [
    "net = visualize_network(G, 'network_topology.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_node_failure(G, node_to_fail):\n",
    "    \"\"\"\n",
    "    Simulate a node failure and identify isolated nodes\n",
    "    \n",
    "    Returns:\n",
    "        - isolated_nodes: list of nodes that become isolated\n",
    "    \"\"\"\n",
    "    # Get the node's ring information\n",
    "    if node_to_fail not in G.nodes():\n",
    "        return []\n",
    "    \n",
    "    node_pr = G.nodes[node_to_fail]['physicalringname']\n",
    "    node_lr = G.nodes[node_to_fail]['lrname']\n",
    "    block_name = G.nodes[node_to_fail]['block_name']\n",
    "    \n",
    "    # Get all connected nodes in the same ring before failure\n",
    "    before_graph = G.copy()\n",
    "    block_component = set(nx.node_connected_component(before_graph, block_name))\n",
    "    connected_before = {n for n in block_component \n",
    "                        if n in before_graph.nodes() and \n",
    "                        before_graph.nodes[n].get('physicalringname') == node_pr and \n",
    "                        before_graph.nodes[n].get('lrname') == node_lr}\n",
    "    \n",
    "    # Remove the failed node\n",
    "    after_graph = G.copy()\n",
    "    after_graph.remove_node(node_to_fail)\n",
    "    \n",
    "    # Get connected nodes after failure\n",
    "    if block_name in after_graph.nodes():\n",
    "        block_component_after = set(nx.node_connected_component(after_graph, block_name))\n",
    "        connected_after = {n for n in block_component_after \n",
    "                          if n in after_graph.nodes() and \n",
    "                          after_graph.nodes[n].get('physicalringname') == node_pr and \n",
    "                          after_graph.nodes[n].get('lrname') == node_lr}\n",
    "    else:\n",
    "        connected_after = set()\n",
    "    \n",
    "    # Find isolated nodes (nodes connected before but not after)\n",
    "    isolated_nodes = connected_before - connected_after - {node_to_fail}\n",
    "    \n",
    "    return list(isolated_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_edge_failure(G, edge_to_fail):\n",
    "    \"\"\"\n",
    "    Simulate an edge failure and identify isolated nodes\n",
    "    \n",
    "    Returns:\n",
    "        - isolated_nodes: list of nodes that become isolated\n",
    "    \"\"\"\n",
    "    u, v = edge_to_fail\n",
    "    if not G.has_edge(u, v):\n",
    "        return []\n",
    "    \n",
    "    # Get edge information\n",
    "    edge_pr = G.edges[u, v]['physicalringname']\n",
    "    edge_lr = G.edges[u, v]['lrname']\n",
    "    block_name = G.nodes[u]['block_name']  # Assuming same block for connected nodes\n",
    "    \n",
    "    # Get connected nodes before failure\n",
    "    before_graph = G.copy()\n",
    "    block_component = set(nx.node_connected_component(before_graph, block_name))\n",
    "    connected_before = {n for n in block_component \n",
    "                        if n in before_graph.nodes() and \n",
    "                        before_graph.nodes[n].get('physicalringname') == edge_pr and \n",
    "                        before_graph.nodes[n].get('lrname') == edge_lr}\n",
    "    \n",
    "    # Remove the edge\n",
    "    after_graph = G.copy()\n",
    "    after_graph.remove_edge(u, v)\n",
    "    \n",
    "    # Get connected nodes after failure\n",
    "    block_component_after = set(nx.node_connected_component(after_graph, block_name))\n",
    "    connected_after = {n for n in block_component_after \n",
    "                       if n in after_graph.nodes() and \n",
    "                       after_graph.nodes[n].get('physicalringname') == edge_pr and \n",
    "                       after_graph.nodes[n].get('lrname') == edge_lr}\n",
    "    \n",
    "    # Find isolated nodes\n",
    "    isolated_nodes = connected_before - connected_after\n",
    "    \n",
    "    return list(isolated_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NODE_0_0_0', 'NODE_0_0_1')\n",
      "('NODE_0_0_0', 'BLOCK_0')\n",
      "('NODE_0_0_1', 'NODE_0_0_2')\n",
      "('NODE_0_0_2', 'NODE_0_0_3')\n",
      "('NODE_0_0_3', 'NODE_0_0_4')\n",
      "('NODE_0_0_4', 'NODE_0_0_5')\n",
      "('NODE_0_0_5', 'NODE_0_0_6')\n",
      "('NODE_0_0_6', 'NODE_0_0_7')\n",
      "('NODE_0_0_7', 'NODE_0_0_8')\n",
      "('NODE_0_0_8', 'NODE_0_0_9')\n",
      "('NODE_0_0_9', 'BLOCK_0')\n",
      "('BLOCK_0', 'NODE_0_1_0')\n",
      "('BLOCK_0', 'NODE_0_1_9')\n",
      "('BLOCK_0', 'NODE_0_2_0')\n",
      "('BLOCK_0', 'NODE_0_2_9')\n",
      "('NODE_0_1_0', 'NODE_0_1_1')\n",
      "('NODE_0_1_1', 'NODE_0_1_2')\n",
      "('NODE_0_1_2', 'NODE_0_1_3')\n",
      "('NODE_0_1_3', 'NODE_0_1_4')\n",
      "('NODE_0_1_4', 'NODE_0_1_5')\n",
      "('NODE_0_1_5', 'NODE_0_1_6')\n",
      "('NODE_0_1_6', 'NODE_0_1_7')\n",
      "('NODE_0_1_7', 'NODE_0_1_8')\n",
      "('NODE_0_1_8', 'NODE_0_1_9')\n",
      "('NODE_0_2_0', 'NODE_0_2_1')\n",
      "('NODE_0_2_1', 'NODE_0_2_2')\n",
      "('NODE_0_2_2', 'NODE_0_2_3')\n",
      "('NODE_0_2_3', 'NODE_0_2_4')\n",
      "('NODE_0_2_4', 'NODE_0_2_5')\n",
      "('NODE_0_2_5', 'NODE_0_2_6')\n",
      "('NODE_0_2_6', 'NODE_0_2_7')\n",
      "('NODE_0_2_7', 'NODE_0_2_8')\n",
      "('NODE_0_2_8', 'NODE_0_2_9')\n",
      "('NODE_1_0_0', 'NODE_1_0_1')\n",
      "('NODE_1_0_0', 'BLOCK_1')\n",
      "('NODE_1_0_1', 'NODE_1_0_2')\n",
      "('NODE_1_0_2', 'NODE_1_0_3')\n",
      "('NODE_1_0_3', 'NODE_1_0_4')\n",
      "('NODE_1_0_4', 'NODE_1_0_5')\n",
      "('NODE_1_0_5', 'NODE_1_0_6')\n",
      "('NODE_1_0_6', 'NODE_1_0_7')\n",
      "('NODE_1_0_7', 'NODE_1_0_8')\n",
      "('NODE_1_0_8', 'NODE_1_0_9')\n",
      "('NODE_1_0_9', 'BLOCK_1')\n",
      "('BLOCK_1', 'NODE_1_1_0')\n",
      "('BLOCK_1', 'NODE_1_1_9')\n",
      "('BLOCK_1', 'NODE_1_2_0')\n",
      "('BLOCK_1', 'NODE_1_2_9')\n",
      "('NODE_1_1_0', 'NODE_1_1_1')\n",
      "('NODE_1_1_1', 'NODE_1_1_2')\n",
      "('NODE_1_1_2', 'NODE_1_1_3')\n",
      "('NODE_1_1_3', 'NODE_1_1_4')\n",
      "('NODE_1_1_4', 'NODE_1_1_5')\n",
      "('NODE_1_1_5', 'NODE_1_1_6')\n",
      "('NODE_1_1_6', 'NODE_1_1_7')\n",
      "('NODE_1_1_7', 'NODE_1_1_8')\n",
      "('NODE_1_1_8', 'NODE_1_1_9')\n",
      "('NODE_1_2_0', 'NODE_1_2_1')\n",
      "('NODE_1_2_1', 'NODE_1_2_2')\n",
      "('NODE_1_2_2', 'NODE_1_2_3')\n",
      "('NODE_1_2_3', 'NODE_1_2_4')\n",
      "('NODE_1_2_4', 'NODE_1_2_5')\n",
      "('NODE_1_2_5', 'NODE_1_2_6')\n",
      "('NODE_1_2_6', 'NODE_1_2_7')\n",
      "('NODE_1_2_7', 'NODE_1_2_8')\n",
      "('NODE_1_2_8', 'NODE_1_2_9')\n",
      "('NODE_2_0_0', 'NODE_2_0_1')\n",
      "('NODE_2_0_0', 'BLOCK_2')\n",
      "('NODE_2_0_1', 'NODE_2_0_2')\n",
      "('NODE_2_0_2', 'NODE_2_0_3')\n",
      "('NODE_2_0_3', 'NODE_2_0_4')\n",
      "('NODE_2_0_4', 'NODE_2_0_5')\n",
      "('NODE_2_0_5', 'NODE_2_0_6')\n",
      "('NODE_2_0_6', 'NODE_2_0_7')\n",
      "('NODE_2_0_7', 'NODE_2_0_8')\n",
      "('NODE_2_0_8', 'NODE_2_0_9')\n",
      "('NODE_2_0_9', 'BLOCK_2')\n",
      "('BLOCK_2', 'NODE_2_1_0')\n",
      "('BLOCK_2', 'NODE_2_1_9')\n",
      "('BLOCK_2', 'NODE_2_2_0')\n",
      "('BLOCK_2', 'NODE_2_2_9')\n",
      "('NODE_2_1_0', 'NODE_2_1_1')\n",
      "('NODE_2_1_1', 'NODE_2_1_2')\n",
      "('NODE_2_1_2', 'NODE_2_1_3')\n",
      "('NODE_2_1_3', 'NODE_2_1_4')\n",
      "('NODE_2_1_4', 'NODE_2_1_5')\n",
      "('NODE_2_1_5', 'NODE_2_1_6')\n",
      "('NODE_2_1_6', 'NODE_2_1_7')\n",
      "('NODE_2_1_7', 'NODE_2_1_8')\n",
      "('NODE_2_1_8', 'NODE_2_1_9')\n",
      "('NODE_2_2_0', 'NODE_2_2_1')\n",
      "('NODE_2_2_1', 'NODE_2_2_2')\n",
      "('NODE_2_2_2', 'NODE_2_2_3')\n",
      "('NODE_2_2_3', 'NODE_2_2_4')\n",
      "('NODE_2_2_4', 'NODE_2_2_5')\n",
      "('NODE_2_2_5', 'NODE_2_2_6')\n",
      "('NODE_2_2_6', 'NODE_2_2_7')\n",
      "('NODE_2_2_7', 'NODE_2_2_8')\n",
      "('NODE_2_2_8', 'NODE_2_2_9')\n",
      "('NODE_3_0_0', 'NODE_3_0_1')\n",
      "('NODE_3_0_0', 'BLOCK_3')\n",
      "('NODE_3_0_1', 'NODE_3_0_2')\n",
      "('NODE_3_0_2', 'NODE_3_0_3')\n",
      "('NODE_3_0_3', 'NODE_3_0_4')\n",
      "('NODE_3_0_4', 'NODE_3_0_5')\n",
      "('NODE_3_0_5', 'NODE_3_0_6')\n",
      "('NODE_3_0_6', 'NODE_3_0_7')\n",
      "('NODE_3_0_7', 'NODE_3_0_8')\n",
      "('NODE_3_0_8', 'NODE_3_0_9')\n",
      "('NODE_3_0_9', 'BLOCK_3')\n",
      "('BLOCK_3', 'NODE_3_1_0')\n",
      "('BLOCK_3', 'NODE_3_1_9')\n",
      "('BLOCK_3', 'NODE_3_2_0')\n",
      "('BLOCK_3', 'NODE_3_2_9')\n",
      "('NODE_3_1_0', 'NODE_3_1_1')\n",
      "('NODE_3_1_1', 'NODE_3_1_2')\n",
      "('NODE_3_1_2', 'NODE_3_1_3')\n",
      "('NODE_3_1_3', 'NODE_3_1_4')\n",
      "('NODE_3_1_4', 'NODE_3_1_5')\n",
      "('NODE_3_1_5', 'NODE_3_1_6')\n",
      "('NODE_3_1_6', 'NODE_3_1_7')\n",
      "('NODE_3_1_7', 'NODE_3_1_8')\n",
      "('NODE_3_1_8', 'NODE_3_1_9')\n",
      "('NODE_3_2_0', 'NODE_3_2_1')\n",
      "('NODE_3_2_1', 'NODE_3_2_2')\n",
      "('NODE_3_2_2', 'NODE_3_2_3')\n",
      "('NODE_3_2_3', 'NODE_3_2_4')\n",
      "('NODE_3_2_4', 'NODE_3_2_5')\n",
      "('NODE_3_2_5', 'NODE_3_2_6')\n",
      "('NODE_3_2_6', 'NODE_3_2_7')\n",
      "('NODE_3_2_7', 'NODE_3_2_8')\n",
      "('NODE_3_2_8', 'NODE_3_2_9')\n",
      "('NODE_4_0_0', 'NODE_4_0_1')\n",
      "('NODE_4_0_0', 'BLOCK_4')\n",
      "('NODE_4_0_1', 'NODE_4_0_2')\n",
      "('NODE_4_0_2', 'NODE_4_0_3')\n",
      "('NODE_4_0_3', 'NODE_4_0_4')\n",
      "('NODE_4_0_4', 'NODE_4_0_5')\n",
      "('NODE_4_0_5', 'NODE_4_0_6')\n",
      "('NODE_4_0_6', 'NODE_4_0_7')\n",
      "('NODE_4_0_7', 'NODE_4_0_8')\n",
      "('NODE_4_0_8', 'NODE_4_0_9')\n",
      "('NODE_4_0_9', 'BLOCK_4')\n",
      "('BLOCK_4', 'NODE_4_1_0')\n",
      "('BLOCK_4', 'NODE_4_1_9')\n",
      "('BLOCK_4', 'NODE_4_2_0')\n",
      "('BLOCK_4', 'NODE_4_2_9')\n",
      "('NODE_4_1_0', 'NODE_4_1_1')\n",
      "('NODE_4_1_1', 'NODE_4_1_2')\n",
      "('NODE_4_1_2', 'NODE_4_1_3')\n",
      "('NODE_4_1_3', 'NODE_4_1_4')\n",
      "('NODE_4_1_4', 'NODE_4_1_5')\n",
      "('NODE_4_1_5', 'NODE_4_1_6')\n",
      "('NODE_4_1_6', 'NODE_4_1_7')\n",
      "('NODE_4_1_7', 'NODE_4_1_8')\n",
      "('NODE_4_1_8', 'NODE_4_1_9')\n",
      "('NODE_4_2_0', 'NODE_4_2_1')\n",
      "('NODE_4_2_1', 'NODE_4_2_2')\n",
      "('NODE_4_2_2', 'NODE_4_2_3')\n",
      "('NODE_4_2_3', 'NODE_4_2_4')\n",
      "('NODE_4_2_4', 'NODE_4_2_5')\n",
      "('NODE_4_2_5', 'NODE_4_2_6')\n",
      "('NODE_4_2_6', 'NODE_4_2_7')\n",
      "('NODE_4_2_7', 'NODE_4_2_8')\n",
      "('NODE_4_2_8', 'NODE_4_2_9')\n",
      "('NODE_5_0_0', 'NODE_5_0_1')\n",
      "('NODE_5_0_0', 'BLOCK_5')\n",
      "('NODE_5_0_1', 'NODE_5_0_2')\n",
      "('NODE_5_0_2', 'NODE_5_0_3')\n",
      "('NODE_5_0_3', 'NODE_5_0_4')\n",
      "('NODE_5_0_4', 'NODE_5_0_5')\n",
      "('NODE_5_0_5', 'NODE_5_0_6')\n",
      "('NODE_5_0_6', 'NODE_5_0_7')\n",
      "('NODE_5_0_7', 'NODE_5_0_8')\n",
      "('NODE_5_0_8', 'NODE_5_0_9')\n",
      "('NODE_5_0_9', 'BLOCK_5')\n",
      "('BLOCK_5', 'NODE_5_1_0')\n",
      "('BLOCK_5', 'NODE_5_1_9')\n",
      "('BLOCK_5', 'NODE_5_2_0')\n",
      "('BLOCK_5', 'NODE_5_2_9')\n",
      "('NODE_5_1_0', 'NODE_5_1_1')\n",
      "('NODE_5_1_1', 'NODE_5_1_2')\n",
      "('NODE_5_1_2', 'NODE_5_1_3')\n",
      "('NODE_5_1_3', 'NODE_5_1_4')\n",
      "('NODE_5_1_4', 'NODE_5_1_5')\n",
      "('NODE_5_1_5', 'NODE_5_1_6')\n",
      "('NODE_5_1_6', 'NODE_5_1_7')\n",
      "('NODE_5_1_7', 'NODE_5_1_8')\n",
      "('NODE_5_1_8', 'NODE_5_1_9')\n",
      "('NODE_5_2_0', 'NODE_5_2_1')\n",
      "('NODE_5_2_1', 'NODE_5_2_2')\n",
      "('NODE_5_2_2', 'NODE_5_2_3')\n",
      "('NODE_5_2_3', 'NODE_5_2_4')\n",
      "('NODE_5_2_4', 'NODE_5_2_5')\n",
      "('NODE_5_2_5', 'NODE_5_2_6')\n",
      "('NODE_5_2_6', 'NODE_5_2_7')\n",
      "('NODE_5_2_7', 'NODE_5_2_8')\n",
      "('NODE_5_2_8', 'NODE_5_2_9')\n",
      "('NODE_6_0_0', 'NODE_6_0_1')\n",
      "('NODE_6_0_0', 'BLOCK_6')\n",
      "('NODE_6_0_1', 'NODE_6_0_2')\n",
      "('NODE_6_0_2', 'NODE_6_0_3')\n",
      "('NODE_6_0_3', 'NODE_6_0_4')\n",
      "('NODE_6_0_4', 'NODE_6_0_5')\n",
      "('NODE_6_0_5', 'NODE_6_0_6')\n",
      "('NODE_6_0_6', 'NODE_6_0_7')\n",
      "('NODE_6_0_7', 'NODE_6_0_8')\n",
      "('NODE_6_0_8', 'NODE_6_0_9')\n",
      "('NODE_6_0_9', 'BLOCK_6')\n",
      "('BLOCK_6', 'NODE_6_1_0')\n",
      "('BLOCK_6', 'NODE_6_1_9')\n",
      "('BLOCK_6', 'NODE_6_2_0')\n",
      "('BLOCK_6', 'NODE_6_2_9')\n",
      "('NODE_6_1_0', 'NODE_6_1_1')\n",
      "('NODE_6_1_1', 'NODE_6_1_2')\n",
      "('NODE_6_1_2', 'NODE_6_1_3')\n",
      "('NODE_6_1_3', 'NODE_6_1_4')\n",
      "('NODE_6_1_4', 'NODE_6_1_5')\n",
      "('NODE_6_1_5', 'NODE_6_1_6')\n",
      "('NODE_6_1_6', 'NODE_6_1_7')\n",
      "('NODE_6_1_7', 'NODE_6_1_8')\n",
      "('NODE_6_1_8', 'NODE_6_1_9')\n",
      "('NODE_6_2_0', 'NODE_6_2_1')\n",
      "('NODE_6_2_1', 'NODE_6_2_2')\n",
      "('NODE_6_2_2', 'NODE_6_2_3')\n",
      "('NODE_6_2_3', 'NODE_6_2_4')\n",
      "('NODE_6_2_4', 'NODE_6_2_5')\n",
      "('NODE_6_2_5', 'NODE_6_2_6')\n",
      "('NODE_6_2_6', 'NODE_6_2_7')\n",
      "('NODE_6_2_7', 'NODE_6_2_8')\n",
      "('NODE_6_2_8', 'NODE_6_2_9')\n",
      "('NODE_7_0_0', 'NODE_7_0_1')\n",
      "('NODE_7_0_0', 'BLOCK_7')\n",
      "('NODE_7_0_1', 'NODE_7_0_2')\n",
      "('NODE_7_0_2', 'NODE_7_0_3')\n",
      "('NODE_7_0_3', 'NODE_7_0_4')\n",
      "('NODE_7_0_4', 'NODE_7_0_5')\n",
      "('NODE_7_0_5', 'NODE_7_0_6')\n",
      "('NODE_7_0_6', 'NODE_7_0_7')\n",
      "('NODE_7_0_7', 'NODE_7_0_8')\n",
      "('NODE_7_0_8', 'NODE_7_0_9')\n",
      "('NODE_7_0_9', 'BLOCK_7')\n",
      "('BLOCK_7', 'NODE_7_1_0')\n",
      "('BLOCK_7', 'NODE_7_1_9')\n",
      "('BLOCK_7', 'NODE_7_2_0')\n",
      "('BLOCK_7', 'NODE_7_2_9')\n",
      "('NODE_7_1_0', 'NODE_7_1_1')\n",
      "('NODE_7_1_1', 'NODE_7_1_2')\n",
      "('NODE_7_1_2', 'NODE_7_1_3')\n",
      "('NODE_7_1_3', 'NODE_7_1_4')\n",
      "('NODE_7_1_4', 'NODE_7_1_5')\n",
      "('NODE_7_1_5', 'NODE_7_1_6')\n",
      "('NODE_7_1_6', 'NODE_7_1_7')\n",
      "('NODE_7_1_7', 'NODE_7_1_8')\n",
      "('NODE_7_1_8', 'NODE_7_1_9')\n",
      "('NODE_7_2_0', 'NODE_7_2_1')\n",
      "('NODE_7_2_1', 'NODE_7_2_2')\n",
      "('NODE_7_2_2', 'NODE_7_2_3')\n",
      "('NODE_7_2_3', 'NODE_7_2_4')\n",
      "('NODE_7_2_4', 'NODE_7_2_5')\n",
      "('NODE_7_2_5', 'NODE_7_2_6')\n",
      "('NODE_7_2_6', 'NODE_7_2_7')\n",
      "('NODE_7_2_7', 'NODE_7_2_8')\n",
      "('NODE_7_2_8', 'NODE_7_2_9')\n",
      "('NODE_8_0_0', 'NODE_8_0_1')\n",
      "('NODE_8_0_0', 'BLOCK_8')\n",
      "('NODE_8_0_1', 'NODE_8_0_2')\n",
      "('NODE_8_0_2', 'NODE_8_0_3')\n",
      "('NODE_8_0_3', 'NODE_8_0_4')\n",
      "('NODE_8_0_4', 'NODE_8_0_5')\n",
      "('NODE_8_0_5', 'NODE_8_0_6')\n",
      "('NODE_8_0_6', 'NODE_8_0_7')\n",
      "('NODE_8_0_7', 'NODE_8_0_8')\n",
      "('NODE_8_0_8', 'NODE_8_0_9')\n",
      "('NODE_8_0_9', 'BLOCK_8')\n",
      "('BLOCK_8', 'NODE_8_1_0')\n",
      "('BLOCK_8', 'NODE_8_1_9')\n",
      "('BLOCK_8', 'NODE_8_2_0')\n",
      "('BLOCK_8', 'NODE_8_2_9')\n",
      "('NODE_8_1_0', 'NODE_8_1_1')\n",
      "('NODE_8_1_1', 'NODE_8_1_2')\n",
      "('NODE_8_1_2', 'NODE_8_1_3')\n",
      "('NODE_8_1_3', 'NODE_8_1_4')\n",
      "('NODE_8_1_4', 'NODE_8_1_5')\n",
      "('NODE_8_1_5', 'NODE_8_1_6')\n",
      "('NODE_8_1_6', 'NODE_8_1_7')\n",
      "('NODE_8_1_7', 'NODE_8_1_8')\n",
      "('NODE_8_1_8', 'NODE_8_1_9')\n",
      "('NODE_8_2_0', 'NODE_8_2_1')\n",
      "('NODE_8_2_1', 'NODE_8_2_2')\n",
      "('NODE_8_2_2', 'NODE_8_2_3')\n",
      "('NODE_8_2_3', 'NODE_8_2_4')\n",
      "('NODE_8_2_4', 'NODE_8_2_5')\n",
      "('NODE_8_2_5', 'NODE_8_2_6')\n",
      "('NODE_8_2_6', 'NODE_8_2_7')\n",
      "('NODE_8_2_7', 'NODE_8_2_8')\n",
      "('NODE_8_2_8', 'NODE_8_2_9')\n",
      "('NODE_9_0_0', 'NODE_9_0_1')\n",
      "('NODE_9_0_0', 'BLOCK_9')\n",
      "('NODE_9_0_1', 'NODE_9_0_2')\n",
      "('NODE_9_0_2', 'NODE_9_0_3')\n",
      "('NODE_9_0_3', 'NODE_9_0_4')\n",
      "('NODE_9_0_4', 'NODE_9_0_5')\n",
      "('NODE_9_0_5', 'NODE_9_0_6')\n",
      "('NODE_9_0_6', 'NODE_9_0_7')\n",
      "('NODE_9_0_7', 'NODE_9_0_8')\n",
      "('NODE_9_0_8', 'NODE_9_0_9')\n",
      "('NODE_9_0_9', 'BLOCK_9')\n",
      "('BLOCK_9', 'NODE_9_1_0')\n",
      "('BLOCK_9', 'NODE_9_1_9')\n",
      "('BLOCK_9', 'NODE_9_2_0')\n",
      "('BLOCK_9', 'NODE_9_2_9')\n",
      "('NODE_9_1_0', 'NODE_9_1_1')\n",
      "('NODE_9_1_1', 'NODE_9_1_2')\n",
      "('NODE_9_1_2', 'NODE_9_1_3')\n",
      "('NODE_9_1_3', 'NODE_9_1_4')\n",
      "('NODE_9_1_4', 'NODE_9_1_5')\n",
      "('NODE_9_1_5', 'NODE_9_1_6')\n",
      "('NODE_9_1_6', 'NODE_9_1_7')\n",
      "('NODE_9_1_7', 'NODE_9_1_8')\n",
      "('NODE_9_1_8', 'NODE_9_1_9')\n",
      "('NODE_9_2_0', 'NODE_9_2_1')\n",
      "('NODE_9_2_1', 'NODE_9_2_2')\n",
      "('NODE_9_2_2', 'NODE_9_2_3')\n",
      "('NODE_9_2_3', 'NODE_9_2_4')\n",
      "('NODE_9_2_4', 'NODE_9_2_5')\n",
      "('NODE_9_2_5', 'NODE_9_2_6')\n",
      "('NODE_9_2_6', 'NODE_9_2_7')\n",
      "('NODE_9_2_7', 'NODE_9_2_8')\n",
      "('NODE_9_2_8', 'NODE_9_2_9')\n"
     ]
    }
   ],
   "source": [
    "# Print all edges in the graph\n",
    "for edge in G.edges():\n",
    "    print(edge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NODE_1_2_6',\n",
       " 'NODE_1_2_2',\n",
       " 'NODE_1_2_4',\n",
       " 'NODE_1_2_9',\n",
       " 'NODE_1_2_7',\n",
       " 'NODE_1_2_1',\n",
       " 'NODE_1_2_3',\n",
       " 'NODE_1_2_0',\n",
       " 'NODE_1_2_5',\n",
       " 'NODE_1_2_8']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isolated_nodes = simulate_node_failure(G,'BLOCK_1')\n",
    "isolated_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_multiple_failures(G, elements_to_fail, failure_type='node'):\n",
    "    \"\"\"\n",
    "    Simulate multiple node or edge failures and identify isolated nodes\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        elements_to_fail: List of nodes or edges to fail\n",
    "        failure_type: 'node' or 'edge'\n",
    "    \n",
    "    Returns:\n",
    "        List of isolated nodes\n",
    "    \"\"\"\n",
    "    # Create a copy of the graph \n",
    "    after_graph = G.copy()\n",
    "    \n",
    "    # Remove all failed elements\n",
    "    if failure_type == 'node':\n",
    "        for node in elements_to_fail:\n",
    "            if node in after_graph:\n",
    "                after_graph.remove_node(node)\n",
    "    else:  # edge\n",
    "        for edge in elements_to_fail:\n",
    "            if after_graph.has_edge(*edge):\n",
    "                after_graph.remove_edge(*edge)\n",
    "    \n",
    "    # Find isolated nodes (nodes that can't reach their block)\n",
    "    isolated_nodes = []\n",
    "    \n",
    "    # Group nodes by physical ring and logical ring\n",
    "    nodes_by_ring = {}\n",
    "    for node in G.nodes():\n",
    "        if 'BLOCK' in node:\n",
    "            continue\n",
    "            \n",
    "        pr = G.nodes[node]['physicalringname']\n",
    "        lr = G.nodes[node]['lrname']\n",
    "        block = G.nodes[node]['block_name']\n",
    "        \n",
    "        key = (pr, lr, block)\n",
    "        if key not in nodes_by_ring:\n",
    "            nodes_by_ring[key] = []\n",
    "        nodes_by_ring[key].append(node)\n",
    "    \n",
    "    # Check connectivity for each group\n",
    "    for (pr, lr, block), nodes in nodes_by_ring.items():\n",
    "        if block not in after_graph:\n",
    "            # If block is gone, all nodes in this group are isolated\n",
    "            isolated_nodes.extend(nodes)\n",
    "            continue\n",
    "            \n",
    "        for node in nodes:\n",
    "            if node in elements_to_fail or node not in after_graph:\n",
    "                continue  # Skip failed nodes\n",
    "                \n",
    "            # Check if node can reach its block\n",
    "            try:\n",
    "                path = nx.shortest_path(after_graph, node, block)\n",
    "            except nx.NetworkXNoPath:\n",
    "                isolated_nodes.append(node)\n",
    "    \n",
    "    return isolated_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NODE_1_0_0',\n",
       " 'NODE_1_0_1',\n",
       " 'NODE_1_0_2',\n",
       " 'NODE_1_0_3',\n",
       " 'NODE_1_0_4',\n",
       " 'NODE_1_0_5',\n",
       " 'NODE_1_0_6',\n",
       " 'NODE_1_0_7',\n",
       " 'NODE_1_0_8',\n",
       " 'NODE_1_0_9',\n",
       " 'NODE_1_1_0',\n",
       " 'NODE_1_1_1',\n",
       " 'NODE_1_1_2',\n",
       " 'NODE_1_1_3',\n",
       " 'NODE_1_1_4',\n",
       " 'NODE_1_1_5',\n",
       " 'NODE_1_1_6',\n",
       " 'NODE_1_1_7',\n",
       " 'NODE_1_1_8',\n",
       " 'NODE_1_1_9',\n",
       " 'NODE_1_2_0',\n",
       " 'NODE_1_2_1',\n",
       " 'NODE_1_2_2',\n",
       " 'NODE_1_2_3',\n",
       " 'NODE_1_2_4',\n",
       " 'NODE_1_2_5',\n",
       " 'NODE_1_2_6',\n",
       " 'NODE_1_2_7',\n",
       " 'NODE_1_2_8',\n",
       " 'NODE_1_2_9']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isolated_nodes = simulate_multiple_failures(G,['BLOCK_1'],'node')\n",
    "isolated_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_isolation_prediction_dataset(topology_df, num_simulations=1000, max_failures=3, balance_ratio=2):\n",
    "    \"\"\"\n",
    "    Create an optimized dataset for predicting node isolations after failures\n",
    "    \n",
    "    Args:\n",
    "        topology_df: DataFrame with network topology information\n",
    "        num_simulations: Total number of examples to generate\n",
    "        max_failures: Maximum number of simultaneous failures to simulate\n",
    "        balance_ratio: Maximum ratio of non-isolation to isolation examples\n",
    "        \n",
    "    Returns:\n",
    "        data_list: List of PyG Data objects\n",
    "        node_list: List of all node names\n",
    "        node_to_idx: Dictionary mapping node names to indices\n",
    "    \"\"\"\n",
    "    print(\"Building network graph...\")\n",
    "    G = build_network_graph(topology_df)\n",
    "    \n",
    "    # Create node mapping\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # Identify regular nodes (non-block nodes)\n",
    "    regular_nodes = [n for n in node_list if \"BLOCK\" not in n]\n",
    "    \n",
    "    print(\"Computing network vulnerability metrics...\")\n",
    "    # Pre-compute network properties for isolation prediction\n",
    "    articulation_points = set(nx.articulation_points(G))\n",
    "    bridges = list(nx.bridges(G))\n",
    "    \n",
    "    # Group nodes by physical ring and logical ring\n",
    "    nodes_by_ring = {}\n",
    "    for node in regular_nodes:\n",
    "        if node not in G.nodes():\n",
    "            continue\n",
    "            \n",
    "        pr = G.nodes[node]['physicalringname']\n",
    "        lr = G.nodes[node]['lrname']\n",
    "        block = G.nodes[node]['block_name']\n",
    "        key = (pr, lr, block)\n",
    "        \n",
    "        if key not in nodes_by_ring:\n",
    "            nodes_by_ring[key] = []\n",
    "        nodes_by_ring[key].append(node)\n",
    "    \n",
    "    # Helper function to calculate path redundancy\n",
    "    def get_path_redundancy(G, node, block):\n",
    "        \"\"\"Return number of edge-disjoint paths from node to block\"\"\"\n",
    "        try:\n",
    "            return len(list(nx.edge_disjoint_paths(G, node, block)))\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    # Helper function to calculate vulnerability score\n",
    "    def calculate_vulnerability_score(G, node, block, articulation_points, bridges):\n",
    "        \"\"\"Calculate a node's vulnerability to isolation\"\"\"\n",
    "        vulnerability = 0.0\n",
    "        \n",
    "        # Factor 1: Few paths to block\n",
    "        paths = get_path_redundancy(G, node, block)\n",
    "        if paths == 0:\n",
    "            vulnerability += 1.0  # Already isolated\n",
    "        elif paths == 1:\n",
    "            vulnerability += 0.6  # Critical - only one path\n",
    "        elif paths == 2:\n",
    "            vulnerability += 0.3  # Vulnerable - two paths\n",
    "        \n",
    "        # Factor 2: Node is articulation point\n",
    "        if node in articulation_points:\n",
    "            vulnerability += 0.2\n",
    "        \n",
    "        # Factor 3: Connected to bridges\n",
    "        bridge_connections = 0\n",
    "        for u, v in bridges:\n",
    "            if node == u or node == v:\n",
    "                bridge_connections += 1\n",
    "        vulnerability += min(0.3, bridge_connections * 0.1)\n",
    "        \n",
    "        # Factor 4: Path to block goes through articulation points\n",
    "        try:\n",
    "            path = nx.shortest_path(G, node, block)\n",
    "            critical_on_path = sum(1 for n in path[1:-1] if n in articulation_points)\n",
    "            vulnerability += min(0.3, critical_on_path * 0.1)\n",
    "        except:\n",
    "            vulnerability += 0.2  # No path exists\n",
    "        \n",
    "        return min(1.0, vulnerability)  # Cap at 1.0\n",
    "    \n",
    "    print(\"Creating node features...\")\n",
    "    # Create isolation-focused node features\n",
    "    node_features = []\n",
    "    for node in node_list:\n",
    "        features = []\n",
    "        \n",
    "        # Basic features\n",
    "        pr = G.nodes[node]['physicalringname']\n",
    "        pr_hash = hash(pr) % 10\n",
    "        features.append(pr_hash/10.0)\n",
    "        \n",
    "        lr = G.nodes[node]['lrname']\n",
    "        lr_hash = hash(lr) % 10\n",
    "        features.append(lr_hash/10.0)\n",
    "        \n",
    "        is_block = 1.0 if \"BLOCK\" in node else 0.0\n",
    "        features.append(is_block)\n",
    "        \n",
    "        # Skip advanced features for block nodes\n",
    "        if is_block:\n",
    "            # Add placeholder values for block nodes to match feature dimensions\n",
    "            features.extend([0.0] * 7)  # 7 extra features for non-block nodes\n",
    "        else:\n",
    "            # Get block for this node\n",
    "            block = G.nodes[node]['block_name']\n",
    "            \n",
    "            # 1. Path redundancy - key indicator of isolation risk\n",
    "            path_redundancy = get_path_redundancy(G, node, block) / 5.0\n",
    "            features.append(path_redundancy)\n",
    "            \n",
    "            # 2. Critical point features\n",
    "            is_articulation = 1.0 if node in articulation_points else 0.0\n",
    "            has_bridge = any((node == u or node == v) for u, v in bridges)\n",
    "            features.extend([is_articulation, float(has_bridge)])\n",
    "            \n",
    "            # 3. Path to block features\n",
    "            try:\n",
    "                path_length = nx.shortest_path_length(G, node, block) / 10.0\n",
    "                path = nx.shortest_path(G, node, block)\n",
    "                critical_on_path = len(set(path[1:-1]).intersection(articulation_points)) / 5.0\n",
    "            except:\n",
    "                path_length = 1.0\n",
    "                critical_on_path = 1.0\n",
    "            features.extend([path_length, critical_on_path])\n",
    "            \n",
    "            # 4. Vulnerability score (combined metric)\n",
    "            vulnerability = calculate_vulnerability_score(G, node, block, articulation_points, bridges)\n",
    "            features.append(vulnerability)\n",
    "            \n",
    "            # 5. Network centrality\n",
    "            degree = G.degree(node) / len(G)\n",
    "            features.append(degree)\n",
    "        \n",
    "        node_features.append(features)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Create edge index\n",
    "    edges = []\n",
    "    for u, v in G.edges():\n",
    "        edges.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edges.append([node_to_idx[v], node_to_idx[u]])  # Add reverse edge for undirected graph\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Track dataset statistics\n",
    "    examples_with_isolations = []\n",
    "    examples_without_isolations = []\n",
    "    isolation_counts = []\n",
    "    failure_counts = []\n",
    "    \n",
    "    print(\"Generating dataset examples...\")\n",
    "    # PHASE 1: Generate examples that are likely to have isolations (50% of dataset)\n",
    "    target_isolation_examples = num_simulations // 2\n",
    "    phase1_attempts = 0\n",
    "    \n",
    "    while len(examples_with_isolations) < target_isolation_examples and phase1_attempts < num_simulations * 2:\n",
    "        phase1_attempts += 1\n",
    "        \n",
    "        # Choose failure type (node or edge)\n",
    "        failure_type = random.choice(['node', 'edge'])\n",
    "        \n",
    "        if failure_type == 'node':\n",
    "            # STRATEGY: Target critical nodes or nodes in the same ring\n",
    "            strategy = random.choice(['critical', 'same_ring', 'random'])\n",
    "            \n",
    "            if strategy == 'critical':\n",
    "                # Target articulation points and their neighbors\n",
    "                potential_targets = list(articulation_points.intersection(set(regular_nodes)))\n",
    "                if len(potential_targets) == 0:\n",
    "                    potential_targets = regular_nodes\n",
    "                \n",
    "                # Select 1-3 critical nodes\n",
    "                num_failures = random.randint(1, min(max_failures, len(potential_targets)))\n",
    "                nodes_to_fail = random.sample(potential_targets, num_failures)\n",
    "                \n",
    "            elif strategy == 'same_ring':\n",
    "                # Target multiple nodes in the same ring\n",
    "                if not nodes_by_ring:\n",
    "                    # Fallback to random\n",
    "                    nodes_to_fail = random.sample(regular_nodes, random.randint(1, min(max_failures, len(regular_nodes))))\n",
    "                else:\n",
    "                    # Pick a random ring\n",
    "                    ring_key = random.choice(list(nodes_by_ring.keys()))\n",
    "                    ring_nodes = nodes_by_ring[ring_key]\n",
    "                    \n",
    "                    # Select 1-3 nodes from this ring\n",
    "                    num_failures = random.randint(1, min(max_failures, len(ring_nodes)))\n",
    "                    nodes_to_fail = random.sample(ring_nodes, num_failures)\n",
    "            \n",
    "            else:  # random strategy\n",
    "                num_failures = random.randint(1, min(max_failures, len(regular_nodes)))\n",
    "                nodes_to_fail = random.sample(regular_nodes, num_failures)\n",
    "            \n",
    "            # Simulate failures\n",
    "            isolated_nodes = simulate_multiple_failures(G, nodes_to_fail, 'node')\n",
    "            \n",
    "            # Create target tensor\n",
    "            y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "            for node in isolated_nodes:\n",
    "                if node in node_to_idx:\n",
    "                    y[node_to_idx[node]] = 1.0\n",
    "            \n",
    "            # Create edge mask\n",
    "            valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "            for node in nodes_to_fail:\n",
    "                node_idx = node_to_idx[node]\n",
    "                invalid_edges = (edge_index[0] == node_idx) | (edge_index[1] == node_idx)\n",
    "                valid_edge_mask[invalid_edges] = False\n",
    "            \n",
    "            # Create data object\n",
    "            data = Data(\n",
    "                x=node_features,\n",
    "                edge_index=edge_index,\n",
    "                y=y,\n",
    "                valid_edge_mask=valid_edge_mask,\n",
    "                failure_type=torch.tensor([0]),\n",
    "                num_failures=torch.tensor([len(nodes_to_fail)], dtype=torch.long)\n",
    "            )\n",
    "            \n",
    "        else:  # edge failure\n",
    "            # STRATEGY: Target bridge edges or edges in the same ring\n",
    "            strategy = random.choice(['bridges', 'same_ring', 'random'])\n",
    "            \n",
    "            if strategy == 'bridges':\n",
    "                # Target bridge edges\n",
    "                potential_targets = [(u, v) for u, v in bridges if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                if len(potential_targets) == 0:\n",
    "                    # Fallback to random edges\n",
    "                    all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                    potential_targets = all_edges\n",
    "                \n",
    "                # Select 1-3 bridge edges\n",
    "                num_failures = random.randint(1, min(max_failures, len(potential_targets)))\n",
    "                edges_to_fail = random.sample(potential_targets, num_failures)\n",
    "                \n",
    "            elif strategy == 'same_ring':\n",
    "                # Target multiple edges in the same ring\n",
    "                ring_edges = {}\n",
    "                for u, v in G.edges():\n",
    "                    if \"BLOCK\" in u or \"BLOCK\" in v:\n",
    "                        continue\n",
    "                        \n",
    "                    try:\n",
    "                        pr_u = G.nodes[u]['physicalringname']\n",
    "                        lr_u = G.nodes[u]['lrname']\n",
    "                        pr_v = G.nodes[v]['physicalringname']\n",
    "                        lr_v = G.nodes[v]['lrname']\n",
    "                        \n",
    "                        if pr_u == pr_v and lr_u == lr_v:\n",
    "                            key = (pr_u, lr_u)\n",
    "                            if key not in ring_edges:\n",
    "                                ring_edges[key] = []\n",
    "                            ring_edges[key].append((u, v))\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if not ring_edges:\n",
    "                    # Fallback to random\n",
    "                    all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                    num_failures = random.randint(1, min(max_failures, len(all_edges)))\n",
    "                    edges_to_fail = random.sample(all_edges, num_failures)\n",
    "                else:\n",
    "                    # Pick a random ring\n",
    "                    ring_key = random.choice(list(ring_edges.keys()))\n",
    "                    ring_edge_list = ring_edges[ring_key]\n",
    "                    \n",
    "                    # Select 1-3 edges from this ring\n",
    "                    num_failures = random.randint(1, min(max_failures, len(ring_edge_list)))\n",
    "                    edges_to_fail = random.sample(ring_edge_list, num_failures)\n",
    "            \n",
    "            else:  # random strategy\n",
    "                all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                num_failures = random.randint(1, min(max_failures, len(all_edges)))\n",
    "                edges_to_fail = random.sample(all_edges, num_failures)\n",
    "            \n",
    "            # Simulate failures\n",
    "            isolated_nodes = simulate_multiple_failures(G, edges_to_fail, 'edge')\n",
    "            \n",
    "            # Create target tensor\n",
    "            y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "            for node in isolated_nodes:\n",
    "                if node in node_to_idx:\n",
    "                    y[node_to_idx[node]] = 1.0\n",
    "            \n",
    "            # Create edge mask\n",
    "            valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "            for u, v in edges_to_fail:\n",
    "                u_idx, v_idx = node_to_idx[u], node_to_idx[v]\n",
    "                \n",
    "                for i in range(edge_index.size(1)):\n",
    "                    e_u, e_v = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "                    if (e_u == u_idx and e_v == v_idx) or (e_u == v_idx and e_v == u_idx):\n",
    "                        valid_edge_mask[i] = False\n",
    "            \n",
    "            # Create data object\n",
    "            data = Data(\n",
    "                x=node_features,\n",
    "                edge_index=edge_index,\n",
    "                y=y,\n",
    "                valid_edge_mask=valid_edge_mask,\n",
    "                failure_type=torch.tensor([1]),\n",
    "                num_failures=torch.tensor([len(edges_to_fail)], dtype=torch.long)\n",
    "            )\n",
    "        \n",
    "        # Track isolation information\n",
    "        isolation_count = y.sum().item()\n",
    "        isolation_counts.append(isolation_count)\n",
    "        failure_counts.append(len(nodes_to_fail if failure_type == 'node' else edges_to_fail))\n",
    "        \n",
    "        # Add to appropriate list\n",
    "        if isolation_count > 0:\n",
    "            examples_with_isolations.append(data)\n",
    "        else:\n",
    "            examples_without_isolations.append(data)\n",
    "            \n",
    "        # Report progress\n",
    "        if phase1_attempts % 100 == 0:\n",
    "            print(f\"Phase 1: Generated {len(examples_with_isolations)} examples with isolations after {phase1_attempts} attempts\")\n",
    "    \n",
    "    # PHASE 2: Generate remaining examples randomly to complete the dataset\n",
    "    remaining = num_simulations - len(examples_with_isolations) - len(examples_without_isolations)\n",
    "    \n",
    "    if remaining > 0:\n",
    "        print(f\"Generating {remaining} additional examples for Phase 2...\")\n",
    "        \n",
    "        for _ in range(remaining):\n",
    "            # Choose failure type (node or edge)\n",
    "            failure_type = random.choice(['node', 'edge'])\n",
    "            \n",
    "            if failure_type == 'node':\n",
    "                # Choose random nodes\n",
    "                num_failures = random.randint(1, min(max_failures, len(regular_nodes)))\n",
    "                nodes_to_fail = random.sample(regular_nodes, num_failures)\n",
    "                \n",
    "                # Simulate failures\n",
    "                isolated_nodes = simulate_multiple_failures(G, nodes_to_fail, 'node')\n",
    "                \n",
    "                # Create target tensor\n",
    "                y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "                for node in isolated_nodes:\n",
    "                    if node in node_to_idx:\n",
    "                        y[node_to_idx[node]] = 1.0\n",
    "                \n",
    "                # Create edge mask\n",
    "                valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "                for node in nodes_to_fail:\n",
    "                    node_idx = node_to_idx[node]\n",
    "                    invalid_edges = (edge_index[0] == node_idx) | (edge_index[1] == node_idx)\n",
    "                    valid_edge_mask[invalid_edges] = False\n",
    "                \n",
    "                # Create data object\n",
    "                data = Data(\n",
    "                    x=node_features,\n",
    "                    edge_index=edge_index,\n",
    "                    y=y,\n",
    "                    valid_edge_mask=valid_edge_mask,\n",
    "                    failure_type=torch.tensor([0]),\n",
    "                    num_failures=torch.tensor([len(nodes_to_fail)], dtype=torch.long)\n",
    "                )\n",
    "                \n",
    "            else:  # edge failure\n",
    "                # Choose random edges\n",
    "                all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                num_failures = random.randint(1, min(max_failures, len(all_edges)))\n",
    "                edges_to_fail = random.sample(all_edges, num_failures)\n",
    "                \n",
    "                # Simulate failures\n",
    "                isolated_nodes = simulate_multiple_failures(G, edges_to_fail, 'edge')\n",
    "                \n",
    "                # Create target tensor\n",
    "                y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "                for node in isolated_nodes:\n",
    "                    if node in node_to_idx:\n",
    "                        y[node_to_idx[node]] = 1.0\n",
    "                \n",
    "                # Create edge mask\n",
    "                valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "                for u, v in edges_to_fail:\n",
    "                    u_idx, v_idx = node_to_idx[u], node_to_idx[v]\n",
    "                    \n",
    "                    for i in range(edge_index.size(1)):\n",
    "                        e_u, e_v = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "                        if (e_u == u_idx and e_v == v_idx) or (e_u == v_idx and e_v == u_idx):\n",
    "                            valid_edge_mask[i] = False\n",
    "                \n",
    "                # Create data object\n",
    "                data = Data(\n",
    "                    x=node_features,\n",
    "                    edge_index=edge_index,\n",
    "                    y=y,\n",
    "                    valid_edge_mask=valid_edge_mask,\n",
    "                    failure_type=torch.tensor([1]),\n",
    "                    num_failures=torch.tensor([len(edges_to_fail)], dtype=torch.long)\n",
    "                )\n",
    "            \n",
    "            # Track isolation information\n",
    "            isolation_count = y.sum().item()\n",
    "            isolation_counts.append(isolation_count)\n",
    "            failure_counts.append(len(nodes_to_fail if failure_type == 'node' else edges_to_fail))\n",
    "            \n",
    "            # Add to appropriate list\n",
    "            if isolation_count > 0:\n",
    "                examples_with_isolations.append(data)\n",
    "            else:\n",
    "                examples_without_isolations.append(data)\n",
    "    \n",
    "    # Balance the dataset using the specified ratio\n",
    "    print(f\"Balancing dataset with isolation:non-isolation ratio of 1:{balance_ratio}\")\n",
    "    if len(examples_without_isolations) > len(examples_with_isolations) * balance_ratio:\n",
    "        # Downsample negative examples\n",
    "        target_non_isolations = len(examples_with_isolations) * balance_ratio\n",
    "        examples_without_isolations = random.sample(examples_without_isolations, int(target_non_isolations))\n",
    "    \n",
    "    # Combine and shuffle the final dataset\n",
    "    final_data_list = examples_with_isolations + examples_without_isolations\n",
    "    random.shuffle(final_data_list)\n",
    "    \n",
    "    # Print dataset statistics\n",
    "    total_examples = len(final_data_list)\n",
    "    total_isolations = len(examples_with_isolations)\n",
    "    total_non_isolations = len(examples_without_isolations)\n",
    "    \n",
    "    print(\"\\n=== Dataset Statistics ===\")\n",
    "    print(f\"Total examples: {total_examples}\")\n",
    "    print(f\"Examples with isolations: {total_isolations} ({total_isolations/total_examples*100:.1f}%)\")\n",
    "    print(f\"Examples without isolations: {total_non_isolations} ({total_non_isolations/total_examples*100:.1f}%)\")\n",
    "    print(f\"Average isolations per positive example: {sum(c for c in isolation_counts if c > 0)/max(1, total_isolations):.2f}\")\n",
    "    print(f\"Average failures per example: {sum(failure_counts)/len(failure_counts):.2f}\")\n",
    "    \n",
    "    # Detailed breakdown by failure type\n",
    "    node_isolations = sum(1 for data in examples_with_isolations if data.failure_type.item() == 0)\n",
    "    edge_isolations = sum(1 for data in examples_with_isolations if data.failure_type.item() == 1)\n",
    "    \n",
    "    print(\"\\nIsolation examples by failure type:\")\n",
    "    print(f\"  Node failures: {node_isolations} ({node_isolations/max(1, total_isolations)*100:.1f}%)\")\n",
    "    print(f\"  Edge failures: {edge_isolations} ({edge_isolations/max(1, total_isolations)*100:.1f}%)\")\n",
    "    \n",
    "    # Breakdown by number of failures\n",
    "    failure_counts_dist = {}\n",
    "    for data in final_data_list:\n",
    "        num_fails = data.num_failures.item()\n",
    "        has_isolations = data.y.sum().item() > 0\n",
    "        \n",
    "        key = f\"{num_fails}_{'iso' if has_isolations else 'non'}\"\n",
    "        if key not in failure_counts_dist:\n",
    "            failure_counts_dist[key] = 0\n",
    "        failure_counts_dist[key] += 1\n",
    "    \n",
    "    print(\"\\nExamples by number of failures:\")\n",
    "    for k in sorted(failure_counts_dist.keys()):\n",
    "        num, iso_status = k.split('_')\n",
    "        status = \"with isolations\" if iso_status == 'iso' else \"without isolations\"\n",
    "        print(f\"  {num} failures {status}: {failure_counts_dist[k]}\")\n",
    "    \n",
    "    return final_data_list, node_list, node_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, GCNConv, GraphConv, global_mean_pool, global_max_pool\n",
    "\n",
    "class ImprovedGNN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, edge_dim=None, hidden_channels=64):\n",
    "        super(ImprovedGNN, self).__init__()\n",
    "        # Increase model capacity with more sophisticated layers\n",
    "        \n",
    "        # Initial feature extraction with GCN\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        # Attention layer to focus on important connections\n",
    "        self.gat1 = GATv2Conv(hidden_channels, hidden_channels, heads=4, dropout=0.2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels * 4)\n",
    "        \n",
    "        # Additional graph convolution for message passing\n",
    "        self.conv2 = GraphConv(hidden_channels * 4, hidden_channels * 2)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_channels * 2)\n",
    "        \n",
    "        # Final GCN layer\n",
    "        self.conv3 = GCNConv(hidden_channels * 2, hidden_channels)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        # Output layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_channels // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # Handle either individual inputs or a PyG Data object\n",
    "        if hasattr(data, 'x') and hasattr(data, 'edge_index'):\n",
    "            x, edge_index = data.x, data.edge_index\n",
    "        else:\n",
    "            # Assume direct inputs\n",
    "            x, edge_index = data\n",
    "        \n",
    "        # First convolution block\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Attention block - learn which connections matter most\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Second convolution block\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "        \n",
    "        # Final convolution\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Classification head\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_isolation_prediction_dataset(topology_df, num_simulations=5000, max_failures=5, min_isolation_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Create a high-quality dataset for training a GNN to predict node isolations after failures.\n",
    "    \n",
    "    Args:\n",
    "        topology_df: DataFrame with network topology information\n",
    "        num_simulations: Total number of examples to generate\n",
    "        max_failures: Maximum number of simultaneous failures to simulate\n",
    "        min_isolation_ratio: Minimum ratio of examples that must contain isolations\n",
    "        \n",
    "    Returns:\n",
    "        data_list: List of PyG Data objects\n",
    "        node_list: List of all node names\n",
    "        node_to_idx: Dictionary mapping node names to indices\n",
    "    \"\"\"\n",
    "    print(\"Building network graph...\")\n",
    "    G = build_network_graph(topology_df)\n",
    "    \n",
    "    # Create node mapping\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # Identify regular nodes (non-block nodes)\n",
    "    regular_nodes = [n for n in node_list if \"BLOCK\" not in n]\n",
    "    \n",
    "    print(\"Computing network vulnerability metrics...\")\n",
    "    # Pre-compute network properties for better feature engineering\n",
    "    articulation_points = set(nx.articulation_points(G))\n",
    "    bridges = list(nx.bridges(G))\n",
    "    \n",
    "    # Pre-compute edge betweenness - identifying critical edges\n",
    "    edge_betweenness = nx.edge_betweenness_centrality(G)\n",
    "    \n",
    "    # Group nodes by physical ring and logical ring\n",
    "    nodes_by_ring = {}\n",
    "    for node in regular_nodes:\n",
    "        if node not in G.nodes():\n",
    "            continue\n",
    "            \n",
    "        pr = G.nodes[node].get('physicalringname', 'unknown')\n",
    "        lr = G.nodes[node].get('lrname', 'unknown')\n",
    "        block = G.nodes[node].get('block_name', 'unknown')\n",
    "        key = (pr, lr, block)\n",
    "        \n",
    "        if key not in nodes_by_ring:\n",
    "            nodes_by_ring[key] = []\n",
    "        nodes_by_ring[key].append(node)\n",
    "    \n",
    "    print(\"Calculating node vulnerability scores...\")\n",
    "    # Calculate vulnerability score for each node\n",
    "    vulnerability_scores = {}\n",
    "    for node in regular_nodes:\n",
    "        if \"BLOCK\" in node:\n",
    "            vulnerability_scores[node] = 0.0\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            block = G.nodes[node].get('block_name', None)\n",
    "            if not block:\n",
    "                vulnerability_scores[node] = 0.5\n",
    "                continue\n",
    "                \n",
    "            # Factor 1: Path redundancy to block\n",
    "            try:\n",
    "                paths = len(list(nx.edge_disjoint_paths(G, node, block)))\n",
    "                if paths == 0:\n",
    "                    path_score = 1.0  # Already isolated\n",
    "                elif paths == 1: \n",
    "                    path_score = 0.8  # Critical - only one path\n",
    "                elif paths == 2:\n",
    "                    path_score = 0.5  # Vulnerable - two paths\n",
    "                else:\n",
    "                    path_score = 0.1  # Low risk\n",
    "            except:\n",
    "                path_score = 1.0  # Unable to calculate paths\n",
    "                \n",
    "            # Factor 2: Is it an articulation point?\n",
    "            is_articulation = 1.0 if node in articulation_points else 0.0\n",
    "            \n",
    "            # Factor 3: Connected to bridges\n",
    "            bridge_connections = sum(1 for u, v in bridges if node in (u, v))\n",
    "            bridge_score = min(1.0, bridge_connections * 0.2)\n",
    "            \n",
    "            # Factor 4: Betweenness centrality - how important is it for connecting other nodes\n",
    "            betweenness = nx.betweenness_centrality(G, k=min(100, len(G))).get(node, 0)\n",
    "            \n",
    "            # Factor 5: Closeness to block\n",
    "            try:\n",
    "                closeness = 1.0 / max(1.0, nx.shortest_path_length(G, node, block))\n",
    "            except:\n",
    "                closeness = 0.0\n",
    "                \n",
    "            # Combined score (weighted average)\n",
    "            vulnerability = (\n",
    "                0.4 * path_score + \n",
    "                0.2 * is_articulation + \n",
    "                0.2 * bridge_score + \n",
    "                0.1 * betweenness +\n",
    "                0.1 * closeness\n",
    "            )\n",
    "            \n",
    "            vulnerability_scores[node] = min(1.0, vulnerability)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating vulnerability for {node}: {e}\")\n",
    "            vulnerability_scores[node] = 0.5\n",
    "    \n",
    "    print(\"Creating node features...\")\n",
    "    # Create more informative node features\n",
    "    node_features = []\n",
    "    for node in node_list:\n",
    "        features = []\n",
    "        \n",
    "        # Basic features\n",
    "        pr = G.nodes[node].get('physicalringname', 'unknown')\n",
    "        pr_hash = int(hashlib.md5(pr.encode()).hexdigest(), 16) % 10\n",
    "        features.append(pr_hash/10.0)\n",
    "        \n",
    "        lr = G.nodes[node].get('lrname', 'unknown')\n",
    "        lr_hash = int(hashlib.md5(lr.encode()).hexdigest(), 16) % 10\n",
    "        features.append(lr_hash/10.0)\n",
    "        \n",
    "        is_block = 1.0 if \"BLOCK\" in node else 0.0\n",
    "        features.append(is_block)\n",
    "        \n",
    "        # Skip advanced features for block nodes\n",
    "        if is_block:\n",
    "            # Add placeholder values for block nodes\n",
    "            features.extend([0.0] * 8)  # 8 extra features \n",
    "        else:\n",
    "            # Topological importance features\n",
    "            degree = G.degree(node) / max(1, max(dict(G.degree()).values()))\n",
    "            features.append(degree)\n",
    "            \n",
    "            clustering = nx.clustering(G, node)\n",
    "            features.append(clustering)\n",
    "            \n",
    "            is_articulation = 1.0 if node in articulation_points else 0.0\n",
    "            features.append(is_articulation)\n",
    "            \n",
    "            # Edge connectivity features\n",
    "            bridges_connected = min(1.0, sum(1 for u, v in bridges if node in (u, v)) / 5.0)\n",
    "            features.append(bridges_connected)\n",
    "            \n",
    "            # Block connectivity\n",
    "            block = G.nodes[node].get('block_name', None)\n",
    "            if block:\n",
    "                try:\n",
    "                    path_length = nx.shortest_path_length(G, node, block) / 10.0\n",
    "                    disjoint_paths = len(list(nx.edge_disjoint_paths(G, node, block))) / 5.0\n",
    "                except:\n",
    "                    path_length = 1.0\n",
    "                    disjoint_paths = 0.0\n",
    "            else:\n",
    "                path_length = 1.0\n",
    "                disjoint_paths = 0.0\n",
    "            features.append(path_length)\n",
    "            features.append(disjoint_paths)\n",
    "            \n",
    "            # Centrality metrics\n",
    "            betweenness = nx.betweenness_centrality(G, k=min(100, len(G))).get(node, 0)\n",
    "            features.append(betweenness)\n",
    "            \n",
    "            # Pre-computed vulnerability score\n",
    "            vulnerability = vulnerability_scores.get(node, 0.5)\n",
    "            features.append(vulnerability)\n",
    "        \n",
    "        node_features.append(features)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Create edge index\n",
    "    edges = []\n",
    "    for u, v in G.edges():\n",
    "        edges.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edges.append([node_to_idx[v], node_to_idx[u]])  # Undirected graph\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Create edge features\n",
    "    edge_features = []\n",
    "    for u, v in G.edges():\n",
    "        # Compute various edge features\n",
    "        # 1. Edge betweenness\n",
    "        betweenness = edge_betweenness.get((u, v), edge_betweenness.get((v, u), 0))\n",
    "        \n",
    "        # 2. Same physical ring\n",
    "        same_pr = int(G.nodes[u].get('physicalringname') == G.nodes[v].get('physicalringname'))\n",
    "        \n",
    "        # 3. Same logical ring\n",
    "        same_lr = int(G.nodes[u].get('lrname') == G.nodes[v].get('lrname'))\n",
    "        \n",
    "        # 4. Is this a bridge?\n",
    "        is_bridge = 1.0 if (u, v) in bridges or (v, u) in bridges else 0.0\n",
    "        \n",
    "        # Duplicate for reverse edge\n",
    "        edge_features.append([betweenness, same_pr, same_lr, is_bridge])\n",
    "        edge_features.append([betweenness, same_pr, same_lr, is_bridge])\n",
    "    \n",
    "    edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "    \n",
    "    # Track dataset statistics\n",
    "    examples_with_isolations = []\n",
    "    examples_without_isolations = []\n",
    "    \n",
    "    # Generate dataset with strategic failure selection\n",
    "    print(\"Generating dataset examples...\")\n",
    "    \n",
    "    # PHASE 1: Generate examples targeting vulnerabilities\n",
    "    phase1_target = int(num_simulations * 0.7)  # 70% from targeted strategies\n",
    "    \n",
    "    while len(examples_with_isolations) + len(examples_without_isolations) < phase1_target:\n",
    "        # Choose failure type (node or edge with 70/30 split)\n",
    "        failure_type = 'node' if random.random() < 0.7 else 'edge'\n",
    "        \n",
    "        # Strategies weighted by effectiveness\n",
    "        if failure_type == 'node':\n",
    "            # Node failure strategies\n",
    "            strategies = [\n",
    "                ('high_vulnerability', 0.4),  # Target vulnerable nodes\n",
    "                ('articulation', 0.3),        # Target articulation points\n",
    "                ('same_ring', 0.2),           # Target nodes in same ring\n",
    "                ('random', 0.1)               # Random targets\n",
    "            ]\n",
    "            \n",
    "            strategy = random.choices([s[0] for s in strategies], \n",
    "                                     [s[1] for s in strategies])[0]\n",
    "            \n",
    "            if strategy == 'high_vulnerability':\n",
    "                # Target high vulnerability nodes\n",
    "                sorted_nodes = sorted([(n, vulnerability_scores.get(n, 0)) \n",
    "                                      for n in regular_nodes if \"BLOCK\" not in n],\n",
    "                                     key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                # Take from top 20% vulnerable nodes\n",
    "                top_vulnerable = sorted_nodes[:max(1, int(len(sorted_nodes) * 0.2))]\n",
    "                potential_targets = [n[0] for n in top_vulnerable]\n",
    "                \n",
    "                if not potential_targets:\n",
    "                    potential_targets = regular_nodes\n",
    "                \n",
    "                # Select 1-N failures\n",
    "                num_failures = random.randint(1, min(max_failures, len(potential_targets)))\n",
    "                nodes_to_fail = random.sample(potential_targets, num_failures)\n",
    "                \n",
    "            elif strategy == 'articulation':\n",
    "                # Target articulation points\n",
    "                potential_targets = list(articulation_points & set(regular_nodes))\n",
    "                if len(potential_targets) < 2:\n",
    "                    potential_targets = regular_nodes\n",
    "                    \n",
    "                num_failures = random.randint(1, min(max_failures, len(potential_targets)))\n",
    "                nodes_to_fail = random.sample(potential_targets, num_failures)\n",
    "                \n",
    "            elif strategy == 'same_ring':\n",
    "                # Select nodes in the same ring \n",
    "                if not nodes_by_ring:\n",
    "                    # Fallback to random\n",
    "                    nodes_to_fail = random.sample(regular_nodes, \n",
    "                                                 random.randint(1, min(max_failures, len(regular_nodes))))\n",
    "                else:\n",
    "                    # Pick a random ring group\n",
    "                    rings_with_enough_nodes = [key for key, nodes in nodes_by_ring.items() \n",
    "                                              if len(nodes) >= 2]\n",
    "                    \n",
    "                    if not rings_with_enough_nodes:\n",
    "                        # Fallback to random  \n",
    "                        nodes_to_fail = random.sample(regular_nodes,\n",
    "                                                     random.randint(1, min(max_failures, len(regular_nodes))))\n",
    "                    else:\n",
    "                        ring_key = random.choice(rings_with_enough_nodes)\n",
    "                        ring_nodes = nodes_by_ring[ring_key]\n",
    "                        \n",
    "                        # Select multiple nodes from this ring\n",
    "                        num_failures = random.randint(1, min(max_failures, len(ring_nodes)))\n",
    "                        nodes_to_fail = random.sample(ring_nodes, num_failures)\n",
    "            \n",
    "            else:  # random strategy\n",
    "                num_failures = random.randint(1, min(max_failures, len(regular_nodes)))\n",
    "                nodes_to_fail = random.sample(regular_nodes, num_failures)\n",
    "            \n",
    "            # Simulate failures\n",
    "            isolated_nodes = simulate_multiple_failures(G, nodes_to_fail, 'node')\n",
    "            \n",
    "            # Create target tensor\n",
    "            y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "            for node in isolated_nodes:\n",
    "                if node in node_to_idx:\n",
    "                    y[node_to_idx[node]] = 1.0\n",
    "            \n",
    "            # Create edge mask\n",
    "            valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "            for node in nodes_to_fail:\n",
    "                node_idx = node_to_idx[node]\n",
    "                invalid_edges = (edge_index[0] == node_idx) | (edge_index[1] == node_idx)\n",
    "                valid_edge_mask[invalid_edges] = False\n",
    "            \n",
    "            # Create data object\n",
    "            data = Data(\n",
    "                x=node_features,\n",
    "                edge_index=edge_index,\n",
    "                edge_attr=edge_attr,\n",
    "                y=y,\n",
    "                valid_edge_mask=valid_edge_mask,\n",
    "                failure_type=torch.tensor([0]),\n",
    "                failed_elements=[node_to_idx[n] for n in nodes_to_fail],\n",
    "                num_failures=torch.tensor([len(nodes_to_fail)], dtype=torch.long)\n",
    "            )\n",
    "            \n",
    "        else:  # edge failure\n",
    "            # Edge failure strategies\n",
    "            strategies = [\n",
    "                ('bridges', 0.4),       # Target bridge edges \n",
    "                ('high_betweenness', 0.3),  # Target high betweenness edges\n",
    "                ('same_ring', 0.2),     # Target edges in the same ring\n",
    "                ('random', 0.1)         # Random edges\n",
    "            ]\n",
    "            \n",
    "            strategy = random.choices([s[0] for s in strategies], \n",
    "                                     [s[1] for s in strategies])[0]\n",
    "            \n",
    "            if strategy == 'bridges':\n",
    "                # Target bridge edges\n",
    "                potential_targets = [(u, v) for u, v in bridges if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                if len(potential_targets) == 0:\n",
    "                    # Fallback to random edges\n",
    "                    all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                    potential_targets = all_edges\n",
    "                \n",
    "                # Select multiple bridge edges\n",
    "                num_failures = random.randint(1, min(max_failures, len(potential_targets)))\n",
    "                edges_to_fail = random.sample(potential_targets, num_failures)\n",
    "                \n",
    "            elif strategy == 'high_betweenness':\n",
    "                # Target high betweenness edges\n",
    "                sorted_edges = sorted([(u, v) for u, v in edge_betweenness.keys() \n",
    "                                     if \"BLOCK\" not in u and \"BLOCK\" not in v],\n",
    "                                    key=lambda e: edge_betweenness.get(e, 0), \n",
    "                                    reverse=True)\n",
    "                \n",
    "                # Take top 20% high betweenness edges\n",
    "                potential_targets = sorted_edges[:max(1, int(len(sorted_edges) * 0.2))]\n",
    "                \n",
    "                if not potential_targets:\n",
    "                    # Fallback to random\n",
    "                    all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                    potential_targets = all_edges\n",
    "                    \n",
    "                num_failures = random.randint(1, min(max_failures, len(potential_targets)))\n",
    "                edges_to_fail = random.sample(potential_targets, num_failures)\n",
    "                \n",
    "            elif strategy == 'same_ring':\n",
    "                # Target multiple edges in the same ring\n",
    "                ring_edges = {}\n",
    "                for u, v in G.edges():\n",
    "                    if \"BLOCK\" in u or \"BLOCK\" in v:\n",
    "                        continue\n",
    "                        \n",
    "                    try:\n",
    "                        pr_u = G.nodes[u].get('physicalringname')\n",
    "                        lr_u = G.nodes[u].get('lrname')\n",
    "                        pr_v = G.nodes[v].get('physicalringname')\n",
    "                        lr_v = G.nodes[v].get('lrname')\n",
    "                        \n",
    "                        if pr_u == pr_v and lr_u == lr_v:\n",
    "                            key = (pr_u, lr_u)\n",
    "                            if key not in ring_edges:\n",
    "                                ring_edges[key] = []\n",
    "                            ring_edges[key].append((u, v))\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if not ring_edges:\n",
    "                    # Fallback to random\n",
    "                    all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                    num_failures = random.randint(1, min(max_failures, len(all_edges)))\n",
    "                    edges_to_fail = random.sample(all_edges, num_failures)\n",
    "                else:\n",
    "                    # Pick a random ring\n",
    "                    ring_key = random.choice(list(ring_edges.keys()))\n",
    "                    ring_edge_list = ring_edges[ring_key]\n",
    "                    \n",
    "                    # Select multiple edges from this ring\n",
    "                    num_failures = random.randint(1, min(max_failures, len(ring_edge_list)))\n",
    "                    edges_to_fail = random.sample(ring_edge_list, num_failures)\n",
    "            \n",
    "            else:  # random strategy\n",
    "                all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                num_failures = random.randint(1, min(max_failures, len(all_edges)))\n",
    "                edges_to_fail = random.sample(all_edges, num_failures)\n",
    "            \n",
    "            # Simulate failures\n",
    "            isolated_nodes = simulate_multiple_failures(G, edges_to_fail, 'edge')\n",
    "            \n",
    "            # Create target tensor\n",
    "            y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "            for node in isolated_nodes:\n",
    "                if node in node_to_idx:\n",
    "                    y[node_to_idx[node]] = 1.0\n",
    "            \n",
    "            # Create edge mask\n",
    "            valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "            failed_edge_indices = []\n",
    "            \n",
    "            for u, v in edges_to_fail:\n",
    "                u_idx, v_idx = node_to_idx[u], node_to_idx[v]\n",
    "                \n",
    "                for i in range(edge_index.size(1)):\n",
    "                    e_u, e_v = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "                    if (e_u == u_idx and e_v == v_idx) or (e_u == v_idx and e_v == u_idx):\n",
    "                        valid_edge_mask[i] = False\n",
    "                        failed_edge_indices.append(i)\n",
    "            \n",
    "            # Create data object\n",
    "            data = Data(\n",
    "                x=node_features,\n",
    "                edge_index=edge_index,\n",
    "                edge_attr=edge_attr,\n",
    "                y=y,\n",
    "                valid_edge_mask=valid_edge_mask,\n",
    "                failure_type=torch.tensor([1]),\n",
    "                failed_elements=failed_edge_indices,\n",
    "                num_failures=torch.tensor([len(edges_to_fail)], dtype=torch.long)\n",
    "            )\n",
    "        \n",
    "        # Add to appropriate list\n",
    "        if y.sum().item() > 0:\n",
    "            examples_with_isolations.append(data)\n",
    "        else:\n",
    "            examples_without_isolations.append(data)\n",
    "            \n",
    "        # Report progress\n",
    "        total_examples = len(examples_with_isolations) + len(examples_without_isolations)\n",
    "        if total_examples % 100 == 0:\n",
    "            isolation_ratio = len(examples_with_isolations) / max(1, total_examples)\n",
    "            print(f\"Generated {total_examples} examples, {len(examples_with_isolations)} with isolations ({isolation_ratio:.2f})\")\n",
    "    \n",
    "    # PHASE 2: Generate remaining examples with random approach\n",
    "    phase2_target = num_simulations - len(examples_with_isolations) - len(examples_without_isolations)\n",
    "    \n",
    "    if phase2_target > 0:\n",
    "        print(f\"Generating {phase2_target} additional examples for Phase 2...\")\n",
    "        \n",
    "        for _ in range(phase2_target):\n",
    "            # Choose failure type (node or edge)\n",
    "            failure_type = random.choice(['node', 'edge'])\n",
    "            \n",
    "            if failure_type == 'node':\n",
    "                # Choose random nodes\n",
    "                num_failures = random.randint(1, min(max_failures, len(regular_nodes)))\n",
    "                nodes_to_fail = random.sample(regular_nodes, num_failures)\n",
    "                \n",
    "                # Simulate failures\n",
    "                isolated_nodes = simulate_multiple_failures(G, nodes_to_fail, 'node')\n",
    "                \n",
    "                # Create target tensor\n",
    "                y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "                for node in isolated_nodes:\n",
    "                    if node in node_to_idx:\n",
    "                        y[node_to_idx[node]] = 1.0\n",
    "                \n",
    "                # Create edge mask\n",
    "                valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "                for node in nodes_to_fail:\n",
    "                    node_idx = node_to_idx[node]\n",
    "                    invalid_edges = (edge_index[0] == node_idx) | (edge_index[1] == node_idx)\n",
    "                    valid_edge_mask[invalid_edges] = False\n",
    "                \n",
    "                # Create data object\n",
    "                data = Data(\n",
    "                    x=node_features,\n",
    "                    edge_index=edge_index,\n",
    "                    edge_attr=edge_attr,\n",
    "                    y=y,\n",
    "                    valid_edge_mask=valid_edge_mask,\n",
    "                    failure_type=torch.tensor([0]),\n",
    "                    failed_elements=[node_to_idx[n] for n in nodes_to_fail],\n",
    "                    num_failures=torch.tensor([len(nodes_to_fail)], dtype=torch.long)\n",
    "                )\n",
    "                \n",
    "            else:  # edge failure\n",
    "                # Choose random edges\n",
    "                all_edges = [(u, v) for u, v in G.edges() if \"BLOCK\" not in u and \"BLOCK\" not in v]\n",
    "                num_failures = random.randint(1, min(max_failures, len(all_edges)))\n",
    "                edges_to_fail = random.sample(all_edges, num_failures)\n",
    "                \n",
    "                # Simulate failures\n",
    "                isolated_nodes = simulate_multiple_failures(G, edges_to_fail, 'edge')\n",
    "                \n",
    "                # Create target tensor\n",
    "                y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "                for node in isolated_nodes:\n",
    "                    if node in node_to_idx:\n",
    "                        y[node_to_idx[node]] = 1.0\n",
    "                \n",
    "                # Create edge mask\n",
    "                valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "                failed_edge_indices = []\n",
    "                \n",
    "                for u, v in edges_to_fail:\n",
    "                    u_idx, v_idx = node_to_idx[u], node_to_idx[v]\n",
    "                    \n",
    "                    for i in range(edge_index.size(1)):\n",
    "                        e_u, e_v = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "                        if (e_u == u_idx and e_v == v_idx) or (e_u == v_idx and e_v == u_idx):\n",
    "                            valid_edge_mask[i] = False\n",
    "                            failed_edge_indices.append(i)\n",
    "                \n",
    "                # Create data object\n",
    "                data = Data(\n",
    "                    x=node_features,\n",
    "                    edge_index=edge_index,\n",
    "                    edge_attr=edge_attr,\n",
    "                    y=y,\n",
    "                    valid_edge_mask=valid_edge_mask,\n",
    "                    failure_type=torch.tensor([1]),\n",
    "                    failed_elements=failed_edge_indices,\n",
    "                    num_failures=torch.tensor([len(edges_to_fail)], dtype=torch.long)\n",
    "                )\n",
    "            \n",
    "            # Add to appropriate list\n",
    "            if y.sum().item() > 0:\n",
    "                examples_with_isolations.append(data)\n",
    "            else:\n",
    "                examples_without_isolations.append(data)\n",
    "    \n",
    "    # Balance the dataset based on min_isolation_ratio\n",
    "    isolation_count = len(examples_with_isolations)\n",
    "    total_count = isolation_count + len(examples_without_isolations)\n",
    "    current_ratio = isolation_count / total_count\n",
    "    \n",
    "    print(f\"Current isolation ratio: {current_ratio:.2f}, target: {min_isolation_ratio:.2f}\")\n",
    "    \n",
    "    if current_ratio < min_isolation_ratio and len(examples_without_isolations) > 0:\n",
    "        # Need to remove some non-isolation examples\n",
    "        target_non_isolation = int(isolation_count * (1 - min_isolation_ratio) / min_isolation_ratio)\n",
    "        if target_non_isolation < len(examples_without_isolations):\n",
    "            print(f\"Downsampling non-isolation examples from {len(examples_without_isolations)} to {target_non_isolation}\")\n",
    "            examples_without_isolations = random.sample(examples_without_isolations, target_non_isolation)\n",
    "    \n",
    "    # Combine and shuffle the final dataset\n",
    "    final_data_list = examples_with_isolations + examples_without_isolations\n",
    "    random.shuffle(final_data_list)\n",
    "    \n",
    "    # Print dataset statistics\n",
    "    total_examples = len(final_data_list)\n",
    "    total_isolations = len(examples_with_isolations)\n",
    "    total_non_isolations = len(examples_without_isolations)\n",
    "    \n",
    "    print(\"\\n=== Dataset Statistics ===\")\n",
    "    print(f\"Total examples: {total_examples}\")\n",
    "    print(f\"Examples with isolations: {total_isolations} ({total_isolations/total_examples*100:.1f}%)\")\n",
    "    print(f\"Examples without isolations: {total_non_isolations} ({total_non_isolations/total_examples*100:.1f}%)\")\n",
    "    \n",
    "    # Calculate average isolations per positive example\n",
    "    avg_isolations = sum(data.y.sum().item() for data in examples_with_isolations) / max(1, total_isolations)\n",
    "    print(f\"Average isolated nodes per positive example: {avg_isolations:.2f}\")\n",
    "    \n",
    "    # Detailed breakdown by failure type\n",
    "    node_failures = sum(1 for data in final_data_list if data.failure_type.item() == 0)\n",
    "    edge_failures = sum(1 for data in final_data_list if data.failure_type.item() == 1)\n",
    "    \n",
    "    node_isolations = sum(1 for data in examples_with_isolations if data.failure_type.item() == 0)\n",
    "    edge_isolations = sum(1 for data in examples_with_isolations if data.failure_type.item() == 1)\n",
    "    \n",
    "    print(\"\\nExamples by failure type:\")\n",
    "    print(f\"  Node failures: {node_failures} ({node_failures/total_examples*100:.1f}%)\")\n",
    "    print(f\"  Edge failures: {edge_failures} ({edge_failures/total_examples*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nIsolation examples by failure type:\")\n",
    "    print(f\"  Node failures with isolations: {node_isolations} ({node_isolations/total_isolations*100:.1f}%)\")\n",
    "    print(f\"  Edge failures with isolations: {edge_isolations} ({edge_isolations/total_isolations*100:.1f}%)\")\n",
    "    \n",
    "    # Breakdown by number of failures\n",
    "    failure_counts = {}\n",
    "    for data in final_data_list:\n",
    "        num_fails = data.num_failures.item()\n",
    "        has_isolations = data.y.sum().item() > 0\n",
    "        \n",
    "        key = f\"{num_fails}_{'iso' if has_isolations else 'non'}\"\n",
    "        if key not in failure_counts:\n",
    "            failure_counts[key] = 0\n",
    "        failure_counts[key] += 1\n",
    "    \n",
    "    print(\"\\nExamples by number of failures:\")\n",
    "    for k in sorted(failure_counts.keys()):\n",
    "        num, iso_status = k.split('_')\n",
    "        status = \"with isolations\" if iso_status == 'iso' else \"without isolations\"\n",
    "        print(f\"  {num} failures {status}: {failure_counts[k]}\")\n",
    "    \n",
    "    return final_data_list, node_list, node_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def train_isolation_model(data_list, epochs=50):\n",
    "    # Split data\n",
    "    total = len(data_list)\n",
    "    train_size = int(0.7 * total)\n",
    "    val_size = int(0.15 * total)\n",
    "    test_size = total - train_size - val_size\n",
    "    \n",
    "    # Shuffle and split\n",
    "    random.shuffle(data_list)\n",
    "    train_data = data_list[:train_size]\n",
    "    val_data = data_list[train_size:train_size+val_size]\n",
    "    test_data = data_list[train_size+val_size:]\n",
    "    \n",
    "    # Count positives in training set\n",
    "    positive_count = sum(data.y.sum().item() > 0 for data in train_data)\n",
    "    positive_rate = positive_count / len(train_data)\n",
    "    print(f\"Training set: {len(train_data)} examples, {positive_count} with isolations ({positive_rate:.2%})\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Initialize model with edge features if available\n",
    "    edge_dim = train_data[0].edge_attr.size(1) if hasattr(train_data[0], 'edge_attr') else None\n",
    "    model = ImprovedGNN(num_node_features=train_data[0].x.size(1), edge_dim=edge_dim)\n",
    "\n",
    "    \n",
    "    # Calculate positive weight based on class imbalance\n",
    "    # This is crucial! Use a large value (50.0) to force the model to predict positives\n",
    "    pos_weight = torch.tensor(40.0)  # Very high weight for rare isolation cases\n",
    "    \n",
    "    # You could also calculate from your data:\n",
    "    # pos_samples = sum(d.y.sum().item() for d in train_data)\n",
    "    # neg_samples = sum((~d.y.bool()).sum().item() for d in train_data)\n",
    "    # pos_weight = torch.tensor(neg_samples / max(1, pos_samples))\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    # Use Adam with weight decay and a lower learning rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=10, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_f1 = 0\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            \n",
    "            # Flatten predictions and targets\n",
    "            pred = out.squeeze()\n",
    "            target = batch.y\n",
    "            \n",
    "            loss = criterion(pred, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent explosion\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                out = model(batch)\n",
    "                pred = out.squeeze()\n",
    "                target = batch.y\n",
    "                \n",
    "                loss = criterion(pred, target)\n",
    "                val_loss += loss.item() * batch.num_graphs\n",
    "                \n",
    "                # Use a MUCH LOWER threshold (0.1 or 0.2) to detect isolations\n",
    "                # This counteracts the model's tendency to predict all negatives\n",
    "                binary_pred = (torch.sigmoid(pred) > 0.2).float()\n",
    "                \n",
    "                all_preds.append(binary_pred)\n",
    "                all_targets.append(target)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        # Concatenate predictions and targets\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_targets = torch.cat(all_targets, dim=0)\n",
    "        \n",
    "        # Calculate metrics - but handle the case of no positive predictions\n",
    "        tp = ((all_preds == 1) & (all_targets == 1)).sum().item()\n",
    "        fp = ((all_preds == 1) & (all_targets == 0)).sum().item()\n",
    "        fn = ((all_preds == 0) & (all_targets == 1)).sum().item()\n",
    "        tn = ((all_preds == 0) & (all_targets == 0)).sum().item()\n",
    "        \n",
    "        precision = tp / max(tp + fp, 1)\n",
    "        recall = tp / max(tp + fn, 1)\n",
    "        f1 = 2 * precision * recall / max(precision + recall, 1e-8)\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        \n",
    "        # Update learning rate based on validation loss\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Early stopping based on F1 score\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_isolation_model.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best F1: {best_f1:.4f}\")\n",
    "                model.load_state_dict(torch.load(\"best_isolation_model.pt\"))\n",
    "                break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_example(G, node_list, node_to_idx, failure_type, elements_to_fail):\n",
    "    \"\"\"\n",
    "    Create a PyG Data object for testing the model with specific failures.\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        node_list: List of all nodes in the graph\n",
    "        node_to_idx: Mapping from node names to indices\n",
    "        failure_type: 'node' or 'edge'\n",
    "        elements_to_fail: List of nodes or edges to fail\n",
    "        \n",
    "    Returns:\n",
    "        PyG Data object ready for model inference\n",
    "    \"\"\"\n",
    "    # Create node features - must match the exact features used in training\n",
    "    node_features = []\n",
    "    for node in node_list:\n",
    "        features = []\n",
    "        \n",
    "        # Basic features\n",
    "        pr = G.nodes[node].get('physicalringname', 'unknown')\n",
    "        pr_hash = int(hashlib.md5(pr.encode()).hexdigest(), 16) % 10\n",
    "        features.append(pr_hash/10.0)\n",
    "        \n",
    "        lr = G.nodes[node].get('lrname', 'unknown')\n",
    "        lr_hash = int(hashlib.md5(lr.encode()).hexdigest(), 16) % 10\n",
    "        features.append(lr_hash/10.0)\n",
    "        \n",
    "        is_block = 1.0 if \"BLOCK\" in node else 0.0\n",
    "        features.append(is_block)\n",
    "        \n",
    "        # Add the same advanced features used in your training dataset\n",
    "        if is_block:\n",
    "            # Add placeholder values for block nodes\n",
    "            features.extend([0.0] * 8)  # Must match the number used in training\n",
    "        else:\n",
    "            # Calculate same advanced features\n",
    "            degree = G.degree(node) / max(1, max(dict(G.degree()).values()))\n",
    "            features.append(degree)\n",
    "            \n",
    "            clustering = nx.clustering(G, node)\n",
    "            features.append(clustering)\n",
    "            \n",
    "            # Precompute these if needed for efficiency\n",
    "            articulation_points = set(nx.articulation_points(G))\n",
    "            bridges = list(nx.bridges(G))\n",
    "            \n",
    "            is_articulation = 1.0 if node in articulation_points else 0.0\n",
    "            features.append(is_articulation)\n",
    "            \n",
    "            bridges_connected = min(1.0, sum(1 for u, v in bridges if node in (u, v)) / 5.0)\n",
    "            features.append(bridges_connected)\n",
    "            \n",
    "            # Add the rest of your advanced features here\n",
    "            # For example:\n",
    "            block = G.nodes[node].get('block_name', None)\n",
    "            if block:\n",
    "                try:\n",
    "                    path_length = nx.shortest_path_length(G, node, block) / 10.0\n",
    "                    disjoint_paths = len(list(nx.edge_disjoint_paths(G, node, block))) / 5.0\n",
    "                    features.append(path_length)\n",
    "                    features.append(disjoint_paths)\n",
    "                except:\n",
    "                    features.append(1.0)  # Max path length\n",
    "                    features.append(0.0)  # No paths\n",
    "            else:\n",
    "                features.append(1.0)\n",
    "                features.append(0.0)\n",
    "                \n",
    "            # Isolation risk features\n",
    "            single_path_vulnerability = 1.0 if disjoint_paths <= 1 else 0.0\n",
    "            features.append(single_path_vulnerability)\n",
    "            \n",
    "            # Add other isolation risk features\n",
    "            neighbors = list(G.neighbors(node))\n",
    "            connects_to_articulation = 1.0 if any(n in articulation_points for n in neighbors) else 0.0\n",
    "            features.append(connects_to_articulation)\n",
    "            \n",
    "            isolation_risk = max(single_path_vulnerability, is_articulation, bridges_connected)\n",
    "            features.append(isolation_risk)\n",
    "            \n",
    "        node_features.append(features)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Create edge index\n",
    "    edge_list = []\n",
    "    for u, v in G.edges():\n",
    "        u_idx, v_idx = node_to_idx[u], node_to_idx[v]\n",
    "        edge_list.append([u_idx, v_idx])\n",
    "        edge_list.append([v_idx, u_idx])  # Add both directions\n",
    "    \n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Create edge features if used in your model\n",
    "    edge_attr = None\n",
    "    if hasattr(G.edges[list(G.edges())[0]], 'features'):\n",
    "        edge_attr = []\n",
    "        for i, (u, v) in enumerate(edge_list):\n",
    "            u_name, v_name = node_list[u], node_list[v]\n",
    "            if G.has_edge(u_name, v_name):\n",
    "                edge_attr.append(G.edges[u_name, v_name].get('features', [0.0]))\n",
    "            else:\n",
    "                edge_attr.append([0.0])\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    \n",
    "    # Create edge mask for failure simulation\n",
    "    valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "    \n",
    "    if failure_type == 'node':\n",
    "        for node in elements_to_fail:\n",
    "            node_idx = node_to_idx[node]\n",
    "            invalid_edges = (edge_index[0] == node_idx) | (edge_index[1] == node_idx)\n",
    "            valid_edge_mask[invalid_edges] = False\n",
    "    else:  # edge failure\n",
    "        for u, v in elements_to_fail:\n",
    "            u_idx, v_idx = node_to_idx[u], node_to_idx[v]\n",
    "            for i in range(edge_index.size(1)):\n",
    "                e_u, e_v = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "                if (e_u == u_idx and e_v == v_idx) or (e_u == v_idx and e_v == u_idx):\n",
    "                    valid_edge_mask[i] = False\n",
    "    \n",
    "    # Create Data object\n",
    "    data = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        valid_edge_mask=valid_edge_mask,\n",
    "        failure_type=torch.tensor([0 if failure_type == 'node' else 1]),\n",
    "        failed_elements=[node_to_idx[n] if failure_type == 'node' \n",
    "                        else [node_to_idx[u], node_to_idx[v]] for n in elements_to_fail]\n",
    "    )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network graph...\n",
      "Computing network vulnerability metrics...\n",
      "Calculating node vulnerability scores...\n",
      "Creating node features...\n",
      "Generating dataset examples...\n",
      "Generated 100 examples, 65 with isolations (0.65)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GCNConv, GATConv\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 1. Create better dataset with proper balance\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m data_list, node_list, node_to_idx = \u001b[43mcreate_isolation_prediction_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopo_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_failures\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_isolation_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Force 50% of examples to have isolations\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 409\u001b[39m, in \u001b[36mcreate_isolation_prediction_dataset\u001b[39m\u001b[34m(topology_df, num_simulations, max_failures, min_isolation_ratio)\u001b[39m\n\u001b[32m    406\u001b[39m u_idx, v_idx = node_to_idx[u], node_to_idx[v]\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(edge_index.size(\u001b[32m1\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     e_u, e_v = \u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, edge_index[\u001b[32m1\u001b[39m, i].item()\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (e_u == u_idx \u001b[38;5;129;01mand\u001b[39;00m e_v == v_idx) \u001b[38;5;129;01mor\u001b[39;00m (e_u == v_idx \u001b[38;5;129;01mand\u001b[39;00m e_v == u_idx):\n\u001b[32m    411\u001b[39m         valid_edge_mask[i] = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import copy\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "\n",
    "# 1. Create better dataset with proper balance\n",
    "data_list, node_list, node_to_idx = create_isolation_prediction_dataset(\n",
    "    topo_data,\n",
    "    num_simulations=5000,\n",
    "    max_failures=20,\n",
    "    min_isolation_ratio=0.5 # Force 50% of examples to have isolations\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 3500 examples, 2201 with isolations (62.89%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nms/.local/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 1.0576, Val Loss: 1.0294, Precision: 0.0236, Recall: 1.0000, F1: 0.0461, Accuracy: 0.2441\n",
      "Epoch 1: Train Loss: 1.0311, Val Loss: 1.0282, Precision: 0.0236, Recall: 1.0000, F1: 0.0461, Accuracy: 0.2441\n",
      "Epoch 2: Train Loss: 1.0296, Val Loss: 1.0267, Precision: 0.0236, Recall: 1.0000, F1: 0.0461, Accuracy: 0.2441\n",
      "Epoch 3: Train Loss: 1.0289, Val Loss: 1.0259, Precision: 0.0236, Recall: 1.0000, F1: 0.0461, Accuracy: 0.2441\n",
      "Epoch 4: Train Loss: 1.0282, Val Loss: 1.0258, Precision: 0.0236, Recall: 1.0000, F1: 0.0461, Accuracy: 0.2441\n",
      "Epoch 5: Train Loss: 1.0284, Val Loss: 1.0256, Precision: 0.0236, Recall: 1.0000, F1: 0.0461, Accuracy: 0.2441\n",
      "Epoch 6: Train Loss: 1.0276, Val Loss: 1.0258, Precision: 0.0236, Recall: 1.0000, F1: 0.0461, Accuracy: 0.2441\n",
      "Epoch 7: Train Loss: 1.0279, Val Loss: 1.0261, Precision: 0.0236, Recall: 1.0000, F1: 0.0461, Accuracy: 0.2441\n",
      "Epoch 8: Train Loss: 1.0274, Val Loss: 1.0255, Precision: 0.0236, Recall: 1.0000, F1: 0.0461, Accuracy: 0.2441\n",
      "Epoch 9: Train Loss: 1.0274, Val Loss: 1.0254, Precision: 0.0236, Recall: 1.0000, F1: 0.0461, Accuracy: 0.2441\n",
      "Epoch 10: Train Loss: 1.0276, Val Loss: 1.0255, Precision: 0.0236, Recall: 1.0000, F1: 0.0461, Accuracy: 0.2441\n",
      "Early stopping at epoch 10. Best F1: 0.0461\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Train with the improved loss function\n",
    "model = train_isolation_model(data_list, epochs=100)\n",
    "\n",
    "# 3. Test with a known isolation case (lower threshold!)\n",
    "def test_model_with_known_case():\n",
    "    # Find a node that causes isolations when it fails\n",
    "    for node in G.nodes():\n",
    "        if \"BLOCK\" not in node:\n",
    "            isolated = simulate_multiple_failures(G, [node], 'node')\n",
    "            if len(isolated) > 0:\n",
    "                print(f\"Testing with node {node} which should cause {len(isolated)} isolations\")\n",
    "                \n",
    "                # Create test data\n",
    "                test_data = create_test_example(G, node_list, node_to_idx, 'node', [node])\n",
    "                \n",
    "                # Predict\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    pred = model(test_data)\n",
    "                    # Use lower threshold here!\n",
    "                    binary_pred = (torch.sigmoid(pred.squeeze()) > 0.2).float()\n",
    "                \n",
    "                # Check results\n",
    "                predicted_nodes = [node_list[i] for i in range(len(node_list)) if binary_pred[i] > 0.5]\n",
    "                print(f\"Actual isolated: {isolated}\")\n",
    "                print(f\"Predicted isolated: {predicted_nodes}\")\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_isolated_nodes(model, G, node_list, node_to_idx, failure_type, elements_to_fail, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Predict which nodes will become isolated after failures using a trained GNN model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained GNN model\n",
    "        G: NetworkX graph\n",
    "        node_list: List of all nodes in the graph\n",
    "        node_to_idx: Mapping from node names to indices\n",
    "        failure_type: 'node' or 'edge'\n",
    "        elements_to_fail: Node(s) or edge(s) to fail\n",
    "        threshold: Probability threshold for considering a node isolated (lower is more sensitive)\n",
    "    \n",
    "    Returns:\n",
    "        List of node names predicted to be isolated\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Pre-compute network properties for better feature engineering\n",
    "    articulation_points = set(nx.articulation_points(G))\n",
    "    bridges = list(nx.bridges(G))\n",
    "    \n",
    "    # Create node features\n",
    "    node_features = []\n",
    "    for node in node_list:\n",
    "        # Feature 1-2: Physical and logical ring\n",
    "        pr = G.nodes[node].get('physicalringname', 'unknown')\n",
    "        pr_hash = hash(pr) % 10\n",
    "        \n",
    "        lr = G.nodes[node].get('lrname', 'unknown')\n",
    "        lr_hash = hash(lr) % 10\n",
    "        \n",
    "        # Feature 3: Is it a block node?\n",
    "        is_block = 1.0 if \"BLOCK\" in node else 0.0\n",
    "        \n",
    "        # Feature 4-5: Network properties\n",
    "        degree = G.degree(node) / max(1, len(G))\n",
    "        clustering = nx.clustering(G, node)\n",
    "        \n",
    "        # Feature 6: Is articulation point?\n",
    "        is_articulation = 1.0 if node in articulation_points else 0.0\n",
    "        \n",
    "        # Feature 7: Connected to bridges\n",
    "        bridges_connected = min(1.0, sum(1 for u, v in bridges if node in (u, v)) / 5.0)\n",
    "        \n",
    "        # Features 8-9: Path to block\n",
    "        block = G.nodes[node].get('block_name', None)\n",
    "        if block and not is_block:\n",
    "            try:\n",
    "                path_length = nx.shortest_path_length(G, node, block) / 10.0\n",
    "                disjoint_paths = len(list(nx.edge_disjoint_paths(G, node, block))) / 5.0\n",
    "            except:\n",
    "                path_length = 1.0\n",
    "                disjoint_paths = 0.0\n",
    "        else:\n",
    "            path_length = 0.0 if is_block else 1.0\n",
    "            disjoint_paths = 1.0 if is_block else 0.0\n",
    "        \n",
    "        # Feature 10: Single path vulnerability\n",
    "        single_path_vulnerability = 1.0 if not is_block and disjoint_paths <= 0.2 else 0.0\n",
    "        \n",
    "        # Feature 11: Isolation risk\n",
    "        isolation_risk = max(single_path_vulnerability, is_articulation, bridges_connected) if not is_block else 0.0\n",
    "        \n",
    "        # Combine all features\n",
    "        node_features.append([\n",
    "            pr_hash/10.0, \n",
    "            lr_hash/10.0, \n",
    "            is_block, \n",
    "            degree, \n",
    "            clustering, \n",
    "            is_articulation, \n",
    "            bridges_connected, \n",
    "            path_length, \n",
    "            disjoint_paths, \n",
    "            single_path_vulnerability, \n",
    "            isolation_risk\n",
    "        ])\n",
    "    \n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Create edge index\n",
    "    edges = []\n",
    "    for u, v in G.edges():\n",
    "        edges.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edges.append([node_to_idx[v], node_to_idx[u]])  # Add reverse edge for undirected graph\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Create edge attributes - FIXED to have exactly 4 features per edge\n",
    "    edge_attr = []\n",
    "    for i in range(edge_index.size(1)):\n",
    "        u_idx, v_idx = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "        u_node, v_node = node_list[u_idx], node_list[v_idx]\n",
    "        \n",
    "        # Edge features\n",
    "        # 1. Same physical ring\n",
    "        same_pr = 1.0 if G.nodes[u_node].get('physicalringname') == G.nodes[v_node].get('physicalringname') else 0.0\n",
    "        \n",
    "        # 2. Same logical ring\n",
    "        same_lr = 1.0 if G.nodes[u_node].get('lrname') == G.nodes[v_node].get('lrname') else 0.0\n",
    "        \n",
    "        # 3. Is this a bridge?\n",
    "        is_bridge = 1.0 if (u_node, v_node) in bridges or (v_node, u_node) in bridges else 0.0\n",
    "        \n",
    "        # 4. Block connection\n",
    "        block_connection = 1.0 if (\"BLOCK\" in u_node) != (\"BLOCK\" in v_node) else 0.0\n",
    "        \n",
    "        # IMPORTANT: Use exactly 4 features for edge_attr\n",
    "        edge_attr.append([same_pr, same_lr, is_bridge, block_connection])\n",
    "    \n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    \n",
    "    # Convert elements_to_fail to list if it's a single element\n",
    "    if not isinstance(elements_to_fail, list):\n",
    "        elements_to_fail = [elements_to_fail]\n",
    "    \n",
    "    # Create edge mask for failure simulation\n",
    "    valid_edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "    \n",
    "    if failure_type == 'node':\n",
    "        # Mark all edges connected to failed nodes as invalid\n",
    "        for node in elements_to_fail:\n",
    "            node_idx = node_to_idx[node]\n",
    "            invalid_edges = (edge_index[0] == node_idx) | (edge_index[1] == node_idx)\n",
    "            valid_edge_mask[invalid_edges] = False\n",
    "    else:  # Edge failure\n",
    "        for failed_edge in elements_to_fail:\n",
    "            u_idx, v_idx = node_to_idx[failed_edge[0]], node_to_idx[failed_edge[1]]\n",
    "            \n",
    "            for i in range(edge_index.size(1)):\n",
    "                e_u, e_v = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "                if (e_u == u_idx and e_v == v_idx) or (e_u == v_idx and e_v == u_idx):\n",
    "                    valid_edge_mask[i] = False\n",
    "    \n",
    "    # Create PyG Data object for the model\n",
    "    data = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        valid_edge_mask=valid_edge_mask,\n",
    "        failure_type=torch.tensor([0 if failure_type == 'node' else 1]),\n",
    "        num_failures=torch.tensor([len(elements_to_fail)], dtype=torch.long)\n",
    "    )\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred_probs = torch.sigmoid(out.squeeze())\n",
    "        \n",
    "        # Handle different output shapes (batch vs single)\n",
    "        if len(pred_probs.shape) > 0:\n",
    "            pred_probs = pred_probs.numpy()\n",
    "        else:\n",
    "            pred_probs = pred_probs.item()\n",
    "    \n",
    "    # Find nodes predicted to be isolated\n",
    "    threshold = 0.7\n",
    "    predicted_isolated_indices = np.where(pred_probs > threshold)[0]\n",
    "    predicted_isolated_nodes = [node_list[idx] for idx in predicted_isolated_indices]\n",
    "    \n",
    "    return predicted_isolated_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NODE_2_2_1', 'NODE_2_2_2', 'NODE_2_2_3']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements_to_fail = ['NODE_2_2_0','NODE_2_2_4']\n",
    "actual_isolated_nodes = simulate_multiple_failures(G, elements_to_fail, failure_type='node')\n",
    "actual_isolated_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_isolated_nodes = predict_isolated_nodes(model, G, node_list, node_to_idx, 'node', elements_to_fail)\n",
    "predicted_isolated_nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(G, model, node_list, node_to_idx, failure_type, elements_to_fail):\n",
    "    \"\"\"\n",
    "    Evaluate how well the model predicts isolated nodes compared to actual simulation\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        model: Trained GNN model\n",
    "        node_list: List of all nodes\n",
    "        node_to_idx: Mapping from node names to indices\n",
    "        failure_type: 'node' or 'edge'\n",
    "        elements_to_fail: List of nodes or edges to fail\n",
    "        \n",
    "    Returns:\n",
    "        Dict with evaluation metrics\n",
    "    \"\"\"\n",
    "    # Get actual isolated nodes via simulation\n",
    "    if failure_type == 'node':\n",
    "        actual_isolated = simulate_multiple_failures(G, elements_to_fail, 'node')\n",
    "    else:\n",
    "        actual_isolated = simulate_multiple_failures(G, elements_to_fail, 'edge')\n",
    "    \n",
    "    # Get predicted isolated nodes from model\n",
    "    predicted_isolated = predict_isolated_nodes(model, G, node_list, node_to_idx, \n",
    "                                               failure_type, elements_to_fail)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    true_positives = len(set(actual_isolated) & set(predicted_isolated))\n",
    "    false_positives = len(set(predicted_isolated) - set(actual_isolated))\n",
    "    false_negatives = len(set(actual_isolated) - set(predicted_isolated))\n",
    "    \n",
    "    # Handle division by zero\n",
    "    if true_positives + false_positives > 0:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "    else:\n",
    "        precision = 1.0 if len(actual_isolated) == 0 else 0.0\n",
    "        \n",
    "    if true_positives + false_negatives > 0:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "    else:\n",
    "        recall = 1.0 if len(predicted_isolated) == 0 else 0.0\n",
    "    \n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    \n",
    "    accuracy = len(set(actual_isolated) & set(predicted_isolated)) / max(1, len(set(actual_isolated) | set(predicted_isolated)))\n",
    "    \n",
    "    # Print results\n",
    "    if failure_type == 'node':\n",
    "        print(f\"Node failure: {', '.join(elements_to_fail)}\")\n",
    "    else:\n",
    "        print(f\"Edge failure: {', '.join([f'({e[0]}-{e[1]})' for e in elements_to_fail])}\")\n",
    "    \n",
    "    print(f\"Actual isolated nodes: {', '.join(actual_isolated) if actual_isolated else 'None'}\")\n",
    "    print(f\"Predicted isolated nodes: {', '.join(predicted_isolated) if predicted_isolated else 'None'}\")\n",
    "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}, Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'actual_isolated': actual_isolated,\n",
    "        'predicted_isolated': predicted_isolated,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TESTING SINGLE NODE FAILURES ===\n",
      "Node failure: NODE_0_0_4\n",
      "Actual isolated nodes: None\n",
      "Predicted isolated nodes: None\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00, Accuracy: 0.00\n",
      "Node failure: NODE_2_2_3\n",
      "Actual isolated nodes: None\n",
      "Predicted isolated nodes: None\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00, Accuracy: 0.00\n",
      "Node failure: NODE_1_2_1\n",
      "Actual isolated nodes: None\n",
      "Predicted isolated nodes: None\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00, Accuracy: 0.00\n",
      "\n",
      "=== TESTING DOUBLE NODE FAILURES ===\n",
      "Node failure: NODE_0_0_1, NODE_0_0_2\n",
      "Actual isolated nodes: None\n",
      "Predicted isolated nodes: None\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00, Accuracy: 0.00\n",
      "Node failure: NODE_1_0_1, NODE_1_0_2\n",
      "Actual isolated nodes: None\n",
      "Predicted isolated nodes: None\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00, Accuracy: 0.00\n",
      "\n",
      "=== TESTING SINGLE EDGE FAILURES ===\n",
      "Edge failure: (NODE_0_1_3-NODE_0_1_4)\n",
      "Actual isolated nodes: None\n",
      "Predicted isolated nodes: None\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00, Accuracy: 0.00\n",
      "Edge failure: (NODE_1_0_0-BLOCK_1)\n",
      "Actual isolated nodes: None\n",
      "Predicted isolated nodes: None\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00, Accuracy: 0.00\n",
      "Edge failure: (NODE_1_0_1-NODE_1_0_2)\n",
      "Actual isolated nodes: None\n",
      "Predicted isolated nodes: None\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00, Accuracy: 0.00\n",
      "\n",
      "=== TESTING DOUBLE EDGE FAILURES ===\n",
      "Edge failure: (NODE_0_0_0-NODE_0_0_1), (NODE_0_0_0-BLOCK_0)\n",
      "Actual isolated nodes: NODE_0_0_0\n",
      "Predicted isolated nodes: None\n",
      "Precision: 0.00, Recall: 0.00, F1: 0.00, Accuracy: 0.00\n",
      "Edge failure: (NODE_1_0_0-NODE_1_0_1), (NODE_1_0_0-BLOCK_1)\n",
      "Actual isolated nodes: NODE_1_0_0\n",
      "Predicted isolated nodes: None\n",
      "Precision: 0.00, Recall: 0.00, F1: 0.00, Accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Test single node failures\n",
    "print(\"\\n=== TESTING SINGLE NODE FAILURES ===\")\n",
    "for node in random.sample(node_list, 3):  # Test 3 random nodes\n",
    "    if \"BLOCK\" not in node:  # Skip block nodes\n",
    "        evaluate_prediction(G, model, node_list, node_to_idx, 'node', [node])\n",
    "\n",
    "# Test double node failures (more likely to cause isolations)\n",
    "print(\"\\n=== TESTING DOUBLE NODE FAILURES ===\")\n",
    "# Find nodes in the same logical ring\n",
    "for pr_idx in range(2):\n",
    "    for lr_idx in range(2):\n",
    "        ring_nodes = [n for n in G.nodes() \n",
    "                     if 'physicalringname' in G.nodes[n] and \n",
    "                        G.nodes[n]['physicalringname'] == f\"RING_{pr_idx}\" and\n",
    "                        'lrname' in G.nodes[n] and\n",
    "                        G.nodes[n]['lrname'] == f\"LR_{pr_idx}_{lr_idx}\" and\n",
    "                        \"BLOCK\" not in n]\n",
    "        \n",
    "        if len(ring_nodes) >= 3:\n",
    "            # Pick two adjacent nodes in the ring\n",
    "            nodes_to_fail = [ring_nodes[1], ring_nodes[2]]\n",
    "            evaluate_prediction(G, model, node_list, node_to_idx, 'node', nodes_to_fail)\n",
    "            break\n",
    "\n",
    "# Test single edge failures\n",
    "print(\"\\n=== TESTING SINGLE EDGE FAILURES ===\")\n",
    "for edge in random.sample(list(G.edges()), 3):\n",
    "    evaluate_prediction(G, model, node_list, node_to_idx, 'edge', [edge])\n",
    "\n",
    "# Test double edge failures\n",
    "print(\"\\n=== TESTING DOUBLE EDGE FAILURES ===\")\n",
    "# Find edges in the same logical ring\n",
    "edge_pairs = []\n",
    "for pr_idx in range(2):\n",
    "    for lr_idx in range(2):\n",
    "        ring_edges = [(u, v) for u, v in G.edges() \n",
    "                      if 'physicalringname' in G.nodes[u] and \n",
    "                         G.nodes[u]['physicalringname'] == f\"RING_{pr_idx}\" and\n",
    "                         'lrname' in G.nodes[u] and \n",
    "                         G.nodes[u]['lrname'] == f\"LR_{pr_idx}_{lr_idx}\"]\n",
    "        \n",
    "        if len(ring_edges) >= 2:\n",
    "            edges_to_fail = [ring_edges[0], ring_edges[1]]\n",
    "            evaluate_prediction(G, model, node_list, node_to_idx, 'edge', edges_to_fail)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'failed_edge_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m      3\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Users\\My PC\\Documents\\CDOT-NMS\\RCA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32me:\\Users\\My PC\\Documents\\CDOT-NMS\\RCA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32me:\\Users\\My PC\\Documents\\CDOT-NMS\\RCA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Users\\My PC\\Documents\\CDOT-NMS\\RCA\\.venv\\Lib\\site-packages\\torch_geometric\\loader\\dataloader.py:27\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     25\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[1;32me:\\Users\\My PC\\Documents\\CDOT-NMS\\RCA\\.venv\\Lib\\site-packages\\torch_geometric\\data\\batch.py:97\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[1;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_data_list\u001b[39m(\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m     exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32me:\\Users\\My PC\\Documents\\CDOT-NMS\\RCA\\.venv\\Lib\\site-packages\\torch_geometric\\data\\collate.py:95\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m exclude_keys:  \u001b[38;5;66;03m# Do not include top-level attribute.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstores\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# The `num_nodes` attribute needs special treatment, as we need to\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# sum their values up instead of merging them to a list:\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32me:\\Users\\My PC\\Documents\\CDOT-NMS\\RCA\\.venv\\Lib\\site-packages\\torch_geometric\\data\\collate.py:95\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m exclude_keys:  \u001b[38;5;66;03m# Do not include top-level attribute.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m values \u001b[38;5;241m=\u001b[39m [\u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m stores]\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# The `num_nodes` attribute needs special treatment, as we need to\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# sum their values up instead of merging them to a list:\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32me:\\Users\\My PC\\Documents\\CDOT-NMS\\RCA\\.venv\\Lib\\site-packages\\torch_geometric\\data\\storage.py:118\u001b[0m, in \u001b[0;36mBaseStorage.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'failed_edge_mask'"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = criterion(out.squeeze(), batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                out = model(batch)\n",
    "                val_loss += criterion(out.squeeze(), batch.y).item()\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gnn_dataset(topology_df, num_simulations=100):\n",
    "    \"\"\"\n",
    "    Create a dataset for training a GNN to predict isolated nodes\n",
    "    \n",
    "    Returns:\n",
    "        - List of PyTorch Geometric Data objects\n",
    "    \"\"\"\n",
    "    # Build the graph\n",
    "    G = build_network_graph(topology_df)\n",
    "    \n",
    "    # Node mapping (for creating numerical indices)\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # Create node features\n",
    "    node_features = []\n",
    "    for node in node_list:\n",
    "        # Feature 1: One-hot encoded physical ring\n",
    "        pr = G.nodes[node]['physicalringname']\n",
    "        pr_hash = hash(pr) % 10  # Simple encoding\n",
    "        \n",
    "        # Feature 2: One-hot encoded logical ring\n",
    "        lr = G.nodes[node]['lrname']\n",
    "        lr_hash = hash(lr) % 10  # Simple encoding\n",
    "        \n",
    "        # Feature 3: Is it a block node?\n",
    "        is_block = 1.0 if node == G.nodes[node]['block_name'] else 0.0\n",
    "        \n",
    "        # Feature 4-5: Degree centrality and clustering coefficient\n",
    "        degree = G.degree(node) / len(G)\n",
    "        clustering = nx.clustering(G, node)\n",
    "        \n",
    "        node_features.append([pr_hash/10.0, lr_hash/10.0, is_block, degree, clustering])\n",
    "    \n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Create edge index for PyG\n",
    "    edges = []\n",
    "    for u, v in G.edges():\n",
    "        edges.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edges.append([node_to_idx[v], node_to_idx[u]])  # Add reverse edge for undirected graph\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Create dataset entries\n",
    "    data_list = []\n",
    "    \n",
    "    # Simulate node failures\n",
    "    for _ in range(num_simulations // 2):  # Half node failures, half edge failures\n",
    "        # Randomly select a node to fail\n",
    "        node_to_fail = np.random.choice(node_list)\n",
    "        \n",
    "        # Find isolated nodes\n",
    "        isolated_nodes = simulate_node_failure(G, node_to_fail)\n",
    "        \n",
    "        # Create target tensor (1 for isolated nodes, 0 for others)\n",
    "        y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "        for node in isolated_nodes:\n",
    "            y[node_to_idx[node]] = 1.0\n",
    "        \n",
    "        # Create node failure mask\n",
    "        node_mask = torch.zeros(len(node_list), dtype=torch.bool)\n",
    "        node_mask[node_to_idx[node_to_fail]] = True\n",
    "        \n",
    "        # Create PyG Data object\n",
    "        data = Data(\n",
    "            x=node_features,\n",
    "            edge_index=edge_index,\n",
    "            y=y,\n",
    "            failed_node_mask=node_mask,\n",
    "            failed_edge_mask=None\n",
    "        )\n",
    "        \n",
    "        data_list.append(data)\n",
    "    \n",
    "    # Simulate edge failures\n",
    "    edge_list = list(G.edges())\n",
    "    for _ in range(num_simulations // 2):\n",
    "        # Randomly select an edge to fail\n",
    "        edge_to_fail = edge_list[np.random.randint(0, len(edge_list))]\n",
    "        \n",
    "        # Find isolated nodes\n",
    "        isolated_nodes = simulate_edge_failure(G, edge_to_fail)\n",
    "        \n",
    "        # Create target tensor\n",
    "        y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "        for node in isolated_nodes:\n",
    "            y[node_to_idx[node]] = 1.0\n",
    "        \n",
    "        # Create edge failure mask\n",
    "        edge_mask = torch.zeros(edge_index.size(1), dtype=torch.bool)\n",
    "        u_idx, v_idx = node_to_idx[edge_to_fail[0]], node_to_idx[edge_to_fail[1]]\n",
    "        \n",
    "        for i in range(edge_index.size(1)):\n",
    "            e_u, e_v = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "            if (e_u == u_idx and e_v == v_idx) or (e_u == v_idx and e_v == u_idx):\n",
    "                edge_mask[i] = True\n",
    "        \n",
    "        # Create PyG Data object\n",
    "        data = Data(\n",
    "            x=node_features,\n",
    "            edge_index=edge_index,\n",
    "            y=y,\n",
    "            failed_node_mask=None,\n",
    "            failed_edge_mask=edge_mask\n",
    "        )\n",
    "        \n",
    "        data_list.append(data)\n",
    "    \n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def load_topology_data(config):\n",
    "    \"\"\"Load topology data from MySQL database\"\"\"\n",
    "    connection = mysql.connector.connect(**config)\n",
    "    cursor = connection.cursor(dictionary=True)\n",
    "    \n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "           aendname, \n",
    "           bendname, \n",
    "           aendip, \n",
    "           bendip, \n",
    "           aendifIndex,\n",
    "           bendifIndex,\n",
    "           block_name, \n",
    "           physicalringname, \n",
    "           lrname \n",
    "        FROM topology_data_logical\n",
    "    \"\"\"\n",
    "    cursor.execute(query)\n",
    "    rows = cursor.fetchall()\n",
    "    connection.close()\n",
    "    \n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_position_based_dataset(topology_df, num_simulations=5000):\n",
    "    \"\"\"\n",
    "    Create a dataset focused only on sequential position, logical ring ID and physical ring ID\n",
    "    \n",
    "    Args:\n",
    "        topology_df: DataFrame with topology data\n",
    "        num_simulations: Number of simulations to generate\n",
    "        \n",
    "    Returns:\n",
    "        data_list: List of PyG Data objects for training\n",
    "        node_list: List of all node names in order\n",
    "        node_to_idx: Dictionary mapping node names to indices\n",
    "    \"\"\"\n",
    "    print(\"Building network graph...\")\n",
    "    G = build_network_graph(topology_df)\n",
    "    \n",
    "    # Node mapping\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # Extract physical ring, logical ring, and position information from node names\n",
    "    node_features = []\n",
    "    ring_groups = {}  # Group nodes by ring\n",
    "    \n",
    "    for node in node_list:\n",
    "        # Parse node information from name (NODE_PR_LR_POS) or get from attributes\n",
    "        if \"NODE_\" in node:\n",
    "            parts = node.split(\"_\")\n",
    "            if len(parts) >= 4:  # Has the pattern NODE_PR_LR_POS\n",
    "                pr_id = int(parts[1])\n",
    "                lr_id = int(parts[2])\n",
    "                pos = int(parts[3])\n",
    "                is_block = 0.0\n",
    "            else:\n",
    "                # Unknown format, use defaults\n",
    "                pr_id = 0\n",
    "                lr_id = 0\n",
    "                pos = 0\n",
    "                is_block = 0.0\n",
    "        elif \"BLOCK_\" in node:\n",
    "            # Block nodes\n",
    "            parts = node.split(\"_\")\n",
    "            pr_id = int(parts[1]) if len(parts) > 1 else 0\n",
    "            lr_id = 0  # Blocks aren't part of a specific logical ring\n",
    "            pos = -1  # Special position for blocks\n",
    "            is_block = 1.0\n",
    "        else:\n",
    "            # Default values if pattern doesn't match\n",
    "            pr_id = 0\n",
    "            lr_id = 0\n",
    "            pos = 0\n",
    "            is_block = 0.0\n",
    "        \n",
    "        # Normalize features\n",
    "        pr_norm = pr_id / 10.0  # Assuming fewer than 10 physical rings\n",
    "        lr_norm = lr_id / 10.0  # Assuming fewer than 10 logical rings per physical ring\n",
    "        pos_norm = pos / 10.0   # Assuming fewer than 10 positions per logical ring\n",
    "        \n",
    "        # Create feature vector [physical_ring_id, logical_ring_id, position, is_block]\n",
    "        features = [pr_norm, lr_norm, pos_norm, is_block]\n",
    "        node_features.append(features)\n",
    "        \n",
    "        # Group nodes by physical ring and logical ring for isolation logic\n",
    "        if not is_block:\n",
    "            key = (pr_id, lr_id)\n",
    "            if key not in ring_groups:\n",
    "                ring_groups[key] = []\n",
    "            ring_groups[key].append((node, pos))\n",
    "    \n",
    "    # Convert to tensor\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Create edge index\n",
    "    edges = []\n",
    "    for u, v in G.edges():\n",
    "        edges.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edges.append([node_to_idx[v], node_to_idx[u]])  # Add reverse edge for undirected graph\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Sort nodes within each ring group by position for sequential isolation logic\n",
    "    for key in ring_groups:\n",
    "        ring_groups[key].sort(key=lambda x: x[1])  # Sort by position\n",
    "    \n",
    "    print(\"Generating dataset examples...\")\n",
    "    data_list = []\n",
    "    examples_with_isolations = 0\n",
    "    \n",
    "    for i in range(num_simulations):\n",
    "        # Choose a random ring that has enough nodes\n",
    "        valid_rings = [key for key, nodes in ring_groups.items() if len(nodes) >= 4]\n",
    "        if not valid_rings:\n",
    "            continue\n",
    "            \n",
    "        ring_key = random.choice(valid_rings)\n",
    "        ring_nodes = ring_groups[ring_key]\n",
    "        \n",
    "        # Choose two nodes to fail with positions that have nodes between them\n",
    "        positions = sorted([node[1] for node in ring_nodes])\n",
    "        if len(positions) < 3:\n",
    "            continue\n",
    "            \n",
    "        # Choose two positions with at least one node between them\n",
    "        pos1_idx = random.randint(0, len(positions) - 3)\n",
    "        pos2_idx = random.randint(pos1_idx + 2, len(positions) - 1)\n",
    "        \n",
    "        pos1 = positions[pos1_idx]\n",
    "        pos2 = positions[pos2_idx]\n",
    "        \n",
    "        # Find the corresponding nodes\n",
    "        node1 = next(node for node, pos in ring_nodes if pos == pos1)\n",
    "        node2 = next(node for node, pos in ring_nodes if pos == pos2)\n",
    "        \n",
    "        # Find nodes that should be isolated (positions between pos1 and pos2)\n",
    "        isolated_nodes = [\n",
    "            node for node, pos in ring_nodes \n",
    "            if pos > pos1 and pos < pos2  # Position is between the two failed nodes\n",
    "        ]\n",
    "        \n",
    "        # Create target tensor\n",
    "        y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "        for node in isolated_nodes:\n",
    "            y[node_to_idx[node]] = 1.0\n",
    "        \n",
    "        # Create node failure mask\n",
    "        node_mask = torch.zeros(len(node_list), dtype=torch.bool)\n",
    "        node_mask[node_to_idx[node1]] = True\n",
    "        node_mask[node_to_idx[node2]] = True\n",
    "        \n",
    "        # Create PyG Data object\n",
    "        data = Data(\n",
    "            x=node_features,\n",
    "            edge_index=edge_index,\n",
    "            y=y,\n",
    "            failed_nodes=torch.tensor([node_to_idx[node1], node_to_idx[node2]]),\n",
    "            failure_positions=torch.tensor([pos1, pos2]),\n",
    "            ring_key=torch.tensor(list(ring_key))\n",
    "        )\n",
    "        \n",
    "        data_list.append(data)\n",
    "        \n",
    "        if len(isolated_nodes) > 0:\n",
    "            examples_with_isolations += 1\n",
    "            \n",
    "        # Print progress\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"Generated {i+1}/{num_simulations} examples, {examples_with_isolations} with isolations\")\n",
    "    \n",
    "    print(f\"Training set: {len(data_list)} examples, {examples_with_isolations} with isolations ({examples_with_isolations/len(data_list)*100:.2f}%)\")\n",
    "    \n",
    "    return data_list, node_list, node_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "class PositionalIsolationModel(nn.Module):\n",
    "    def __init__(self, num_features=4):\n",
    "        super(PositionalIsolationModel, self).__init__()\n",
    "        # Simple architecture with position awareness\n",
    "        self.conv1 = GCNConv(num_features, 32)\n",
    "        \n",
    "        # Position-aware layers\n",
    "        self.position_encoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        \n",
    "        # Final prediction layer\n",
    "        self.predictor = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index, failed_nodes=None):\n",
    "        # Basic feature extraction\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Position encoding\n",
    "        x = self.position_encoder(x)\n",
    "        \n",
    "        # Final prediction\n",
    "        x = self.predictor(x)\n",
    "        \n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_position_model(data_list, epochs=50):\n",
    "    # Split data\n",
    "    train_size = int(0.8 * len(data_list))\n",
    "    train_data = data_list[:train_size]\n",
    "    val_data = data_list[train_size:]\n",
    "    \n",
    "    # Create loaders\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=32)\n",
    "    \n",
    "    # Create model\n",
    "    model = PositionalIsolationModel()\n",
    "    \n",
    "    # Use weighted loss since most nodes aren't isolated\n",
    "    pos_weight = torch.tensor([10.0])\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index, batch.failed_nodes)\n",
    "            loss = criterion(out.squeeze(), batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                out = model(batch.x, batch.edge_index, batch.failed_nodes)\n",
    "                pred = (out.squeeze() > 0.5).float()\n",
    "                val_preds.append(pred)\n",
    "                val_targets.append(batch.y)\n",
    "        \n",
    "        val_preds = torch.cat(val_preds)\n",
    "        val_targets = torch.cat(val_targets)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = ((val_preds == 1) & (val_targets == 1)).sum().float() / (val_preds.sum() + 1e-6)\n",
    "        recall = ((val_preds == 1) & (val_targets == 1)).sum().float() / (val_targets.sum() + 1e-6)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "        accuracy = (val_preds == val_targets).float().mean()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Loss: {total_loss/len(train_loader):.4f}, \"\n",
    "              f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Acc: {accuracy:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network graph...\n",
      "Generating dataset examples...\n",
      "Generated 500/5000 examples, 500 with isolations\n",
      "Generated 1000/5000 examples, 1000 with isolations\n",
      "Generated 1500/5000 examples, 1500 with isolations\n",
      "Generated 2000/5000 examples, 2000 with isolations\n",
      "Generated 2500/5000 examples, 2500 with isolations\n",
      "Generated 3000/5000 examples, 3000 with isolations\n",
      "Generated 3500/5000 examples, 3500 with isolations\n",
      "Generated 4000/5000 examples, 4000 with isolations\n",
      "Generated 4500/5000 examples, 4500 with isolations\n",
      "Generated 5000/5000 examples, 5000 with isolations\n",
      "Training set: 5000 examples, 5000 with isolations (100.00%)\n"
     ]
    }
   ],
   "source": [
    "data_list,_,_ = create_position_based_dataset(topo_data, num_simulations=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Loss: 0.8259, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9913\n",
      "Epoch 2/100: Loss: 0.6949, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9913\n",
      "Epoch 3/100: Loss: 0.6933, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9913\n",
      "Epoch 4/100: Loss: 0.6932, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9913\n",
      "Epoch 5/100: Loss: 0.6932, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9913\n",
      "Epoch 6/100: Loss: 0.6932, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9913\n",
      "Epoch 7/100: Loss: 0.6932, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9913\n",
      "Epoch 8/100: Loss: 0.6932, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9913\n",
      "Epoch 9/100: Loss: 0.6932, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9913\n",
      "Epoch 10/100: Loss: 0.6932, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9913\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_position_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_position_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mtrain_position_model\u001b[39m\u001b[34m(data_list, epochs, lr)\u001b[39m\n\u001b[32m     76\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Load best model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbest_position_model.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/serialization.py:1425\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1423\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1428\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1429\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1430\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/serialization.py:751\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    750\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/serialization.py:732\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'best_position_model.pt'"
     ]
    }
   ],
   "source": [
    "train_position_model(data_list, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class DirectPositionalModel(nn.Module):\n",
    "    def __init__(self, hidden_size=64):\n",
    "        super(DirectPositionalModel, self).__init__()\n",
    "        # Simpler, more direct architecture\n",
    "        self.fc1 = nn.Linear(5, hidden_size)  # 5 features explained below\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # No graph structure, just direct positional reasoning\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def create_direct_dataset(topology_df, num_examples=1000):\n",
    "    \"\"\"Create dataset with explicit positional features\"\"\"\n",
    "    # Extract nodes with position information\n",
    "    nodes_by_ring = {}\n",
    "    \n",
    "    for _, row in topology_df.iterrows():\n",
    "        # Extract ring IDs and positions from node names\n",
    "        for node_col in ['aendname', 'bendname']:\n",
    "            node = row[node_col]\n",
    "            if not isinstance(node, str) or 'NODE_' not in node:\n",
    "                continue\n",
    "                \n",
    "            parts = node.split('_')\n",
    "            if len(parts) >= 4:\n",
    "                try:\n",
    "                    pr_id = int(parts[1])\n",
    "                    lr_id = int(parts[2]) \n",
    "                    pos = int(parts[3])\n",
    "                    \n",
    "                    key = (pr_id, lr_id)\n",
    "                    if key not in nodes_by_ring:\n",
    "                        nodes_by_ring[key] = []\n",
    "                    \n",
    "                    nodes_by_ring[key].append((node, pos))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    \n",
    "    # Create dataset\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        # Pick a ring with enough nodes\n",
    "        valid_rings = [ring for ring, nodes in nodes_by_ring.items() if len(nodes) >= 4]\n",
    "        if not valid_rings:\n",
    "            continue\n",
    "            \n",
    "        ring = random.choice(valid_rings)\n",
    "        nodes = nodes_by_ring[ring]\n",
    "        \n",
    "        # Sort nodes by position\n",
    "        nodes.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # Pick two positions to fail with at least one node between them\n",
    "        idx1 = random.randint(0, len(nodes) - 3)\n",
    "        idx2 = random.randint(idx1 + 2, len(nodes) - 1)\n",
    "        \n",
    "        fail_node1, pos1 = nodes[idx1]\n",
    "        fail_node2, pos2 = nodes[idx2]\n",
    "        \n",
    "        # Create features for all nodes in this ring\n",
    "        for node, pos in nodes:\n",
    "            # Key features:\n",
    "            # 1. Is this node between the two failed positions?\n",
    "            is_between = 1.0 if pos1 < pos < pos2 else 0.0\n",
    "            \n",
    "            # 2. Normalized position within the ring\n",
    "            norm_pos = pos / 100.0  # Scale position\n",
    "            \n",
    "            # 3. Normalized distance from failed nodes\n",
    "            dist_to_fail1 = abs(pos - pos1) / 100.0\n",
    "            dist_to_fail2 = abs(pos - pos2) / 100.0\n",
    "            \n",
    "            # 4. Is this one of the failed nodes?\n",
    "            is_failed = 1.0 if node in [fail_node1, fail_node2] else 0.0\n",
    "            \n",
    "            # Feature vector\n",
    "            features = [norm_pos, dist_to_fail1, dist_to_fail2, is_failed, is_between]\n",
    "            X.append(features)\n",
    "            \n",
    "            # Target: 1 if node should be isolated\n",
    "            Y.append(1.0 if is_between else 0.0)\n",
    "    \n",
    "    return torch.tensor(X, dtype=torch.float), torch.tensor(Y, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "def train_direct_model(X, Y, test_split=0.2):\n",
    "    \"\"\"Train model with direct positional features\"\"\"\n",
    "    # Split data\n",
    "    indices = torch.randperm(len(X))\n",
    "    test_size = int(len(X) * test_split)\n",
    "    train_indices = indices[:-test_size]\n",
    "    test_indices = indices[-test_size:]\n",
    "    \n",
    "    X_train, Y_train = X[train_indices], Y[train_indices]\n",
    "    X_test, Y_test = X[test_indices], Y[test_indices]\n",
    "    \n",
    "    # Create model\n",
    "    model = DirectPositionalModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Balance loss with high weight for positive class\n",
    "    pos_weight = (len(Y_train) - Y_train.sum()) / Y_train.sum() if Y_train.sum() > 0 else 10.0\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]))\n",
    "    \n",
    "    # Train\n",
    "    batch_size = 64\n",
    "    epochs = 30\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Training batches\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            batch_X = X_train[i:i+batch_size]\n",
    "            batch_Y = Y_train[i:i+batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test)\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \n",
    "            # Metrics\n",
    "            correct = (predictions == Y_test).sum().item()\n",
    "            accuracy = correct / len(Y_test)\n",
    "            \n",
    "            tp = ((predictions == 1) & (Y_test == 1)).sum().item()\n",
    "            fp = ((predictions == 1) & (Y_test == 0)).sum().item()\n",
    "            fn = ((predictions == 0) & (Y_test == 1)).sum().item()\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Loss: {total_loss/len(X_train):.4f}, \"\n",
    "              f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Acc: {accuracy:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 100000 examples\n",
      "Isolated nodes: 21334.0 (21.33%)\n",
      "Epoch 1/30: Loss: 0.0010, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 2/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 3/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 4/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 5/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 6/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 7/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 8/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 9/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 10/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 11/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 12/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 13/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 14/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 15/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 16/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 17/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 18/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 19/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 20/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 21/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 22/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 23/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 24/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 25/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 26/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 27/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 28/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 29/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n",
      "Epoch 30/30: Loss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, Acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Load your topology data\n",
    "\n",
    "\n",
    "# Create the simplified dataset\n",
    "X, Y = create_direct_dataset(topo_data, num_examples=5000)\n",
    "\n",
    "# See the class distribution\n",
    "print(f\"Dataset created with {len(X)} examples\")\n",
    "print(f\"Isolated nodes: {Y.sum().item()} ({Y.sum().item()/len(Y)*100:.2f}%)\")\n",
    "\n",
    "# Train the model\n",
    "model = train_direct_model(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ring_isolations(model, topology_df, failed_node1, failed_node2):\n",
    "    \"\"\"\n",
    "    Predict isolations for all nodes in a ring when two specific nodes fail\n",
    "    \n",
    "    Args:\n",
    "        model: Trained DirectPositionalModel\n",
    "        topology_df: DataFrame with topology data\n",
    "        failed_node1, failed_node2: The two nodes that will fail\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of {node_name: isolation_probability}\n",
    "    \"\"\"\n",
    "    # Get node position info\n",
    "    def get_node_info(node):\n",
    "        if not isinstance(node, str) or 'NODE_' not in node:\n",
    "            return None\n",
    "        parts = node.split('_')\n",
    "        if len(parts) < 4:\n",
    "            return None\n",
    "        try:\n",
    "            return (int(parts[1]), int(parts[2]), int(parts[3]))\n",
    "        except ValueError:\n",
    "            return None\n",
    "    \n",
    "    # Get failed nodes' info\n",
    "    node1_info = get_node_info(failed_node1)\n",
    "    node2_info = get_node_info(failed_node2)\n",
    "    \n",
    "    if not node1_info or not node2_info:\n",
    "        print(\"Invalid node names\")\n",
    "        return {}\n",
    "    \n",
    "    # Get ring ID and positions\n",
    "    pr_id, lr_id = node1_info[0], node1_info[1]\n",
    "    pos1, pos2 = node1_info[2], node2_info[2]\n",
    "    \n",
    "    # Ensure pos1 < pos2\n",
    "    if pos1 > pos2:\n",
    "        pos1, pos2 = pos2, pos1\n",
    "        \n",
    "    # Collect unique nodes in the ring\n",
    "    ring_nodes = set()\n",
    "    for _, row in topology_df.iterrows():\n",
    "        for col in ['aendname', 'bendname']:\n",
    "            node = row[col]\n",
    "            info = get_node_info(node)\n",
    "            if info and info[0] == pr_id and info[1] == lr_id:\n",
    "                ring_nodes.add(node)\n",
    "    \n",
    "    # Generate features for all nodes\n",
    "    features = []\n",
    "    node_names = []\n",
    "    \n",
    "    for node in ring_nodes:\n",
    "        info = get_node_info(node)\n",
    "        if not info:\n",
    "            continue\n",
    "            \n",
    "        pos = info[2]\n",
    "        \n",
    "        # Generate feature vector\n",
    "        norm_pos = pos / 100.0\n",
    "        dist_to_fail1 = abs(pos - pos1) / 100.0\n",
    "        dist_to_fail2 = abs(pos - pos2) / 100.0\n",
    "        is_failed = 1.0 if node in [failed_node1, failed_node2] else 0.0\n",
    "        is_between = 1.0 if pos1 < pos < pos2 else 0.0\n",
    "        \n",
    "        features.append([norm_pos, dist_to_fail1, dist_to_fail2, is_failed, is_between])\n",
    "        node_names.append(node)\n",
    "    \n",
    "    # Make predictions for all nodes\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X = torch.tensor(features, dtype=torch.float)\n",
    "        outputs = model(X)\n",
    "        probs = torch.sigmoid(outputs).squeeze().tolist()\n",
    "    \n",
    "    # Return results as dictionary\n",
    "    results = {}\n",
    "    for i, node in enumerate(node_names):\n",
    "        # Skip failed nodes themselves\n",
    "        if node in [failed_node1, failed_node2]:\n",
    "            continue\n",
    "        results[node] = probs[i]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted isolations:\n",
      "  NODE_1_1_8: 1.0000 - ISOLATED\n",
      "  NODE_1_1_7: 1.0000 - ISOLATED\n",
      "  NODE_1_1_5: 0.0000\n",
      "  NODE_1_1_4: 0.0000\n",
      "  NODE_1_1_3: 0.0000\n",
      "  NODE_1_1_2: 0.0000\n",
      "  NODE_1_1_1: 0.0000\n",
      "  NODE_1_1_0: 0.0000\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_ring_isolations(model, topo_data, \"NODE_1_1_6\", \"NODE_1_1_9\")\n",
    "\n",
    "print(\"Predicted isolations:\")\n",
    "for node, prob in sorted(predictions.items(), key=lambda x: x[1], reverse=True):\n",
    "    if prob > 0.5:\n",
    "        print(f\"  {node}: {prob:.4f} - ISOLATED\")\n",
    "    else:\n",
    "        print(f\"  {node}: {prob:.4f}\")\n",
    "\n",
    "# Verify against ground truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Build network graph from topology data\n",
    "def build_network_graph(topology_df):\n",
    "    \"\"\"Convert topology DataFrame to NetworkX graph with node attributes\"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add edges from topology data\n",
    "    for _, row in topology_df.iterrows():\n",
    "        aend = row['aendname']\n",
    "        bend = row['bendname']\n",
    "        \n",
    "        # Skip if missing node names\n",
    "        if pd.isna(aend) or pd.isna(bend):\n",
    "            continue\n",
    "            \n",
    "        # Extract attributes from node names (NODE_PR_LR_POS)\n",
    "        aend_attrs = {}\n",
    "        bend_attrs = {}\n",
    "        \n",
    "        if isinstance(aend, str) and 'NODE_' in aend:\n",
    "            parts = aend.split('_')\n",
    "            if len(parts) >= 4:\n",
    "                aend_attrs['pr_id'] = int(parts[1])\n",
    "                aend_attrs['lr_id'] = int(parts[2])\n",
    "                aend_attrs['position'] = int(parts[3])\n",
    "                aend_attrs['is_block'] = False\n",
    "                \n",
    "        if isinstance(bend, str) and 'NODE_' in bend:\n",
    "            parts = bend.split('_')\n",
    "            if len(parts) >= 4:\n",
    "                bend_attrs['pr_id'] = int(parts[1])\n",
    "                bend_attrs['lr_id'] = int(parts[2])\n",
    "                bend_attrs['position'] = int(parts[3])\n",
    "                bend_attrs['is_block'] = False\n",
    "        \n",
    "        # Add nodes with attributes\n",
    "        G.add_node(aend, **(aend_attrs or {}))\n",
    "        G.add_node(bend, **(bend_attrs or {}))\n",
    "        \n",
    "        # Add edge with any additional attributes you want\n",
    "        G.add_edge(aend, bend)\n",
    "    \n",
    "    return G\n",
    "\n",
    "# GNN model for isolation prediction\n",
    "class IsolationGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64):\n",
    "        super(IsolationGNN, self).__init__()\n",
    "        # First layer: standard graph convolution\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        \n",
    "        # Second layer: attention for understanding important connections\n",
    "        self.conv2 = GATConv(hidden_dim, hidden_dim, heads=2, dropout=0.2)\n",
    "        \n",
    "        # Third layer: combine and refine\n",
    "        self.conv3 = GCNConv(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        # Final prediction layer\n",
    "        self.classifier = torch.nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index, failed_nodes=None):\n",
    "        # First convolution\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Attention layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Final convolution\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Prediction\n",
    "        x = self.classifier(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Create dataset for GNN training\n",
    "def create_isolation_dataset(G, num_examples=5000):\n",
    "    \"\"\"Generate training examples by simulating node failures\"\"\"\n",
    "    # Create node mappings\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # Create node features\n",
    "    num_nodes = len(node_list)\n",
    "    node_features = torch.zeros((num_nodes, 6), dtype=torch.float)\n",
    "    \n",
    "    # Prepare node features\n",
    "    for i, node in enumerate(node_list):\n",
    "        attrs = G.nodes[node]\n",
    "        # PR ID\n",
    "        node_features[i, 0] = attrs.get('pr_id', 0) / 10.0\n",
    "        # LR ID\n",
    "        node_features[i, 1] = attrs.get('lr_id', 0) / 10.0\n",
    "        # Position\n",
    "        node_features[i, 2] = attrs.get('position', 0) / 100.0\n",
    "        # Is block node\n",
    "        node_features[i, 3] = 1.0 if attrs.get('is_block', False) else 0.0\n",
    "        # Node degree (normalized)\n",
    "        node_features[i, 4] = G.degree(node) / 5.0\n",
    "        # Status (0=normal, 1=failed)\n",
    "        node_features[i, 5] = 0.0  # Will be set to 1.0 for failed nodes\n",
    "    \n",
    "    # Create edge index\n",
    "    edges = []\n",
    "    for u, v in G.edges():\n",
    "        edges.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edges.append([node_to_idx[v], node_to_idx[u]])  # Undirected graph\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Group nodes by ring for failure simulation\n",
    "    ring_groups = {}\n",
    "    for node in G.nodes():\n",
    "        attrs = G.nodes[node]\n",
    "        if 'pr_id' in attrs and 'lr_id' in attrs:\n",
    "            key = (attrs['pr_id'], attrs['lr_id'])\n",
    "            if key not in ring_groups:\n",
    "                ring_groups[key] = []\n",
    "            ring_groups[key].append(node)\n",
    "    \n",
    "    # Sort nodes by position within each ring\n",
    "    for key in ring_groups:\n",
    "        ring_groups[key].sort(key=lambda n: G.nodes[n].get('position', 0))\n",
    "    \n",
    "    # Generate examples\n",
    "    data_list = []\n",
    "    examples_with_isolations = 0\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        # Choose a ring with enough nodes\n",
    "        valid_rings = [k for k, v in ring_groups.items() if len(v) >= 4]\n",
    "        if not valid_rings:\n",
    "            continue\n",
    "            \n",
    "        ring_key = random.choice(valid_rings)\n",
    "        ring_nodes = ring_groups[ring_key]\n",
    "        \n",
    "        # Choose two positions with nodes between them\n",
    "        idx1 = random.randint(0, len(ring_nodes) - 3)\n",
    "        idx2 = random.randint(idx1 + 2, len(ring_nodes) - 1)\n",
    "        \n",
    "        node1 = ring_nodes[idx1]\n",
    "        node2 = ring_nodes[idx2]\n",
    "        \n",
    "        # Get positions\n",
    "        pos1 = G.nodes[node1].get('position', 0)\n",
    "        pos2 = G.nodes[node2].get('position', 0)\n",
    "        \n",
    "        # Find isolated nodes (positions between pos1 and pos2)\n",
    "        isolated_nodes = []\n",
    "        for node in ring_nodes:\n",
    "            if node == node1 or node == node2:\n",
    "                continue\n",
    "                \n",
    "            pos = G.nodes[node].get('position', 0)\n",
    "            if pos1 < pos < pos2:\n",
    "                isolated_nodes.append(node)\n",
    "        \n",
    "        # Create feature matrix for this example (copy the base features)\n",
    "        x = node_features.clone()\n",
    "        \n",
    "        # Mark failed nodes\n",
    "        x[node_to_idx[node1], 5] = 1.0\n",
    "        x[node_to_idx[node2], 5] = 1.0\n",
    "        \n",
    "        # Create target tensor\n",
    "        y = torch.zeros(num_nodes, dtype=torch.float)\n",
    "        for node in isolated_nodes:\n",
    "            y[node_to_idx[node]] = 1.0\n",
    "        \n",
    "        # Create PyG Data object\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            y=y,\n",
    "            failed_nodes=torch.tensor([node_to_idx[node1], node_to_idx[node2]], dtype=torch.long)\n",
    "        )\n",
    "        \n",
    "        data_list.append(data)\n",
    "        \n",
    "        if isolated_nodes:\n",
    "            examples_with_isolations += 1\n",
    "    \n",
    "    print(f\"Dataset: {len(data_list)} examples, {examples_with_isolations} with isolations ({examples_with_isolations/len(data_list)*100:.2f}%)\")\n",
    "    return data_list, node_list, node_to_idx\n",
    "\n",
    "# Training function\n",
    "def train_isolation_gnn(data_list, epochs=50, lr=0.001):\n",
    "    \"\"\"Train GNN model for isolation prediction\"\"\"\n",
    "    # Split data into training and validation\n",
    "    random.shuffle(data_list)\n",
    "    split = int(0.8 * len(data_list))\n",
    "    train_data = data_list[:split]\n",
    "    val_data = data_list[split:]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=32)\n",
    "    \n",
    "    # Create model\n",
    "    num_features = train_data[0].x.shape[1]\n",
    "    model = IsolationGNN(num_features)\n",
    "    \n",
    "    # Optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    best_f1 = 0\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "            loss = criterion(out.squeeze(), batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                out = model(batch.x, batch.edge_index)\n",
    "                pred = (out.squeeze() > 0.5).float()\n",
    "                val_preds.append(pred)\n",
    "                val_targets.append(batch.y)\n",
    "        \n",
    "        val_preds = torch.cat(val_preds)\n",
    "        val_targets = torch.cat(val_targets)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tp = ((val_preds == 1) & (val_targets == 1)).sum().item()\n",
    "        fp = ((val_preds == 1) & (val_targets == 0)).sum().item()\n",
    "        fn = ((val_preds == 0) & (val_targets == 1)).sum().item()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (val_preds == val_targets).float().mean().item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Loss: {total_loss/len(train_loader):.4f}, \"\n",
    "              f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Acc: {accuracy:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_isolation_gnn.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered. Best F1: {best_f1:.4f}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_isolation_gnn.pt'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 5000 examples, 5000 with isolations (100.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nms/.local/lib/python3.13/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Loss: 0.1496, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9914\n",
      "Epoch 2/100: Loss: 0.0305, Precision: 1.0000, Recall: 0.0518, F1: 0.0985, Acc: 0.9919\n",
      "Epoch 3/100: Loss: 0.0232, Precision: 0.4657, Recall: 0.1351, F1: 0.2095, Acc: 0.9912\n",
      "Epoch 4/100: Loss: 0.0226, Precision: 1.0000, Recall: 0.1351, F1: 0.2381, Acc: 0.9926\n",
      "Epoch 5/100: Loss: 0.0224, Precision: 1.0000, Recall: 0.0946, F1: 0.1728, Acc: 0.9922\n",
      "Epoch 6/100: Loss: 0.0220, Precision: 1.0000, Recall: 0.0248, F1: 0.0484, Acc: 0.9916\n",
      "Epoch 7/100: Loss: 0.0218, Precision: 1.0000, Recall: 0.0019, F1: 0.0037, Acc: 0.9914\n",
      "Epoch 8/100: Loss: 0.0216, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9914\n",
      "Epoch 9/100: Loss: 0.0215, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9914\n",
      "Epoch 10/100: Loss: 0.0213, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9914\n",
      "Epoch 11/100: Loss: 0.0212, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9914\n",
      "Epoch 12/100: Loss: 0.0210, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9914\n",
      "Epoch 13/100: Loss: 0.0210, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9914\n",
      "Epoch 14/100: Loss: 0.0209, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Acc: 0.9914\n",
      "Early stopping triggered. Best F1: 0.2381\n"
     ]
    }
   ],
   "source": [
    "G = build_network_graph(topo_data)\n",
    "data_list,_,_ = create_isolation_dataset(G, num_examples=5000)\n",
    "model = train_isolation_gnn(data_list, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ring_aware_loss(predictions, targets, node_features, failed_pr_id, failed_lr_id, penalty_weight=10.0):\n",
    "    \"\"\"\n",
    "    Custom loss that penalizes predicting isolation for nodes in different rings.\n",
    "    Handles batched data properly.\n",
    "    \"\"\"\n",
    "    # Fix shape mismatch by squeezing predictions to match targets\n",
    "    if predictions.dim() > targets.dim():\n",
    "        predictions_flat = predictions.squeeze()\n",
    "    else:\n",
    "        predictions_flat = predictions\n",
    "    \n",
    "    # Use binary_cross_entropy_with_logits for numerical stability\n",
    "    base_loss = F.binary_cross_entropy_with_logits(predictions_flat, targets)\n",
    "    \n",
    "    # Get batch size and nodes per batch\n",
    "    batch_size = failed_pr_id.size(0)\n",
    "    nodes_per_batch = node_features.size(0) // batch_size\n",
    "    \n",
    "    # Reshape node features for batch processing\n",
    "    batched_features = node_features.view(batch_size, nodes_per_batch, -1)\n",
    "    \n",
    "    # Extract ring IDs from batched features\n",
    "    pr_ids = batched_features[:, :, 1] * 10.0  # Multiply by 10.0 to reverse scaling\n",
    "    lr_ids = batched_features[:, :, 2] * 10.0  # Multiply by 10.0 to reverse scaling\n",
    "    \n",
    "    # Reshape failed IDs for broadcasting\n",
    "    failed_pr_id_expanded = failed_pr_id.unsqueeze(1)  # [batch_size, 1]\n",
    "    failed_lr_id_expanded = failed_lr_id.unsqueeze(1)  # [batch_size, 1]\n",
    "    \n",
    "    # Create mask for nodes in different rings (with proper broadcasting)\n",
    "    diff_ring_mask = ((pr_ids != failed_pr_id_expanded) | (lr_ids != failed_lr_id_expanded)).float()\n",
    "    \n",
    "    # Reshape predictions for batch processing\n",
    "    batched_preds = torch.sigmoid(predictions_flat).view(batch_size, nodes_per_batch)\n",
    "    \n",
    "    # Calculate penalty for each batch and sum\n",
    "    batch_penalties = torch.sum(batched_preds * diff_ring_mask, dim=1)\n",
    "    penalty = torch.mean(batch_penalties) * penalty_weight / nodes_per_batch\n",
    "    \n",
    "    # Combine base loss and penalty\n",
    "    total_loss = base_loss + penalty\n",
    "    \n",
    "    return total_loss, base_loss, penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class EnhancedGNN(nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(EnhancedGNN, self).__init__()\n",
    "        # First layer - extract neighborhood features\n",
    "        self.conv1 = GCNConv(num_node_features, 64)\n",
    "        \n",
    "        # Attention layer to focus on important connections\n",
    "        self.attention = GATConv(64, 64, heads=2, dropout=0.2)\n",
    "        \n",
    "        # Position-aware layer (crucial for your task)\n",
    "        self.position_encoder = nn.Sequential(\n",
    "            nn.Linear(64*2, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64)\n",
    "        )\n",
    "        \n",
    "        # Final prediction\n",
    "        self.classifier = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x, edge_index, failed_nodes_mask=None):\n",
    "        # Initial feature processing\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2,training=self.training)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        x = self.attention(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.position_encoder(x)\n",
    "\n",
    "        \n",
    "        # Final prediction\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def build_graph_with_position_features(topology_df):\n",
    "    \"\"\"Build NetworkX graph with enhanced position features\"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Track ring membership and positions\n",
    "    ring_positions = {}  # (pr_id, lr_id) -> list of positions\n",
    "    \n",
    "    # First pass: identify all rings and node positions\n",
    "    for _, row in topology_df.iterrows():\n",
    "        for node_col in ['aendname', 'bendname']:\n",
    "            node = row[node_col]\n",
    "            if not isinstance(node, str) or 'NODE_' not in node:\n",
    "                continue\n",
    "                \n",
    "            parts = node.split('_')\n",
    "            if len(parts) >= 4:\n",
    "                try:\n",
    "                    pr_id = int(parts[1])\n",
    "                    lr_id = int(parts[2])\n",
    "                    pos = int(parts[3])\n",
    "                    \n",
    "                    key = (pr_id, lr_id)\n",
    "                    if key not in ring_positions:\n",
    "                        ring_positions[key] = []\n",
    "                    \n",
    "                    if pos not in ring_positions[key]:\n",
    "                        ring_positions[key].append(pos)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    \n",
    "    # Sort positions within each ring\n",
    "    for key in ring_positions:\n",
    "        ring_positions[key].sort()\n",
    "    \n",
    "    # Add nodes and edges with position-aware features\n",
    "    for _, row in topology_df.iterrows():\n",
    "        aend = row['aendname']\n",
    "        bend = row['bendname']\n",
    "        \n",
    "        # Add nodes with enhanced features\n",
    "        for node in [aend, bend]:\n",
    "            if node in G:\n",
    "                continue  # Skip if already added\n",
    "                \n",
    "            # Default features\n",
    "            features = {\n",
    "                'is_block': 'BLOCK' in str(node),\n",
    "                'pr_id': -1,\n",
    "                'lr_id': -1,\n",
    "                'position': -1,\n",
    "  \n",
    "            }\n",
    "            \n",
    "            # Extract position information\n",
    "            if isinstance(node, str) and 'NODE_' in node:\n",
    "                parts = node.split('_')\n",
    "                if len(parts) >= 4:\n",
    "                    try:\n",
    "                        pr_id = int(parts[1])\n",
    "                        lr_id = int(parts[2])\n",
    "                        pos = int(parts[3])\n",
    "                        \n",
    "                        # Get normalized position (crucial for learning the pattern)\n",
    "\n",
    "\n",
    "                        \n",
    "                        features.update({\n",
    "                            'pr_id': pr_id,\n",
    "                            'lr_id': lr_id,\n",
    "                            'position': pos,\n",
    "\n",
    "                        })\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            \n",
    "            G.add_node(node, **features)\n",
    "        \n",
    "        # Add edge\n",
    "        G.add_edge(aend, bend)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def create_enhanced_dataset(G, num_simulations=5000):\n",
    "    \"\"\"Create GNN dataset with enhanced position-aware features\"\"\"\n",
    "    # Group nodes by ring\n",
    "    ring_nodes = {}\n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        pr_id = attrs.get('pr_id', -1)\n",
    "        lr_id = attrs.get('lr_id', -1)\n",
    "        \n",
    "        if pr_id >= 0 and lr_id >= 0:\n",
    "            key = (pr_id, lr_id)\n",
    "            if key not in ring_nodes:\n",
    "                ring_nodes[key] = []\n",
    "            ring_nodes[key].append((node, attrs.get('position', -1)))\n",
    "    \n",
    "    # Sort nodes by position within each ring\n",
    "    for key in ring_nodes:\n",
    "        ring_nodes[key] = sorted([n for n in ring_nodes[key] if n[1] >= 0], key=lambda x: x[1])\n",
    "    \n",
    "    # Create node mapping and feature matrix\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # Create edge index\n",
    "    edge_index = []\n",
    "    for u, v in G.edges():\n",
    "        edge_index.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edge_index.append([node_to_idx[v], node_to_idx[u]])  # Add reverse edge\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "    \n",
    "    # Create node features - critical for capturing position patterns\n",
    "    x = []\n",
    "    for node in node_list:\n",
    "        attrs = G.nodes[node]\n",
    "        \n",
    "        # Core features that help predict isolations\n",
    "        features = [\n",
    "            float(attrs.get('is_block', False)),\n",
    "            attrs.get('pr_id', -1) / 10.0,  # Normalize ring IDs\n",
    "            attrs.get('lr_id', -1) / 10.0,\n",
    "            attrs.get('position', -1) / 10.0,  # Normalize position\n",
    "        ]\n",
    "        \n",
    "        x.append(features)\n",
    "    \n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    # Create dataset with balanced sampling\n",
    "    data_list = []\n",
    "    positive_count = 0\n",
    "    \n",
    "    for _ in range(num_simulations):\n",
    "        # Select a random ring with enough nodes\n",
    "        valid_rings = [k for k, nodes in ring_nodes.items() if len(nodes) >= 4]\n",
    "        if not valid_rings:\n",
    "            continue\n",
    "            \n",
    "        ring_key = random.choice(valid_rings)\n",
    "        nodes = ring_nodes[ring_key]\n",
    "        \n",
    "        if len(nodes) < 4:  # Need at least 4 nodes to have isolated nodes\n",
    "            continue\n",
    "        \n",
    "        # Select two positions with nodes between them\n",
    "        idx1 = random.randint(0, len(nodes) - 3)\n",
    "        idx2 = random.randint(idx1 + 2, len(nodes) - 1)\n",
    "        \n",
    "        fail_node1, pos1 = nodes[idx1]\n",
    "        fail_node2, pos2 = nodes[idx2]\n",
    "        pos1 = G.nodes[fail_node1]['position']\n",
    "        pos2 = G.nodes[fail_node2]['position']\n",
    "        min_pos = min(pos1, pos2)\n",
    "        max_pos = max(pos1, pos2)        \n",
    "        # Create mask for failed nodes\n",
    "        failed_mask = torch.zeros(len(node_list), dtype=torch.bool)\n",
    "        failed_mask[node_to_idx[fail_node1]] = True\n",
    "        failed_mask[node_to_idx[fail_node2]] = True\n",
    "        \n",
    "        # Create target: nodes between failed positions are isolated\n",
    "        y = torch.zeros(len(node_list), dtype=torch.float)\n",
    "        \n",
    "        # Nodes between the two failed nodes in the same ring are isolated\n",
    "        for node, pos in nodes:\n",
    "            if pos1 < pos < pos2:  # Position is between failed nodes\n",
    "                y[node_to_idx[node]] = 1.0\n",
    "                positive_count += 1\n",
    "        \n",
    "        # Create additional feature: distance from failed nodes\n",
    "        dist_features = torch.ones((len(node_list),2),dtype=torch.float)\n",
    "        \n",
    "        for node, attrs in G.nodes(data=True):\n",
    "            node_idx = node_to_idx[node]\n",
    "            node_pos = attrs.get('position', -1)\n",
    "            \n",
    "            if attrs.get('pr_id', -1) == ring_key[0] and attrs.get('lr_id', -1) == ring_key[1] and node_pos >= 0:\n",
    "                # Normalized distances to both failed nodes\n",
    "                dist_to_fail1 = abs(node_pos - pos1) / 100.0\n",
    "                dist_to_fail2 = abs(node_pos - pos2) / 100.0\n",
    "                dist_features[node_idx, 0] = dist_to_fail1\n",
    "                dist_features[node_idx, 1] = dist_to_fail2\n",
    "                is_between = 1.0 if (min_pos < node_pos < max_pos) else 0.0\n",
    "\n",
    "        \n",
    "        # Combine all features\n",
    "        node_features = torch.cat([x, dist_features,is_between], dim=1)\n",
    "        failed_pr_id = G.nodes[fail_node1].get('pr_id', -1)\n",
    "        failed_lr_id = G.nodes[fail_node1].get('lr_id', -1)\n",
    "        # Create Data object\n",
    "        data = Data(\n",
    "            x=node_features,\n",
    "            edge_index=edge_index,\n",
    "            y=y,\n",
    "            failed_nodes=failed_mask,\n",
    "            failed_pr_id=torch.tensor([failed_pr_id], dtype=torch.long),\n",
    "            failed_lr_id=torch.tensor([failed_lr_id], dtype=torch.long),\n",
    "            )\n",
    "        \n",
    "        data_list.append(data)\n",
    "    \n",
    "    print(f\"Created {len(data_list)} examples with {positive_count} positive instances\")\n",
    "    \n",
    "    return data_list, node_list, node_to_idx\n",
    "\n",
    "def train_isolation_gnn(data_list, num_epochs=100):\n",
    "    \"\"\"Train the enhanced GNN model with strategies to address class imbalance\"\"\"\n",
    "    # Split data\n",
    "    train_data, val_data = train_test_split(data_list, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Calculate class weights based on imbalance\n",
    "    all_y = torch.cat([data.y for data in train_data])\n",
    "    pos_weight = (len(all_y) - all_y.sum()) / all_y.sum() if all_y.sum() > 0 else 10.0\n",
    "    \n",
    "    print(f\"Using positive class weight: {pos_weight:.2f}\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=32)\n",
    "    \n",
    "    # Create model\n",
    "    num_node_features = train_data[0].x.size(1)\n",
    "    model = EnhancedGNN(num_node_features)\n",
    "    \n",
    "    # Use weighted loss\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]))\n",
    "    \n",
    "    # Optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    \n",
    "    # Tracking best performance\n",
    "    best_f1 = 0\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    " \n",
    "        \n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(data.x, data.edge_index, data.failed_nodes)\n",
    "\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                out.squeeze(), \n",
    "                data.y,\n",
    "                pos_weight=pos_weight  # Handle class imbalance\n",
    "            )            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                out = model(data.x, data.edge_index, data.failed_nodes)\n",
    "                pred = (torch.sigmoid(out) > 0.95).float().squeeze()\n",
    "                \n",
    "                y_true.append(data.y)\n",
    "                y_pred.append(pred)\n",
    "        \n",
    "        # Concatenate results\n",
    "        y_true = torch.cat(y_true)\n",
    "        y_pred = torch.cat(y_pred)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        true_pos = ((y_pred == 1) & (y_true == 1)).sum().item()\n",
    "        pred_pos = y_pred.sum().item()\n",
    "        real_pos = y_true.sum().item()\n",
    "        \n",
    "        precision = true_pos / max(1, pred_pos)\n",
    "        recall = true_pos / max(1, real_pos)\n",
    "        f1 = 2 * precision * recall / max(1e-8, precision + recall)\n",
    "        accuracy = (y_pred == y_true).float().mean().item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Loss: {total_loss/len(train_loader):.4f}, \"\n",
    "              f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Acc: {accuracy:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered. Best F1: {best_f1:.4f}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "def run_enhanced_gnn_pipeline(topology_df):\n",
    "    print(\"Building graph with position features...\")\n",
    "    G = build_graph_with_position_features(topology_df)\n",
    "    \n",
    "    print(\"Creating enhanced dataset...\")\n",
    "    data_list, node_list, node_to_idx = create_enhanced_dataset(G, num_simulations=1000)\n",
    "    \n",
    "    print(\"Training isolation prediction model...\")\n",
    "    model = train_isolation_gnn(data_list)\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'node_list': node_list,\n",
    "        'node_to_idx': node_to_idx\n",
    "    }, 'isolation_gnn_model.pt')\n",
    "    \n",
    "    print(\"Model training complete and saved to isolation_gnn_model.pt\")\n",
    "    return model, G, node_list, node_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def run_enhanced_gnn_pipeline(topology_df):\n",
    "    print(\"Building graph with position features...\")\n",
    "    G = build_graph_with_position_features(topology_df)\n",
    "    \n",
    "    print(\"Creating enhanced dataset...\")\n",
    "    data_list, node_list, node_to_idx = create_enhanced_dataset(G, num_simulations=1000)\n",
    "    \n",
    "    print(\"Training isolation prediction model...\")\n",
    "    model = train_isolation_gnn(data_list)\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'node_list': node_list,\n",
    "        'node_to_idx': node_to_idx\n",
    "    }, 'isolation_gnn_model.pt')\n",
    "    \n",
    "    print(\"Model training complete and saved to isolation_gnn_model.pt\")\n",
    "    return model, G, node_list, node_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building graph with position features...\n",
      "Creating enhanced dataset...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 2 in argument 0, but got float",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_enhanced_gnn_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopo_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mrun_enhanced_gnn_pipeline\u001b[39m\u001b[34m(topology_df)\u001b[39m\n\u001b[32m      4\u001b[39m G = build_graph_with_position_features(topology_df)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating enhanced dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m data_list, node_list, node_to_idx = \u001b[43mcreate_enhanced_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining isolation prediction model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m model = train_isolation_gnn(data_list)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 235\u001b[39m, in \u001b[36mcreate_enhanced_dataset\u001b[39m\u001b[34m(G, num_simulations)\u001b[39m\n\u001b[32m    231\u001b[39m         is_between = \u001b[32m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (min_pos < node_pos < max_pos) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# Combine all features\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m node_features = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43mis_between\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m failed_pr_id = G.nodes[fail_node1].get(\u001b[33m'\u001b[39m\u001b[33mpr_id\u001b[39m\u001b[33m'\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m    237\u001b[39m failed_lr_id = G.nodes[fail_node1].get(\u001b[33m'\u001b[39m\u001b[33mlr_id\u001b[39m\u001b[33m'\u001b[39m, -\u001b[32m1\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: expected Tensor as element 2 in argument 0, but got float"
     ]
    }
   ],
   "source": [
    "run_enhanced_gnn_pipeline(topo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_isolations(model, G, node_list, node_to_idx, failed_node1, failed_node2):\n",
    "    \"\"\"\n",
    "    Predict which nodes will be isolated when two specific nodes fail\n",
    "    \n",
    "    Args:\n",
    "        model: Trained EnhancedGNN model\n",
    "        G: NetworkX graph with node features\n",
    "        node_list: List of all node names\n",
    "        node_to_idx: Dictionary mapping node names to indices\n",
    "        failed_node1: First node that will fail\n",
    "        failed_node2: Second node that will fail\n",
    "        \n",
    "    Returns:\n",
    "        List of nodes predicted to be isolated\n",
    "    \"\"\"\n",
    "    # Check if failed nodes exist in the graph\n",
    "    if failed_node1 not in G or failed_node2 not in G:\n",
    "        missing = []\n",
    "        if failed_node1 not in G:\n",
    "            missing.append(failed_node1)\n",
    "        if failed_node2 not in G:\n",
    "            missing.append(failed_node2)\n",
    "        print(f\"Error: These nodes don't exist in the graph: {', '.join(missing)}\")\n",
    "        return []\n",
    "    \n",
    "    # Get ring information for failed nodes\n",
    "    node1_attrs = G.nodes[failed_node1]\n",
    "    node2_attrs = G.nodes[failed_node2]\n",
    "    \n",
    "    pr_id1, lr_id1 = node1_attrs.get('pr_id', -1), node1_attrs.get('lr_id', -1)\n",
    "    pr_id2, lr_id2 = node2_attrs.get('pr_id', -1), node2_attrs.get('lr_id', -1)\n",
    "    pos1, pos2 = node1_attrs.get('position', -1), node2_attrs.get('position', -1)\n",
    "    \n",
    "    # Verify they're in the same ring\n",
    "    if pr_id1 != pr_id2 or lr_id1 != lr_id2 or pr_id1 < 0 or lr_id1 < 0:\n",
    "        print(f\"Warning: Failed nodes are not in the same logical ring\")\n",
    "        print(f\"  {failed_node1}: PR={pr_id1}, LR={lr_id1}, Pos={pos1}\")\n",
    "        print(f\"  {failed_node2}: PR={pr_id2}, LR={lr_id2}, Pos={pos2}\")\n",
    "        return []\n",
    "    \n",
    "    # Ensure pos1 < pos2\n",
    "    if pos1 > pos2:\n",
    "        failed_node1, failed_node2 = failed_node2, failed_node1\n",
    "        pos1, pos2 = pos2, pos1\n",
    "    \n",
    "    # Create edge index tensor from the graph\n",
    "    edge_index = []\n",
    "    for u, v in G.edges():\n",
    "        edge_index.append([node_to_idx[u], node_to_idx[v]])\n",
    "        edge_index.append([node_to_idx[v], node_to_idx[u]])\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "    \n",
    "    # Create feature matrix with the current state\n",
    "    x = []\n",
    "    for node in node_list:\n",
    "        attrs = G.nodes[node]\n",
    "        \n",
    "        # Basic features\n",
    "        features = [\n",
    "            float(attrs.get('is_block', False)),\n",
    "            attrs.get('pr_id', -1) / 10.0,\n",
    "            attrs.get('lr_id', -1) / 10.0,\n",
    "            attrs.get('position', -1) / 100.0,\n",
    "            attrs.get('position_normalized', 0.0),\n",
    "            attrs.get('ring_size', 0) / 20.0\n",
    "        ]\n",
    "        \n",
    "        # Add distance features to failed nodes\n",
    "        node_pos = attrs.get('position', -1)\n",
    "        node_pr = attrs.get('pr_id', -1)\n",
    "        node_lr = attrs.get('lr_id', -1)\n",
    "        \n",
    "        # Calculate distances only for nodes in the same ring\n",
    "        if node_pr == pr_id1 and node_lr == lr_id1 and node_pos >= 0:\n",
    "            dist_to_fail1 = abs(node_pos - pos1) / 100.0\n",
    "            dist_to_fail2 = abs(node_pos - pos2) / 100.0\n",
    "        else:\n",
    "            dist_to_fail1 = 1.0  # Max normalized distance\n",
    "            dist_to_fail2 = 1.0\n",
    "        \n",
    "        features.extend([dist_to_fail1, dist_to_fail2])\n",
    "        x.append(features)\n",
    "    \n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    # Create mask for failed nodes\n",
    "    failed_mask = torch.zeros(len(node_list), dtype=torch.bool)\n",
    "    failed_mask[node_to_idx[failed_node1]] = True\n",
    "    failed_mask[node_to_idx[failed_node2]] = True\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Create a Data object\n",
    "        data = Data(x=x, edge_index=edge_index, failed_nodes=failed_mask)\n",
    "        \n",
    "        # Run inference\n",
    "        out = model(data.x, data.edge_index, data.failed_nodes)\n",
    "        predictions = torch.sigmoid(out).squeeze()\n",
    "        \n",
    "        # Get isolated nodes (probability > 0.5)\n",
    "        isolated_indices = torch.where(predictions > 0.5)[0].tolist()\n",
    "        isolated_nodes = [node_list[idx] for idx in isolated_indices \n",
    "                         if node_list[idx] not in [failed_node1, failed_node2]]\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nPrediction results when {failed_node1} and {failed_node2} fail:\")\n",
    "    print(f\"  Failed node 1: {failed_node1} (Position {pos1})\")\n",
    "    print(f\"  Failed node 2: {failed_node2} (Position {pos2})\")\n",
    "    print(f\"  Physical Ring: {pr_id1}, Logical Ring: {lr_id1}\")\n",
    "    print(f\"  Predicted isolated nodes: {len(isolated_nodes)}\")\n",
    "    \n",
    "    # Optional: Calculate expected isolations based on position rule\n",
    "    expected_isolated = []\n",
    "    for node in node_list:\n",
    "        attrs = G.nodes[node]\n",
    "        if (attrs.get('pr_id', -1) == pr_id1 and \n",
    "            attrs.get('lr_id', -1) == lr_id1 and \n",
    "            pos1 < attrs.get('position', -1) < pos2):\n",
    "            expected_isolated.append(node)\n",
    "    \n",
    "    # Compare with rule-based expectation\n",
    "    print(\"\\nIsolated nodes based on position rule:\")\n",
    "    for node in expected_isolated:\n",
    "        node_pos = G.nodes[node].get('position', -1)\n",
    "        is_predicted = node in isolated_nodes\n",
    "        print(f\"  {node} (Position {node_pos}): {'✓' if is_predicted else '✗'}\")\n",
    "    \n",
    "    print(\"\\nModel predictions:\")\n",
    "    for node in isolated_nodes:\n",
    "        node_pos = G.nodes[node].get('position', -1)\n",
    "        node_pr = G.nodes[node].get('pr_id', -1)\n",
    "        node_lr = G.nodes[node].get('lr_id', -1)\n",
    "        is_expected = pos1 < node_pos < pos2 and node_pr == pr_id1 and node_lr == lr_id1\n",
    "        print(f\"  {node} (Position {node_pos}, PR={node_pr}, LR={node_lr}): {'✓' if is_expected else '✗'}\")\n",
    "    \n",
    "    # Calculate prediction accuracy\n",
    "    correct = sum(1 for node in isolated_nodes if node in expected_isolated)\n",
    "    total_expected = len(expected_isolated)\n",
    "    if total_expected > 0:\n",
    "        accuracy = correct / total_expected * 100\n",
    "        print(f\"\\nAccuracy: {correct}/{total_expected} correct predictions ({accuracy:.2f}%)\")\n",
    "    \n",
    "    return isolated_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction results when NODE_4_1_2 and NODE_4_1_5 fail:\n",
      "  Failed node 1: NODE_4_1_2 (Position 2)\n",
      "  Failed node 2: NODE_4_1_5 (Position 5)\n",
      "  Physical Ring: 4, Logical Ring: 1\n",
      "  Predicted isolated nodes: 6\n",
      "\n",
      "Isolated nodes based on position rule:\n",
      "  NODE_4_1_3 (Position 3): ✓\n",
      "  NODE_4_1_4 (Position 4): ✓\n",
      "\n",
      "Model predictions:\n",
      "  NODE_4_1_1 (Position 1, PR=4, LR=1): ✗\n",
      "  NODE_4_1_3 (Position 3, PR=4, LR=1): ✓\n",
      "  NODE_4_1_4 (Position 4, PR=4, LR=1): ✓\n",
      "  NODE_4_1_6 (Position 6, PR=4, LR=1): ✗\n",
      "  NODE_4_1_7 (Position 7, PR=4, LR=1): ✗\n",
      "  NODE_4_1_8 (Position 8, PR=4, LR=1): ✗\n",
      "\n",
      "Accuracy: 2/2 correct predictions (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# Load a trained model (if you've saved it)\n",
    "def load_model_and_predict(model_path, topology_df, failed_node1, failed_node2):\n",
    "    # Load the model\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    # Create a new model instance\n",
    "    num_features = 6 # 6 basic features + 2 distance features\n",
    "    model = EnhancedGNN(num_features)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Get node list and mapping\n",
    "    node_list = checkpoint.get('node_list')\n",
    "    node_to_idx = checkpoint.get('node_to_idx')\n",
    "    \n",
    "    # If node mapping wasn't saved, rebuild the graph\n",
    "    if not node_list or not node_to_idx:\n",
    "        print(\"Rebuilding graph from topology data...\")\n",
    "        G = build_graph_with_position_features(topology_df)\n",
    "        node_list = list(G.nodes())\n",
    "        node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    else:\n",
    "        G = build_graph_with_position_features(topology_df)\n",
    "    \n",
    "    # Predict isolations\n",
    "    isolated_nodes = predict_isolations(\n",
    "        model, G, node_list, node_to_idx, failed_node1, failed_node2\n",
    "    )\n",
    "    \n",
    "    return isolated_nodes\n",
    "\n",
    "\n",
    "isolated_nodes = load_model_and_predict(\n",
    "    'isolation_gnn_model.pt',\n",
    "    topo_data,\n",
    "    'NODE_4_1_2',\n",
    "    'NODE_4_1_5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "class PrecisionIsolationGNN(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(PrecisionIsolationGNN, self).__init__()\n",
    "        # Print the input features dimension for debugging\n",
    "        print(f\"Model initialized with {in_features} input features\")\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.conv1 = GCNConv(in_features, 32)\n",
    "        self.conv2 = GATConv(32, 32, heads=2)\n",
    "        \n",
    "        # Prediction layers\n",
    "        self.fc1 = nn.Linear(32*2, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Basic GNN architecture with attention\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Final prediction\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_precision_gnn(data_list, num_epochs=100):\n",
    "    \"\"\"Train with focus on precision\"\"\"\n",
    "    # Check feature dimensions\n",
    "    print(f\"Input data has {data_list[0].x.shape[1]} features per node\")\n",
    "    \n",
    "    # Split training and validation\n",
    "    train_size = int(0.8 * len(data_list))\n",
    "    train_data = data_list[:train_size]\n",
    "    val_data = data_list[train_size:]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=32)\n",
    "    \n",
    "    # Create model with correct input dimension\n",
    "    in_features = data_list[0].x.shape[1]\n",
    "    model = PrecisionIsolationGNN(in_features)\n",
    "    \n",
    "    # Weighted loss - higher penalty for false positives\n",
    "    pos_samples = sum(data.y.sum() for data in train_data)\n",
    "    total_samples = sum(len(data.y) for data in train_data)\n",
    "    pos_weight = (total_samples - pos_samples) / max(1, pos_samples)\n",
    "    \n",
    "    print(f\"Using positive weight: {pos_weight:.2f}\")\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    \n",
    "    # Tracking best model\n",
    "    best_precision = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = criterion(out.squeeze(), data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation with higher threshold for precision\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            all_preds = []\n",
    "            all_targets = []\n",
    "            \n",
    "            for data in val_loader:\n",
    "                out = model(data.x, data.edge_index)\n",
    "                # Higher threshold increases precision\n",
    "                pred = (torch.sigmoid(out) > 0.7).float().squeeze()\n",
    "                all_preds.append(pred)\n",
    "                all_targets.append(data.y)\n",
    "            \n",
    "            all_preds = torch.cat(all_preds)\n",
    "            all_targets = torch.cat(all_targets)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            true_pos = ((all_preds == 1) & (all_targets == 1)).sum().item()\n",
    "            pred_pos = all_preds.sum().item()\n",
    "            actual_pos = all_targets.sum().item()\n",
    "            \n",
    "            # Handle division by zero\n",
    "            precision = true_pos / max(1, pred_pos)\n",
    "            recall = true_pos / max(1, actual_pos)\n",
    "            f1 = 2 * precision * recall / max(0.001, precision + recall)\n",
    "            accuracy = (all_preds == all_targets).float().mean().item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Loss: {total_loss/len(train_loader):.4f}, \"\n",
    "              f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Acc: {accuracy:.4f}\")\n",
    "        \n",
    "        # Save best model based on precision\n",
    "        if precision > best_precision and recall > 0.2:\n",
    "            best_precision = precision\n",
    "            best_model = model.state_dict().copy()\n",
    "            print(f\"New best model (precision: {precision:.4f})\")\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data has 6 features per node\n",
      "Model initialized with 6 input features\n",
      "Using positive weight: 111.90\n",
      "Epoch 1/100: Loss: 0.9874, Precision: 0.3163, Recall: 0.8765, F1: 0.4648, Acc: 0.9823\n",
      "New best model (precision: 0.3163)\n",
      "Epoch 2/100: Loss: 0.4849, Precision: 0.3084, Recall: 0.8765, F1: 0.4563, Acc: 0.9817\n",
      "Epoch 3/100: Loss: 0.4752, Precision: 0.3232, Recall: 0.8765, F1: 0.4723, Acc: 0.9828\n",
      "New best model (precision: 0.3232)\n",
      "Epoch 4/100: Loss: 0.4726, Precision: 0.3171, Recall: 0.8765, F1: 0.4658, Acc: 0.9824\n",
      "Epoch 5/100: Loss: 0.4680, Precision: 0.3294, Recall: 0.8765, F1: 0.4788, Acc: 0.9833\n",
      "New best model (precision: 0.3294)\n",
      "Epoch 6/100: Loss: 0.4601, Precision: 0.3296, Recall: 0.8765, F1: 0.4790, Acc: 0.9833\n",
      "New best model (precision: 0.3296)\n",
      "Epoch 7/100: Loss: 0.4567, Precision: 0.3294, Recall: 0.8765, F1: 0.4788, Acc: 0.9833\n",
      "Epoch 8/100: Loss: 0.4587, Precision: 0.3296, Recall: 0.8765, F1: 0.4791, Acc: 0.9833\n",
      "New best model (precision: 0.3296)\n",
      "Epoch 9/100: Loss: 0.4515, Precision: 0.3294, Recall: 0.8765, F1: 0.4788, Acc: 0.9833\n",
      "Epoch 10/100: Loss: 0.4486, Precision: 0.3301, Recall: 0.8765, F1: 0.4795, Acc: 0.9833\n",
      "New best model (precision: 0.3301)\n",
      "Epoch 11/100: Loss: 0.4457, Precision: 0.3312, Recall: 0.8765, F1: 0.4808, Acc: 0.9834\n",
      "New best model (precision: 0.3312)\n",
      "Epoch 12/100: Loss: 0.4430, Precision: 0.3323, Recall: 0.8765, F1: 0.4819, Acc: 0.9835\n",
      "New best model (precision: 0.3323)\n",
      "Epoch 13/100: Loss: 0.4360, Precision: 0.3488, Recall: 0.8765, F1: 0.4990, Acc: 0.9846\n",
      "New best model (precision: 0.3488)\n",
      "Epoch 14/100: Loss: 0.4260, Precision: 0.3529, Recall: 0.8765, F1: 0.5032, Acc: 0.9848\n",
      "New best model (precision: 0.3529)\n",
      "Epoch 15/100: Loss: 0.4114, Precision: 0.3932, Recall: 0.8765, F1: 0.5429, Acc: 0.9870\n",
      "New best model (precision: 0.3932)\n",
      "Epoch 16/100: Loss: 0.4076, Precision: 0.3952, Recall: 0.8765, F1: 0.5447, Acc: 0.9871\n",
      "New best model (precision: 0.3952)\n",
      "Epoch 17/100: Loss: 0.3981, Precision: 0.4199, Recall: 0.8765, F1: 0.5678, Acc: 0.9883\n",
      "New best model (precision: 0.4199)\n",
      "Epoch 18/100: Loss: 0.3962, Precision: 0.4237, Recall: 0.8765, F1: 0.5713, Acc: 0.9885\n",
      "New best model (precision: 0.4237)\n",
      "Epoch 19/100: Loss: 0.3916, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "New best model (precision: 0.4286)\n",
      "Epoch 20/100: Loss: 0.3921, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 21/100: Loss: 0.3898, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 22/100: Loss: 0.3894, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 23/100: Loss: 0.3871, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 24/100: Loss: 0.3848, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 25/100: Loss: 0.3857, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 26/100: Loss: 0.3847, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 27/100: Loss: 0.3825, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 28/100: Loss: 0.3830, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 29/100: Loss: 0.3824, Precision: 0.4288, Recall: 0.8765, F1: 0.5758, Acc: 0.9887\n",
      "New best model (precision: 0.4288)\n",
      "Epoch 30/100: Loss: 0.3832, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 31/100: Loss: 0.3792, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 32/100: Loss: 0.3799, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 33/100: Loss: 0.3791, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 34/100: Loss: 0.3794, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 35/100: Loss: 0.3750, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 36/100: Loss: 0.3751, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 37/100: Loss: 0.3724, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 38/100: Loss: 0.3739, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 39/100: Loss: 0.3697, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 40/100: Loss: 0.3729, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 41/100: Loss: 0.3687, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 42/100: Loss: 0.3675, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 43/100: Loss: 0.3660, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 44/100: Loss: 0.3660, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 45/100: Loss: 0.3653, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 46/100: Loss: 0.3637, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 47/100: Loss: 0.3624, Precision: 0.4290, Recall: 0.8765, F1: 0.5761, Acc: 0.9887\n",
      "New best model (precision: 0.4290)\n",
      "Epoch 48/100: Loss: 0.3624, Precision: 0.4398, Recall: 0.8765, F1: 0.5857, Acc: 0.9891\n",
      "New best model (precision: 0.4398)\n",
      "Epoch 49/100: Loss: 0.3607, Precision: 0.4398, Recall: 0.8765, F1: 0.5857, Acc: 0.9891\n",
      "Epoch 50/100: Loss: 0.3597, Precision: 0.4398, Recall: 0.8765, F1: 0.5857, Acc: 0.9891\n",
      "Epoch 51/100: Loss: 0.3605, Precision: 0.4329, Recall: 0.8765, F1: 0.5796, Acc: 0.9888\n",
      "Epoch 52/100: Loss: 0.3596, Precision: 0.4398, Recall: 0.8765, F1: 0.5857, Acc: 0.9891\n",
      "Epoch 53/100: Loss: 0.3630, Precision: 0.4398, Recall: 0.8765, F1: 0.5857, Acc: 0.9891\n",
      "Epoch 54/100: Loss: 0.3576, Precision: 0.4286, Recall: 0.8765, F1: 0.5757, Acc: 0.9887\n",
      "Epoch 55/100: Loss: 0.3588, Precision: 0.4398, Recall: 0.8765, F1: 0.5857, Acc: 0.9891\n",
      "Epoch 56/100: Loss: 0.3582, Precision: 0.4450, Recall: 0.8765, F1: 0.5903, Acc: 0.9893\n",
      "New best model (precision: 0.4450)\n",
      "Epoch 57/100: Loss: 0.3573, Precision: 0.4390, Recall: 0.8765, F1: 0.5850, Acc: 0.9891\n",
      "Epoch 58/100: Loss: 0.3568, Precision: 0.4390, Recall: 0.8765, F1: 0.5850, Acc: 0.9891\n",
      "Epoch 59/100: Loss: 0.3566, Precision: 0.4398, Recall: 0.8765, F1: 0.5857, Acc: 0.9891\n",
      "Epoch 60/100: Loss: 0.3551, Precision: 0.4398, Recall: 0.8765, F1: 0.5857, Acc: 0.9891\n",
      "Epoch 61/100: Loss: 0.3558, Precision: 0.4309, Recall: 0.8765, F1: 0.5778, Acc: 0.9888\n",
      "Epoch 62/100: Loss: 0.3550, Precision: 0.4401, Recall: 0.8765, F1: 0.5860, Acc: 0.9891\n",
      "Epoch 63/100: Loss: 0.3545, Precision: 0.4398, Recall: 0.8765, F1: 0.5857, Acc: 0.9891\n",
      "Epoch 64/100: Loss: 0.3527, Precision: 0.4382, Recall: 0.8765, F1: 0.5842, Acc: 0.9891\n",
      "Epoch 65/100: Loss: 0.3534, Precision: 0.4466, Recall: 0.8765, F1: 0.5917, Acc: 0.9894\n",
      "New best model (precision: 0.4466)\n",
      "Epoch 66/100: Loss: 0.3524, Precision: 0.4398, Recall: 0.8765, F1: 0.5857, Acc: 0.9891\n",
      "Epoch 67/100: Loss: 0.3517, Precision: 0.4402, Recall: 0.8765, F1: 0.5860, Acc: 0.9891\n",
      "Epoch 68/100: Loss: 0.3511, Precision: 0.4398, Recall: 0.8765, F1: 0.5857, Acc: 0.9891\n",
      "Epoch 69/100: Loss: 0.3501, Precision: 0.4398, Recall: 0.8765, F1: 0.5857, Acc: 0.9891\n",
      "Epoch 70/100: Loss: 0.3485, Precision: 0.4464, Recall: 0.8765, F1: 0.5916, Acc: 0.9894\n",
      "Epoch 71/100: Loss: 0.3506, Precision: 0.4614, Recall: 0.8765, F1: 0.6045, Acc: 0.9899\n",
      "New best model (precision: 0.4614)\n",
      "Epoch 72/100: Loss: 0.3511, Precision: 0.4470, Recall: 0.8765, F1: 0.5921, Acc: 0.9894\n",
      "Epoch 73/100: Loss: 0.3475, Precision: 0.4697, Recall: 0.8765, F1: 0.6116, Acc: 0.9902\n",
      "New best model (precision: 0.4697)\n",
      "Epoch 74/100: Loss: 0.3467, Precision: 0.4471, Recall: 0.8765, F1: 0.5922, Acc: 0.9894\n",
      "Epoch 75/100: Loss: 0.3469, Precision: 0.4858, Recall: 0.8765, F1: 0.6251, Acc: 0.9908\n",
      "New best model (precision: 0.4858)\n",
      "Epoch 76/100: Loss: 0.3458, Precision: 0.4491, Recall: 0.8765, F1: 0.5939, Acc: 0.9895\n",
      "Epoch 77/100: Loss: 0.3447, Precision: 0.4936, Recall: 0.8765, F1: 0.6315, Acc: 0.9910\n",
      "New best model (precision: 0.4936)\n",
      "Epoch 78/100: Loss: 0.3453, Precision: 0.4666, Recall: 0.8765, F1: 0.6090, Acc: 0.9901\n",
      "Epoch 79/100: Loss: 0.3443, Precision: 0.4835, Recall: 0.8765, F1: 0.6232, Acc: 0.9907\n",
      "Epoch 80/100: Loss: 0.3432, Precision: 0.4678, Recall: 0.8765, F1: 0.6100, Acc: 0.9902\n",
      "Epoch 81/100: Loss: 0.3445, Precision: 0.4680, Recall: 0.8765, F1: 0.6102, Acc: 0.9902\n",
      "Epoch 82/100: Loss: 0.3418, Precision: 0.4936, Recall: 0.8765, F1: 0.6315, Acc: 0.9910\n",
      "Epoch 83/100: Loss: 0.3402, Precision: 0.4847, Recall: 0.8765, F1: 0.6242, Acc: 0.9907\n",
      "Epoch 84/100: Loss: 0.3395, Precision: 0.4876, Recall: 0.8765, F1: 0.6266, Acc: 0.9908\n",
      "Epoch 85/100: Loss: 0.3407, Precision: 0.4768, Recall: 0.8765, F1: 0.6176, Acc: 0.9905\n",
      "Epoch 86/100: Loss: 0.3409, Precision: 0.4848, Recall: 0.8765, F1: 0.6243, Acc: 0.9907\n",
      "Epoch 87/100: Loss: 0.3393, Precision: 0.4995, Recall: 0.8765, F1: 0.6363, Acc: 0.9912\n",
      "New best model (precision: 0.4995)\n",
      "Epoch 88/100: Loss: 0.3391, Precision: 0.4995, Recall: 0.8765, F1: 0.6363, Acc: 0.9912\n",
      "Epoch 89/100: Loss: 0.3393, Precision: 0.4852, Recall: 0.8765, F1: 0.6247, Acc: 0.9908\n",
      "Epoch 90/100: Loss: 0.3385, Precision: 0.4991, Recall: 0.8765, F1: 0.6360, Acc: 0.9912\n",
      "Epoch 91/100: Loss: 0.3374, Precision: 0.4991, Recall: 0.8765, F1: 0.6360, Acc: 0.9912\n",
      "Epoch 92/100: Loss: 0.3372, Precision: 0.4995, Recall: 0.8765, F1: 0.6363, Acc: 0.9912\n",
      "Epoch 93/100: Loss: 0.3376, Precision: 0.4978, Recall: 0.8765, F1: 0.6350, Acc: 0.9912\n",
      "Epoch 94/100: Loss: 0.3380, Precision: 0.4991, Recall: 0.8765, F1: 0.6360, Acc: 0.9912\n",
      "Epoch 95/100: Loss: 0.3372, Precision: 0.4995, Recall: 0.8765, F1: 0.6363, Acc: 0.9912\n",
      "Epoch 96/100: Loss: 0.3388, Precision: 0.4974, Recall: 0.8765, F1: 0.6346, Acc: 0.9911\n",
      "Epoch 97/100: Loss: 0.3356, Precision: 0.4882, Recall: 0.8765, F1: 0.6271, Acc: 0.9909\n",
      "Epoch 98/100: Loss: 0.3356, Precision: 0.4995, Recall: 0.8765, F1: 0.6363, Acc: 0.9912\n",
      "Epoch 99/100: Loss: 0.3362, Precision: 0.4861, Recall: 0.8765, F1: 0.6254, Acc: 0.9908\n",
      "Epoch 100/100: Loss: 0.3347, Precision: 0.4892, Recall: 0.8765, F1: 0.6279, Acc: 0.9909\n"
     ]
    }
   ],
   "source": [
    "model = train_precision_gnn(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to precision_isolation_gnn_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'node_list': node_list,\n",
    "    'node_to_idx': node_to_idx\n",
    "}, 'precision_isolation_gnn_model.pt')\n",
    "\n",
    "print(\"Model saved to precision_isolation_gnn_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_trained_gnn(model, G, failed_node1, failed_node2):\n",
    "    \"\"\"\n",
    "    Make predictions with a trained PrecisionIsolationGNN model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PrecisionIsolationGNN model\n",
    "        G: NetworkX graph with node features\n",
    "        failed_node1, failed_node2: Names of nodes that will fail\n",
    "        \n",
    "    Returns:\n",
    "        List of nodes predicted to be isolated\n",
    "    \"\"\"\n",
    "    # Extract input feature dimension from model\n",
    "    in_features = model.conv1.lin.weight.size(1)\n",
    "    print(f\"Model expects {in_features} input features\")\n",
    "    \n",
    "    # Get node information\n",
    "    if failed_node1 not in G or failed_node2 not in G:\n",
    "        print(f\"Error: Failed nodes not found in graph\")\n",
    "        return []\n",
    "    \n",
    "    # Get position information\n",
    "    node1_attrs = G.nodes[failed_node1]\n",
    "    node2_attrs = G.nodes[failed_node2]\n",
    "    \n",
    "    pr_id1 = node1_attrs.get('pr_id', -1)\n",
    "    lr_id1 = node1_attrs.get('lr_id', -1)\n",
    "    pos1 = node1_attrs.get('position', -1)\n",
    "    \n",
    "    pr_id2 = node2_attrs.get('pr_id', -1)\n",
    "    lr_id2 = node2_attrs.get('lr_id', -1)\n",
    "    pos2 = node2_attrs.get('position', -1)\n",
    "    \n",
    "    # Ensure position order\n",
    "    if pos1 > pos2:\n",
    "        pos1, pos2 = pos2, pos1\n",
    "    \n",
    "    print(f\"\\nPredicting isolations when nodes fail:\")\n",
    "    print(f\"  {failed_node1} (PR={pr_id1}, LR={lr_id1}, Pos={pos1})\")\n",
    "    print(f\"  {failed_node2} (PR={pr_id2}, LR={lr_id2}, Pos={pos2})\")\n",
    "    \n",
    "    # Create node list and mapping\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # Create features with EXACT dimension matching\n",
    "    x = []\n",
    "    for node in node_list:\n",
    "        attrs = G.nodes[node]\n",
    "        \n",
    "        # Extract basic features\n",
    "        node_pr = attrs.get('pr_id', -1)\n",
    "        node_lr = attrs.get('lr_id', -1)\n",
    "        node_pos = attrs.get('position', -1)\n",
    "        \n",
    "        # Create appropriate number of features based on model's expected input\n",
    "        if in_features == 6:\n",
    "            features = [\n",
    "                float(attrs.get('is_block', False)),\n",
    "                node_pr / 10.0,\n",
    "                node_lr / 10.0,\n",
    "                node_pos / 100.0,\n",
    "                attrs.get('position_normalized', 0.0),\n",
    "                attrs.get('ring_size', 0) / 20.0\n",
    "            ]\n",
    "        elif in_features == 8:\n",
    "            # Calculate distances to failed nodes\n",
    "            if node_pr == pr_id1 and node_lr == lr_id1 and node_pos >= 0:\n",
    "                dist_to_fail1 = abs(node_pos - pos1) / 100.0\n",
    "                dist_to_fail2 = abs(node_pos - pos2) / 100.0\n",
    "            else:\n",
    "                dist_to_fail1 = 1.0\n",
    "                dist_to_fail2 = 1.0\n",
    "                \n",
    "            features = [\n",
    "                float(attrs.get('is_block', False)),\n",
    "                node_pr / 10.0,\n",
    "                node_lr / 10.0,\n",
    "                node_pos / 100.0,\n",
    "                attrs.get('position_normalized', 0.0),\n",
    "                attrs.get('ring_size', 0) / 20.0,\n",
    "                dist_to_fail1,\n",
    "                dist_to_fail2\n",
    "            ]\n",
    "        else:\n",
    "            print(f\"Error: Unknown feature dimension: {in_features}\")\n",
    "            return []\n",
    "            \n",
    "        x.append(features)\n",
    "    \n",
    "    # Create edge index\n",
    "    edge_index = []\n",
    "    for u, v in G.edges():\n",
    "        if u in node_to_idx and v in node_to_idx:  # Safety check\n",
    "            edge_index.append([node_to_idx[u], node_to_idx[v]])\n",
    "            edge_index.append([node_to_idx[v], node_to_idx[u]])  # Add reverse edge\n",
    "        \n",
    "    # Convert to tensors\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    print(f\"Feature shape: {x.shape}, Edge index shape: {edge_index.shape}\")\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(x, edge_index)\n",
    "        probs = torch.sigmoid(output).squeeze()\n",
    "        \n",
    "        # Use same threshold as in training (0.7)\n",
    "        predictions = (probs > 0.7).float()\n",
    "    \n",
    "    # Get isolated nodes\n",
    "    isolated_nodes = []\n",
    "    for i, pred in enumerate(predictions):\n",
    "        if pred == 1 and node_list[i] not in [failed_node1, failed_node2]:\n",
    "            isolated_nodes.append(node_list[i])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nPredicted {len(isolated_nodes)} isolated nodes:\")\n",
    "    for node in sorted(isolated_nodes, key=lambda n: G.nodes[n].get('position', -1)):\n",
    "        node_pos = G.nodes[node].get('position', -1)\n",
    "        node_pr = G.nodes[node].get('pr_id', -1)\n",
    "        node_lr = G.nodes[node].get('lr_id', -1)\n",
    "        is_expected = (node_pr == pr_id1 and node_lr == lr_id1 and pos1 < node_pos < pos2)\n",
    "        status = \"✓\" if is_expected else \"✗\"\n",
    "        print(f\"  {node} (PR={node_pr}, LR={node_lr}, Pos={node_pos}) {status}\")\n",
    "    \n",
    "    # Compare with ground truth\n",
    "    expected = []\n",
    "    for node in node_list:\n",
    "        attrs = G.nodes[node]\n",
    "        node_pr = attrs.get('pr_id', -1)\n",
    "        node_lr = attrs.get('lr_id', -1)\n",
    "        node_pos = attrs.get('position', -1)\n",
    "        \n",
    "        if (node_pr == pr_id1 and node_lr == lr_id1 and pos1 < node_pos < pos2):\n",
    "            expected.append(node)\n",
    "    \n",
    "    correct = len([n for n in isolated_nodes if n in expected])\n",
    "    precision = correct / max(1, len(isolated_nodes))\n",
    "    recall = correct / max(1, len(expected))\n",
    "    \n",
    "    print(f\"\\nExpected {len(expected)} isolated nodes\")\n",
    "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}\")\n",
    "    \n",
    "    return isolated_nodes\n",
    "\n",
    "# Usage:\n",
    "# isolated_nodes = predict_with_trained_gnn(model, G, \"NODE_1_2_5\", \"NODE_1_2_9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model expects 6 input features\n",
      "\n",
      "Predicting isolations when nodes fail:\n",
      "  NODE_1_2_5 (PR=1, LR=2, Pos=5)\n",
      "  NODE_1_2_9 (PR=1, LR=2, Pos=9)\n",
      "Feature shape: torch.Size([310, 6]), Edge index shape: torch.Size([2, 660])\n",
      "\n",
      "Predicted 0 isolated nodes:\n",
      "\n",
      "Expected 3 isolated nodes\n",
      "Precision: 0.00, Recall: 0.00\n"
     ]
    }
   ],
   "source": [
    "failed_node1 = \"NODE_1_2_5\"\n",
    "failed_node2 = \"NODE_1_2_9\"\n",
    "node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "\n",
    "isolated_nodes = predict_with_trained_gnn(model, G, \"NODE_1_2_5\", \"NODE_1_2_9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
